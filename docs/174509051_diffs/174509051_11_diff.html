<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 7.0,
  "old_code": "# Feel free to pip install and import any useful libraries within the rewrite cell.\\n# The \`segmentation-models-pytorch\` and \`timm\` libraries are already installed.\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nimport segmentation_models_pytorch as smp\\nimport numpy as np\\nfrom PIL import Image\\nimport os\\nfrom tqdm import tqdm # For progress bars\\n\\n# Define constants for the model\\nNUM_CLASSES = 17 # As specified in the problem description (airplane, bare-soil, ..., water)\\nIMAGE_SIZE = 256 # Images are 256x256 pixels as specified for UC Merced Land Use Dataset\\n\\n# --- Custom Transform for both Image and Mask ---\\nclass CustomResizeAndToTensor:\\n    \\"\\"\\"\\n    A custom transformation that resizes an image and its corresponding mask,\\n    then converts them to PyTorch tensors.\\n    \\"\\"\\"\\n    def __init__(self, size):\\n        self.size = size\\n        # Use BILINEAR for images to maintain quality, NEAREST for masks to preserve class integrity\\n        self.resize_image = transforms.Resize(size, Image.BILINEAR)\\n        self.resize_mask = transforms.Resize(size, Image.NEAREST)\\n\\n    def __call__(self, image_np, mask_np):\\n        # Convert numpy arrays to PIL Images for torchvision.transforms\\n        image_pil = Image.fromarray(image_np).convert(\\"RGB\\") # Ensure 3 channels\\n        mask_pil = Image.fromarray(mask_np.astype(np.uint8), mode='L') # Ensure 8-bit grayscale for mask\\n\\n        # Apply resizing\\n        image_resized_pil = self.resize_image(image_pil)\\n        mask_resized_pil = self.resize_mask(mask_pil)\\n\\n        # Convert back to numpy arrays\\n        image_resized_np = np.array(image_resized_pil)\\n        mask_resized_np = np.array(mask_resized_pil)\\n\\n        return image_resized_np, mask_resized_np\\n\\n\\n# --- Dataset Class ---\\nclass DLRSDDataset(Dataset):\\n    \\"\\"\\"\\n    PyTorch Dataset for loading DLRSD images and corresponding masks.\\n    \\"\\"\\"\\n    def __init__(self, data_paths, transform=None):\\n        self.data_paths = data_paths\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.data_paths)\\n\\n    def __getitem__(self, idx):\\n        image_path, label_path = self.data_paths[idx]\\n\\n        # Load image and label\\n        image = Image.open(image_path).convert(\\"RGB\\")\\n        label = Image.open(label_path).convert(\\"L\\") # Load as grayscale (8-bit pixels)\\n\\n        image_np = np.array(image)\\n        label_np = np.array(label)\\n\\n        # Apply transformations if provided\\n        if self.transform:\\n            image_np, label_np = self.transform(image_np, label_np)\\n\\n        # Labels are 1-indexed (1 to 17) in the dataset,\\n        # but PyTorch's CrossEntropyLoss expects 0-indexed (0 to 16).\\n        # We subtract 1 here. The evaluation function \`calculate_mean_iou\` expects 1-indexed.\\n        label_np = label_np - 1\\n\\n        # Convert to torch tensors\\n        # For images: (H, W, C) -> (C, H, W) and normalize\\n        image_tensor = torch.from_numpy(image_np).float().permute(2, 0, 1)\\n        image_tensor = image_tensor / 255.0 # Scale to [0, 1]\\n\\n        # Standardize with ImageNet means and stds for pre-trained encoder\\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\\n        image_tensor = (image_tensor - mean) / std\\n\\n        # For labels: ensure it's a long tensor for CrossEntropyLoss\\n        label_tensor = torch.from_numpy(label_np).long()\\n\\n        return image_tensor, label_tensor\\n\\n\\n# --- Main Model Class (Wrapper for SMP U-Net) ---\\nclass SemanticSegmentationModel:\\n    \\"\\"\\"\\n    Encapsulates the semantic segmentation model, training, and prediction logic.\\n    \\"\\"\\"\\n    def __init__(self, num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\\n        self.num_classes = num_classes\\n        self.image_size = image_size\\n        self.device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n        # Initialize U-Net model from segmentation_models_pytorch\\n        # Using efficientnet-b0 as a common, efficient pre-trained backbone\\n        self.model = smp.Unet(\\n            encoder_name=\\"efficientnet-b0\\",\\n            encoder_weights=\\"imagenet\\", # Use pre-trained weights\\n            in_channels=3, # RGB images\\n            classes=self.num_classes, # Number of output classes\\n        )\\n        self.model.to(self.device) # Move model to GPU/CPU\\n\\n        # Initialize the image/mask transform\\n        self.transform = CustomResizeAndToTensor(self.image_size)\\n\\n    def train(self, training_data_paths, num_epochs=10, batch_size=16, learning_rate=1e-3):\\n        \\"\\"\\"\\n        Trains the segmentation model using the provided data paths.\\n        \\"\\"\\"\\n        train_dataset = DLRSDDataset(training_data_paths, transform=self.transform)\\n        # Using fewer workers for potentially lower memory consumption and better compatibility\\n        # on various systems. Half of CPU count, minimum 1.\\n        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() // 2 or 1)\\n\\n        loss_fn = nn.CrossEntropyLoss() # Standard loss for multi-class pixel classification\\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\\n\\n        self.model.train() # Set model to training mode\\n\\n        print(f\\"Training model on {len(training_data_paths)} images for {num_epochs} epochs...\\")\\n        for epoch in range(num_epochs):\\n            running_loss = 0.0\\n            # Use tqdm for a progress bar during training\\n            for images, masks in tqdm(train_dataloader, desc=f\\"Epoch {epoch+1}/{num_epochs}\\"):\\n                images = images.to(self.device)\\n                masks = masks.to(self.device)\\n\\n                optimizer.zero_grad() # Clear gradients\\n                outputs = self.model(images) # Forward pass\\n                loss = loss_fn(outputs, masks) # Compute loss\\n                loss.backward() # Backward pass (compute gradients)\\n                optimizer.step() # Update model weights\\n\\n                running_loss += loss.item() * images.size(0)\\n\\n            epoch_loss = running_loss / len(train_dataset)\\n            print(f\\"Epoch {epoch+1} completed. Loss: {epoch_loss:.4f}\\")\\n        print(\\"Training complete.\\")\\n\\n    def segment_single_image(self, image_np: np.ndarray) -> np.ndarray:\\n        \\"\\"\\"\\n        Predicts the segmentation mask for a single input image.\\n\\n        Args:\\n            image_np: A NumPy array representing the input image (H, W, C).\\n\\n        Returns:\\n            A NumPy array of the predicted segmentation mask (H, W) with 1-indexed class IDs.\\n        \\"\\"\\"\\n        self.model.eval() # Set model to evaluation mode\\n\\n        # Preprocess the input image for inference:\\n        # 1. Apply the same resizing and type conversion as during training.\\n        #    CustomResizeAndToTensor expects both image and mask, so pass a dummy mask.\\n        dummy_mask = np.zeros(image_np.shape[:2], dtype=np.uint8)\\n        processed_image_np, _ = self.transform(image_np, dummy_mask)\\n\\n        # 2. Convert to tensor, permute dimensions (HWC -> CHW), and normalize.\\n        image_tensor = torch.from_numpy(processed_image_np).float().permute(2, 0, 1)\\n        image_tensor = image_tensor / 255.0\\n\\n        # Standardize with ImageNet stats\\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\\n        image_tensor = (image_tensor - mean) / std\\n\\n        # Add a batch dimension (B, C, H, W) and move to device\\n        image_tensor = image_tensor.unsqueeze(0).to(self.device)\\n\\n        with torch.no_grad(): # Disable gradient calculations during inference\\n            outputs = self.model(image_tensor)\\n        \\n        # Get predicted class indices (0-indexed) by taking argmax along the channel dimension\\n        # outputs shape: (1, num_classes, H, W) -> predicted_mask_0_indexed shape: (H, W)\\n        predicted_mask_0_indexed = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy()\\n\\n        # Convert back to 1-indexed (1 to 17) for compatibility with evaluation function\\n        predicted_mask_1_indexed = predicted_mask_0_indexed + 1\\n\\n        # The model's output and original images are 256x256, so no resizing back to original size is needed.\\n        # This check ensures consistency if image sizes ever varied:\\n        # original_H, original_W = image_np.shape[:2]\\n        # if predicted_mask_1_indexed.shape[0] != original_H or predicted_mask_1_indexed.shape[1] != original_W:\\n        #     predicted_mask_pil = Image.fromarray(predicted_mask_1_indexed.astype(np.uint8), mode='L')\\n        #     predicted_mask_resized = predicted_mask_pil.resize((original_W, original_H), Image.NEAREST)\\n        #     predicted_mask_1_indexed = np.array(predicted_mask_resized)\\n\\n        return predicted_mask_1_indexed\\n\\n\\n# Instantiate the model\\n# The model variable is expected by the evaluation script.\\nmodel = SemanticSegmentationModel(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE)\\n\\n# Train the model using the entire training_data_paths list.\\n# The boilerplate \`for image, label_mask in training_data_paths:\` loop\\n# is not suitable for typical deep learning model training (which processes\\n# batches via DataLoaders). So, we train the model comprehensively once here.\\n# num_epochs is set to 5 for quicker initial validation on Kaggle, can be increased.\\n# batch_size is set to 8 to be conservative with GPU memory.\\nmodel.train(training_data_paths, num_epochs=5, batch_size=8)\\n\\n# The following loop is part of the original notebook structure.\\n# Since the model is already trained, this loop will execute once and break.\\nfor image_path, label_path in training_data_paths:\\n  # The model has already been trained above. This loop is effectively a no-op now.\\n  break",
  "new_index": 11,
  "new_code": "# TODO:  1. Feel free to pip install and import any useful libraries within the\\n#  rewrite cell.\\n\\n# Feel free to pip install and import any useful libraries within the rewrite cell.\\n# The \`segmentation-models-pytorch\` and \`timm\` libraries are already installed.\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nimport segmentation_models_pytorch as smp\\nimport numpy as np\\nfrom PIL import Image\\nimport os\\nfrom tqdm import tqdm # For progress bars\\n\\n# Define constants for the model\\nNUM_CLASSES = 17 # As specified in the problem description (airplane, bare-soil, ..., water)\\nIMAGE_SIZE = 256 # Images are 256x256 pixels as specified for UC Merced Land Use Dataset\\n\\n# --- Custom Transform for both Image and Mask ---\\nclass CustomResizeAndToTensor:\\n    \\"\\"\\"\\n    A custom transformation that resizes an image and its corresponding mask,\\n    then converts them to PyTorch tensors.\\n    \\"\\"\\"\\n    def __init__(self, size):\\n        self.size = size\\n        # Use BILINEAR for images to maintain quality, NEAREST for masks to preserve class integrity\\n        self.resize_image = transforms.Resize(size, Image.BILINEAR)\\n        self.resize_mask = transforms.Resize(size, Image.NEAREST)\\n\\n    def __call__(self, image_np, mask_np):\\n        # Convert numpy arrays to PIL Images for torchvision.transforms\\n        image_pil = Image.fromarray(image_np).convert(\\"RGB\\") # Ensure 3 channels\\n        mask_pil = Image.fromarray(mask_np.astype(np.uint8), mode='L') # Ensure 8-bit grayscale for mask\\n\\n        # Apply resizing\\n        image_resized_pil = self.resize_image(image_pil)\\n        mask_resized_pil = self.resize_mask(mask_pil)\\n\\n        # Convert back to numpy arrays\\n        image_resized_np = np.array(image_resized_pil)\\n        mask_resized_np = np.array(mask_resized_pil)\\n\\n        return image_resized_np, mask_resized_np\\n\\n\\n# --- Dataset Class ---\\nclass DLRSDDataset(Dataset):\\n    \\"\\"\\"\\n    PyTorch Dataset for loading DLRSD images and corresponding masks.\\n    \\"\\"\\"\\n    def __init__(self, data_paths, transform=None):\\n        self.data_paths = data_paths\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.data_paths)\\n\\n    def __getitem__(self, idx):\\n        image_path, label_path = self.data_paths[idx]\\n\\n        # Load image and label\\n        image = Image.open(image_path).convert(\\"RGB\\")\\n        label = Image.open(label_path).convert(\\"L\\") # Load as grayscale (8-bit pixels)\\n\\n        image_np = np.array(image)\\n        label_np = np.array(label)\\n\\n        # Apply transformations if provided\\n        if self.transform:\\n            image_np, label_np = self.transform(image_np, label_np)\\n\\n        # Labels are 1-indexed (1 to 17) in the dataset,\\n        # but PyTorch's CrossEntropyLoss expects 0-indexed (0 to 16).\\n        # We subtract 1 here. The evaluation function \`calculate_mean_iou\` expects 1-indexed.\\n        label_np = label_np - 1\\n\\n        # Convert to torch tensors\\n        # For images: (H, W, C) -> (C, H, W) and normalize\\n        image_tensor = torch.from_numpy(image_np).float().permute(2, 0, 1)\\n        image_tensor = image_tensor / 255.0 # Scale to [0, 1]\\n\\n        # Standardize with ImageNet means and stds for pre-trained encoder\\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\\n        image_tensor = (image_tensor - mean) / std\\n\\n        # For labels: ensure it's a long tensor for CrossEntropyLoss\\n        label_tensor = torch.from_numpy(label_np).long()\\n\\n        return image_tensor, label_tensor\\n\\n\\n# --- Main Model Class (Wrapper for SMP U-Net) ---\\nclass SemanticSegmentationModel:\\n    \\"\\"\\"\\n    Encapsulates the semantic segmentation model, training, and prediction logic.\\n    \\"\\"\\"\\n    def __init__(self, num_classes=NUM_CLASSES, image_size=IMAGE_SIZE):\\n        self.num_classes = num_classes\\n        self.image_size = image_size\\n        self.device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n        # Initialize U-Net model from segmentation_models_pytorch\\n        # Using efficientnet-b0 as a common, efficient pre-trained backbone\\n        self.model = smp.Unet(\\n            encoder_name=\\"efficientnet-b0\\",\\n            encoder_weights=\\"imagenet\\", # Use pre-trained weights\\n            in_channels=3, # RGB images\\n            classes=self.num_classes, # Number of output classes\\n        )\\n        self.model.to(self.device) # Move model to GPU/CPU\\n\\n        # Initialize the image/mask transform\\n        self.transform = CustomResizeAndToTensor(self.image_size)\\n\\n    def train(self, training_data_paths, num_epochs=10, batch_size=16, learning_rate=1e-3):\\n        \\"\\"\\"\\n        Trains the segmentation model using the provided data paths.\\n        \\"\\"\\"\\n        train_dataset = DLRSDDataset(training_data_paths, transform=self.transform)\\n        # Using fewer workers for potentially lower memory consumption and better compatibility\\n        # on various systems. Half of CPU count, minimum 1.\\n        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count() // 2 or 1)\\n\\n        loss_fn = nn.CrossEntropyLoss() # Standard loss for multi-class pixel classification\\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\\n\\n        self.model.train() # Set model to training mode\\n\\n        print(f\\"Training model on {len(training_data_paths)} images for {num_epochs} epochs...\\")\\n        for epoch in range(num_epochs):\\n            running_loss = 0.0\\n            # Use tqdm for a progress bar during training\\n            for images, masks in tqdm(train_dataloader, desc=f\\"Epoch {epoch+1}/{num_epochs}\\"):\\n                images = images.to(self.device)\\n                masks = masks.to(self.device)\\n\\n                optimizer.zero_grad() # Clear gradients\\n                outputs = self.model(images) # Forward pass\\n                loss = loss_fn(outputs, masks) # Compute loss\\n                loss.backward() # Backward pass (compute gradients)\\n                optimizer.step() # Update model weights\\n\\n                running_loss += loss.item() * images.size(0)\\n\\n            epoch_loss = running_loss / len(train_dataset)\\n            print(f\\"Epoch {epoch+1} completed. Loss: {epoch_loss:.4f}\\")\\n        print(\\"Training complete.\\")\\n\\n    def segment_single_image(self, image_np: np.ndarray) -> np.ndarray:\\n        \\"\\"\\"\\n        Predicts the segmentation mask for a single input image.\\n\\n        Args:\\n            image_np: A NumPy array representing the input image (H, W, C).\\n\\n        Returns:\\n            A NumPy array of the predicted segmentation mask (H, W) with 1-indexed class IDs.\\n        \\"\\"\\"\\n        self.model.eval() # Set model to evaluation mode\\n\\n        # Preprocess the input image for inference:\\n        # 1. Apply the same resizing and type conversion as during training.\\n        #    CustomResizeAndToTensor expects both image and mask, so pass a dummy mask.\\n        dummy_mask = np.zeros(image_np.shape[:2], dtype=np.uint8)\\n        processed_image_np, _ = self.transform(image_np, dummy_mask)\\n\\n        # 2. Convert to tensor, permute dimensions (HWC -> CHW), and normalize.\\n        image_tensor = torch.from_numpy(processed_image_np).float().permute(2, 0, 1)\\n        image_tensor = image_tensor / 255.0\\n\\n        # Standardize with ImageNet stats\\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\\n        image_tensor = (image_tensor - mean) / std\\n\\n        # Add a batch dimension (B, C, H, W) and move to device\\n        image_tensor = image_tensor.unsqueeze(0).to(self.device)\\n\\n        with torch.no_grad(): # Disable gradient calculations during inference\\n            outputs = self.model(image_tensor)\\n        \\n        # Get predicted class indices (0-indexed) by taking argmax along the channel dimension\\n        # outputs shape: (1, num_classes, H, W) -> predicted_mask_0_indexed shape: (H, W)\\n        predicted_mask_0_indexed = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy()\\n\\n        # Convert back to 1-indexed (1 to 17) for compatibility with evaluation function\\n        predicted_mask_1_indexed = predicted_mask_0_indexed + 1\\n\\n        # The model's output and original images are 256x256, so no resizing back to original size is needed.\\n        # This check ensures consistency if image sizes ever varied:\\n        # original_H, original_W = image_np.shape[:2]\\n        # if predicted_mask_1_indexed.shape[0] != original_H or predicted_mask_1_indexed.shape[1] != original_W:\\n        #     predicted_mask_pil = Image.fromarray(predicted_mask_1_indexed.astype(np.uint8), mode='L')\\n        #     predicted_mask_resized = predicted_mask_pil.resize((original_W, original_H), Image.NEAREST)\\n        #     predicted_mask_1_indexed = np.array(predicted_mask_resized)\\n\\n        return predicted_mask_1_indexed\\n\\n\\n# Instantiate the model\\n# The model variable is expected by the evaluation script.\\nmodel = SemanticSegmentationModel(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE)\\n\\n# Train the model using the entire training_data_paths list.\\n# The boilerplate \`for image, label_mask in training_data_paths:\` loop\\n# is not suitable for typical deep learning model training (which processes\\n# # batches via DataLoaders). So, we train the model comprehensively once here.\\n# num_epochs is set to 5 for quicker initial validation on Kaggle, can be increased.\\n# batch_size is set to 8 to be conservative with GPU memory.\\n# REFINEMENT: Reduced batch_size to 4 and increased num_epochs to 10 for stability and more training.\\nmodel.train(training_data_paths, num_epochs=10, batch_size=4)\\n\\n# The following loop is part of the original notebook structure.\\n# Since the model is already trained, this loop will execute once and break.\\nfor image_path, label_path in training_data_paths:\\n  # The model has already been trained above. This loop is effectively a no-op now.\\n  break"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

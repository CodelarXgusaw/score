<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 866.0,
  "old_code": "# TODO:  1. Feel free to pip install and import any useful libraries within the\\n#  rewrite cell.\\n\\n# TODO:  1. Feel free to pip install and import any useful libraries within the\\n#  rewrite cell.\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.optim import lr_scheduler\\nfrom torch.utils.data import Dataset, DataLoader\\nimport segmentation_models_pytorch as smp\\nimport numpy as np\\nfrom PIL import Image\\nimport os\\nimport albumentations as A\\nfrom albumentations.pytorch import ToTensorV2\\nfrom torch.amp import GradScaler, autocast\\n\\n# Constants for model training\\nNUM_CLASSES = 17\\nIMG_SIZE = 256 # DLRSD images are 256x256\\nBATCH_SIZE = 16 # Adjust based on GPU memory\\nLEARNING_RATE = 1e-4\\nEPOCHS = 350 # MODIFICATION: Reverted epochs to 350 after the previous increase showed a slight drop in performance.\\n\\n# Device configuration (CUDA if available, else CPU)\\ndevice = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n# Custom Dataset Class\\nclass DLRSDDataset(Dataset):\\n    def __init__(self, data_paths, transform=None):\\n        self.data_paths = data_paths\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.data_paths)\\n\\n    def __getitem__(self, idx):\\n        image_path, label_path = self.data_paths[idx]\\n\\n        # Load image and ensure 3 channels (RGB)\\n        image = np.array(Image.open(image_path).convert(\\"RGB\\"))\\n        \\n        # Load label mask. Labels are 1-indexed (1 to 17) in the dataset.\\n        # PyTorch's CrossEntropyLoss expects 0-indexed targets (0 to 16).\\n        label_mask = np.array(Image.open(label_path), dtype=np.int64) - 1\\n\\n        if self.transform:\\n            # Albumentations expects images as HWC and masks as HW\\n            transformed = self.transform(image=image, mask=label_mask)\\n            image = transformed[\\"image\\"]\\n            label_mask = transformed[\\"mask\\"]\\n\\n        return image, label_mask\\n\\n# Define transformation pipelines for training and validation\\ntrain_transform = A.Compose([\\n    A.Resize(IMG_SIZE, IMG_SIZE),\\n    A.HorizontalFlip(p=0.75), # MODIFICATION: Increased flip probability\\n    A.VerticalFlip(p=0.75),   # MODIFICATION: Increased flip probability\\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.10, rotate_limit=20, p=0.5), # MODIFICATION: Reverted rotate_limit from 30 to 20\\n    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5), # MODIFICATION: Increased brightness and contrast limits\\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n    ToTensorV2(),\\n])\\n\\nval_transform = A.Compose([\\n    A.Resize(IMG_SIZE, IMG_SIZE),\\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n    ToTensorV2(),\\n])\\n\\n# Main Model Wrapper Class\\nclass SemanticSegmentationModel:\\n    def __init__(self, num_classes=NUM_CLASSES):\\n        # Initialize U-Net model with a pre-trained ResNeXt101 encoder (MODIFICATION: Upgraded encoder)\\n        self.model = smp.Unet(\\n            encoder_name=\\"se_resnext101_32x4d\\", # MODIFICATION\\n            encoder_weights=\\"imagenet\\",\\n            in_channels=3,\\n            classes=num_classes,\\n            decoder_dropout=0.05, # MODIFICATION: Adjusted decoder_dropout for regularization (from 0.1 to 0.05)\\n        )\\n        self.model.to(device)\\n        # Switched to AdamW optimizer for better performance with weight decay\\n        # MODIFICATION: Reverted weight_decay from 5e-5 to 1e-4\\n        # NEW MODIFICATION: Adjusted weight_decay from 1e-4 to 7.5e-5\\n        # CURRENT MODIFICATION: Reverted weight_decay from 7.5e-5 to 1e-4\\n        self.optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4) # MODIFIED\\n        \\n        # Define combined loss function: 60% Dice Loss + 40% Cross-Entropy Loss (MODIFICATION)\\n        dice_loss = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\\n        ce_loss = nn.CrossEntropyLoss()\\n        \\n        # Wrapper function for the combined loss\\n        def combined_loss(y_pred, y_true):\\n            return 0.6 * dice_loss(y_pred, y_true) + 0.4 * ce_loss(y_pred, y_true) # MODIFICATION\\n        \\n        self.criterion = combined_loss\\n        \\n        # Initialize learning rate scheduler, added min_lr\\n        # MODIFICATION: Decreased patience from 10 to 7\\n        # PREVIOUS MODIFICATION: Changed factor from 0.5 to 0.7 for less aggressive decay\\n        # CURRENT MODIFICATION: Reverted factor from 0.7 to 0.5 for more aggressive decay\\n        # NEW MODIFICATION: Reverted patience from 7 to 10\\n        # MODIFICATION FOR CURRENT ITERATION: Decreased patience from 10 to 7 to make decay more responsive.\\n        self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7) # MODIFIED\\n        \\n        # Initialize GradScaler for mixed precision training\\n        self.scaler = GradScaler('cuda')\\n\\n    def train_model(self, training_data_paths, epochs=EPOCHS):\\n        \\"\\"\\"\\n        Trains the segmentation model using the provided training data paths.\\n        \\"\\"\\"\\n        train_dataset = DLRSDDataset(training_data_paths, transform=train_transform)\\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) # MODIFICATION: Adjusted num_workers from 8 to 4\\n\\n        print(f\\"Starting training for {epochs} epochs on {device}...\\")\\n        self.model.train()\\n\\n        for epoch in range(epochs):\\n            total_loss = 0\\n            for batch_idx, (images, masks) in enumerate(train_loader):\\n                images = images.to(device)\\n                masks = masks.to(device)\\n                # Ensure masks are of type torch.long for CrossEntropyLoss\\n                masks = masks.long() \\n\\n                self.optimizer.zero_grad()\\n                \\n                # Forward pass with mixed precision\\n                with autocast('cuda'):\\n                    outputs = self.model(images)\\n                    # Calculate loss\\n                    loss = self.criterion(outputs, masks)\\n                \\n                # Backward pass and optimization with gradient scaling\\n                self.scaler.scale(loss).backward()\\n                self.scaler.step(self.optimizer)\\n                self.scaler.update()\\n\\n                total_loss += loss.item()\\n\\n            avg_loss = total_loss / len(train_loader)\\n            print(f\\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\\")\\n            \\n            self.scheduler.step(avg_loss)\\n\\n        print(\\"Training complete.\\")\\n\\n    def segment_single_image(self, image: np.ndarray) -> np.ndarray:\\n        \\"\\"\\"\\n        Predicts the segmentation mask for a single input image using Test-Time Augmentation (TTA).\\n\\n        Args:\\n            image: A NumPy array representing the input image (H, W, C).\\n\\n        Returns:\\n            A NumPy array representing the predicted segmentation mask (H, W),\\n            with pixel values corresponding to 1-indexed class IDs (1 to 17).\\n        \\"\\"\\"\\n        self.model.eval()\\n        with torch.no_grad():\\n            # List to store predictions (logits) from different augmentations\\n            all_logits = []\\n\\n            # Prepare original image tensor\\n            transformed_original = val_transform(image=image)\\n            image_tensor_original = transformed_original[\\"image\\"].unsqueeze(0).to(device) # Shape: (1, C, H, W)\\n\\n            # 1. Original image\\n            with autocast('cuda'):\\n                logits_original = self.model(image_tensor_original)\\n            all_logits.append(logits_original)\\n\\n            # 2. Horizontal flip\\n            # Apply flip on image (NumPy) then re-transform\\n            image_hflip = A.HorizontalFlip(p=1.0)(image=image)[\\"image\\"]\\n            transformed_hflip = val_transform(image=image_hflip)\\n            image_tensor_hflip = transformed_hflip[\\"image\\"].unsqueeze(0).to(device)\\n            with autocast('cuda'):\\n                logits_hflip = self.model(image_tensor_hflip)\\n            # Reverse horizontal flip on logits (flip along width dimension, which is 3)\\n            all_logits.append(torch.flip(logits_hflip, dims=[3]))\\n\\n            # 3. Vertical flip\\n            # Apply flip on image (NumPy) then re-transform\\n            image_vflip = A.VerticalFlip(p=1.0)(image=image)[\\"image\\"]\\n            transformed_vflip = val_transform(image=image_vflip)\\n            image_tensor_vflip = transformed_vflip[\\"image\\"].unsqueeze(0).to(device)\\n            with autocast('cuda'):\\n                logits_vflip = self.model(image_tensor_vflip)\\n            # Reverse vertical flip on logits (flip along height dimension, which is 2)\\n            all_logits.append(torch.flip(logits_vflip, dims=[2]))\\n\\n            # 4. Horizontal + Vertical flip\\n            # Apply both flips on image (NumPy) then re-transform\\n            image_hvflip = A.Compose([A.HorizontalFlip(p=1.0), A.VerticalFlip(p=1.0)])(image=image)[\\"image\\"]\\n            transformed_hvflip = val_transform(image=image_hvflip)\\n            image_tensor_hvflip = transformed_hvflip[\\"image\\"].unsqueeze(0).to(device)\\n            with autocast('cuda'):\\n                logits_hvflip = self.model(image_tensor_hvflip)\\n            # Reverse both flips on logits\\n            all_logits.append(torch.flip(torch.flip(logits_hvflip, dims=[3]), dims=[2]))\\n\\n            # --- NEW ADDITIONS: Rotations (k=1 for 90, k=2 for 180, k=3 for 270 degrees clockwise) ---\\n            # For these, it's easier to apply torch.rot90 directly on the tensor\\n            # 5. Rotate 90 degrees clockwise\\n            image_tensor_rot90 = torch.rot90(image_tensor_original, k=1, dims=[2, 3])\\n            with autocast('cuda'):\\n                logits_rot90 = self.model(image_tensor_rot90)\\n            all_logits.append(torch.rot90(logits_rot90, k=-1, dims=[2, 3])) # Revert 90 deg counter-clockwise\\n\\n            # 6. Rotate 180 degrees\\n            image_tensor_rot180 = torch.rot90(image_tensor_original, k=2, dims=[2, 3])\\n            with autocast('cuda'):\\n                logits_rot180 = self.model(image_tensor_rot180)\\n            all_logits.append(torch.rot90(logits_rot180, k=-2, dims=[2, 3])) # Revert 180 deg\\n\\n            # 7. Rotate 270 degrees clockwise\\n            image_tensor_rot270 = torch.rot90(image_tensor_original, k=3, dims=[2, 3])\\n            with autocast('cuda'):\\n                logits_rot270 = self.model(image_tensor_rot270)\\n            all_logits.append(torch.rot90(logits_rot270, k=-3, dims=[2, 3])) # Revert 270 deg counter-clockwise\\n\\n            # Average the logits from all augmented predictions\\n            averaged_logits = torch.mean(torch.stack(all_logits), dim=0)\\n\\n            # Get class with highest probability for each pixel\\n            predicted_mask = torch.argmax(averaged_logits, dim=1).squeeze(0)\\n\\n            # Convert to NumPy array and adjust to 1-indexed class IDs (0-indexed to 1-indexed)\\n            pred_mask_np = predicted_mask.cpu().numpy() + 1\\n\\n        return pred_mask_np\\n\\n# Initialize and train the model\\nmodel = SemanticSegmentationModel()\\nmodel.train_model(training_data_paths)",
  "new_index": 881,
  "new_code": "import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.optim import lr_scheduler\\nfrom torch.utils.data import Dataset, DataLoader\\nimport numpy as np\\nfrom PIL import Image\\nimport os\\nimport albumentations as A\\n# No ToTensorV2 import if AutoImageProcessor handles it fully for images\\n\\n# --- NEW IMPORTS FOR SEGFORMER ---\\nfrom transformers import SegformerForSemanticSegmentation, AutoImageProcessor\\n# --- END NEW IMPORTS ---\\n\\n# Constants for model training\\nNUM_CLASSES = 17\\nIMG_SIZE = 256 # DLRSD images are 256x256\\nBATCH_SIZE = 8 # MODIFICATION: Reduced batch size for SegFormer as it's more memory intensive\\nLEARNING_RATE = 5e-5 # MODIFICATION: Adjusted LR for SegFormer, usually lower\\nEPOCHS = 100 # MODIFICATION: Reduced epochs for initial exploration with new model, to quickly test viability\\n\\n# Device configuration (CUDA if available, else CPU)\\ndevice = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n# Custom Dataset Class\\nclass DLRSDDataset(Dataset):\\n    def __init__(self, data_paths, transform=None, image_processor=None):\\n        self.data_paths = data_paths\\n        self.transform = transform\\n        self.image_processor = image_processor\\n\\n    def __len__(self):\\n        return len(self.data_paths)\\n\\n    def __getitem__(self, idx):\\n        image_path, label_path = self.data_paths[idx]\\n\\n        # Load image (HWC) and ensure 3 channels (RGB)\\n        image = np.array(Image.open(image_path).convert(\\"RGB\\"))\\n        \\n        # Load label mask (HW). Labels are 1-indexed (1 to 17) in the dataset.\\n        # PyTorch's CrossEntropyLoss expects 0-indexed targets (0 to 16).\\n        label_mask = np.array(Image.open(label_path), dtype=np.int64) - 1\\n\\n        if self.transform:\\n            # Albumentations expects images as HWC and masks as HW.\\n            # It applies data augmentations and resizing.\\n            transformed = self.transform(image=image, mask=label_mask)\\n            image = transformed[\\"image\\"] # This is now augmented/resized NumPy array (HWC)\\n            label_mask = transformed[\\"mask\\"] # This is now augmented/resized NumPy array (HW)\\n        \\n        # The HuggingFace image_processor takes NumPy arrays (HWC) and converts them to tensors (CHW),\\n        # applying normalization and potentially further resizing based on its 'size' configuration.\\n        if self.image_processor:\\n            # \`image\` is still a NumPy array (HWC) here\\n            # \`image_processor\` will return pixel_values as a tensor (1, C, H, W)\\n            processed_inputs = self.image_processor(images=image, return_tensors=\\"pt\\")\\n            image_tensor = processed_inputs.pixel_values.squeeze(0) # Remove batch dimension\\n\\n            # Ensure label_mask is also a tensor. It's already 0-indexed.\\n            label_mask_tensor = torch.from_numpy(label_mask).long()\\n\\n            return image_tensor, label_mask_tensor\\n        else:\\n            # Fallback if no image_processor (though model requires it)\\n            # This path is typically not taken with the current setup.\\n            return torch.from_numpy(image).permute(2,0,1).float(), torch.from_numpy(label_mask).long()\\n\\n\\n# Define transformation pipelines for training and validation\\n# Albumentations for augmentations and initial resizing (on numpy arrays),\\n# without normalization or ToTensorV2, as AutoImageProcessor handles those.\\ntrain_transform = A.Compose([\\n    A.Resize(IMG_SIZE, IMG_SIZE), # Ensure consistent size before processor\\n    A.HorizontalFlip(p=0.75),\\n    A.VerticalFlip(p=0.75),\\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.10, rotate_limit=20, p=0.5),\\n    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5),\\n])\\n\\nval_transform = A.Compose([\\n    A.Resize(IMG_SIZE, IMG_SIZE), # Ensure consistent size before processor\\n])\\n\\n\\n# Main Model Wrapper Class\\nclass SemanticSegmentationModel:\\n    def __init__(self, num_classes=NUM_CLASSES):\\n        self.model_name = \\"nvidia/mit-b0\\" # Lightweight ViT backbone\\n        self.image_processor = AutoImageProcessor.from_pretrained(self.model_name)\\n        \\n        # Override the image processor's default size to match IMG_SIZE (256x256)\\n        # This ensures the images fed to the model are 256x256 after processing.\\n        # This is critical if the model was not pre-trained for 512x512 inputs and we want 256.\\n        self.image_processor.size = {\\"height\\": IMG_SIZE, \\"width\\": IMG_SIZE}\\n\\n        self.model = SegformerForSemanticSegmentation.from_pretrained(\\n            self.model_name,\\n            num_labels=num_classes,\\n            ignore_mismatched_sizes=True # Needed if head dimension changes\\n        )\\n        self.model.to(device)\\n        \\n        # Reverted to AdamW optimizer for consistency, and keeping weight_decay as before.\\n        self.optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\\n        \\n        # SegFormerForSemanticSegmentation has a built-in loss calculation (CrossEntropyLoss)\\n        # when \`labels\` are passed to its \`forward\` method.\\n        # So, we don't explicitly define \`self.criterion\` here for the loss that SegFormer calculates.\\n        # However, for training loop simplicity, we want to retrieve and print it.\\n        # The \`outputs.loss\` will be the CrossEntropyLoss.\\n\\n        self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-7)\\n        self.scaler = GradScaler('cuda')\\n\\n    def train_model(self, training_data_paths, epochs=EPOCHS):\\n        \\"\\"\\"\\n        Trains the segmentation model using the provided training data paths.\\n        \\"\\"\\"\\n        train_dataset = DLRSDDataset(training_data_paths, transform=train_transform, image_processor=self.image_processor)\\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\\n\\n        print(f\\"Starting training for {epochs} epochs on {device}...\\")\\n        self.model.train()\\n\\n        for epoch in range(epochs):\\n            total_loss = 0\\n            for batch_idx, (images, masks) in enumerate(train_loader):\\n                images = images.to(device) # C, H, W tensor\\n                masks = masks.to(device) # H, W tensor (long)\\n\\n                self.optimizer.zero_grad()\\n                \\n                with autocast('cuda'):\\n                    # SegFormer's forward pass expects pixel_values (input image tensor)\\n                    # and labels (ground truth masks) for loss calculation.\\n                    # It returns a dictionary including 'loss' and 'logits'.\\n                    outputs = self.model(pixel_values=images, labels=masks)\\n                    loss = outputs.loss # SegFormer computes loss internally\\n                \\n                self.scaler.scale(loss).backward()\\n                self.scaler.step(self.optimizer)\\n                self.scaler.update()\\n\\n                total_loss += loss.item()\\n\\n            avg_loss = total_loss / len(train_loader)\\n            print(f\\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\\")\\n            \\n            # Use outputs.loss for scheduler step, as it represents the training loss\\n            self.scheduler.step(avg_loss)\\n\\n        print(\\"Training complete.\\")\\n\\n    def segment_single_image(self, image: np.ndarray) -> np.ndarray:\\n        \\"\\"\\"\\n        Predicts the segmentation mask for a single input image using HuggingFace's AutoImageProcessor.\\n        Simplified TTA for initial exploration (only original image prediction).\\n\\n        Args:\\n            image: A NumPy array representing the input image (H, W, C).\\n\\n        Returns:\\n            A NumPy array representing the predicted segmentation mask (H, W),\\n            with pixel values corresponding to 1-indexed class IDs (1 to 17).\\n        \\"\\"\\"\\n        self.model.eval()\\n        with torch.no_grad():\\n            # Apply base val_transform (resizing, but no augs, no normalize, no to_tensor)\\n            transformed = val_transform(image=image)\\n            image_np_augmented = transformed[\\"image\\"] # This is resized NumPy array (HWC)\\n            \\n            # Use the image processor for final normalization and tensor conversion.\\n            # This handles: resizing to image_processor.size (256x256), normalization, and ToTensor.\\n            inputs = self.image_processor(images=image_np_augmented, return_tensors=\\"pt\\")\\n            image_tensor = inputs.pixel_values.to(device) # Shape: (1, C, H, W)\\n\\n            with autocast('cuda'):\\n                # SegFormer takes pixel_values directly for inference\\n                outputs = self.model(pixel_values=image_tensor)\\n            \\n            # SegFormer returns logits. Logits shape: (Batch_size, Num_classes, H_out, W_out)\\n            # For \`mit-b0\` (and other SegFormer variants), the output resolution \`(H_out, W_out)\` is\\n            # typically 1/4th or 1/8th of the input resolution.\\n            # We need to interpolate these logits back to the original \`IMG_SIZE\` (256x256).\\n            logits = nn.functional.interpolate(\\n                outputs.logits,\\n                size=(IMG_SIZE, IMG_SIZE),\\n                mode='bilinear',\\n                align_corners=False # Set to False for non-feature map interpolation (standard practice)\\n            )\\n            predicted_mask = torch.argmax(logits, dim=1).squeeze(0)\\n\\n            # Convert to NumPy array and adjust to 1-indexed class IDs (0-indexed to 1-indexed)\\n            pred_mask_np = predicted_mask.cpu().numpy() + 1\\n\\n        return pred_mask_np\\n\\n# Initialize and train the model\\nmodel = SemanticSegmentationModel()\\nmodel.train_model(training_data_paths)"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

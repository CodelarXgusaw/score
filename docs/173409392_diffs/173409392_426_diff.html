<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 421.0,
  "old_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\n\\n# Custom Learning Rate Scheduler for Cosine Annealing with Warm Restarts\\nclass CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n    def __init__(self, initial_lr, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0):\\n        super().__init__()\\n        self.initial_lr = initial_lr\\n        self.first_decay_steps = first_decay_steps\\n        self.t_mul = t_mul\\n        self.m_mul = m_mul\\n        self.alpha = alpha\\n        self.current_cycle_steps = 0\\n        self.cycle_length = float(first_decay_steps)\\n        self.lrs = [] # To store learning rates per epoch\\n\\n    def on_epoch_begin(self, epoch, logs=None):\\n        if not hasattr(self.model, 'optimizer') or self.model.optimizer is None:\\n            raise ValueError('Model optimizer is not set. Ensure model.compile() is called before fit().')\\n        \\n        # Access the learning_rate attribute, which is a tf.Variable for Adam\\n        # This is more robust than 'lr' which might be an alias or not always present.\\n        if not hasattr(self.model.optimizer, 'learning_rate'):\\n            raise ValueError('Optimizer must have a \\"learning_rate\\" attribute.')\\n\\n        # Calculate the learning rate for the current epoch\\n        if epoch == 0:\\n            current_lr = self.initial_lr\\n        else:\\n            # Check if we finished a cycle and need to restart\\n            if (epoch - self.current_cycle_steps) >= self.cycle_length:\\n                self.current_cycle_steps += self.cycle_length\\n                self.cycle_length *= self.t_mul\\n                self.initial_lr *= self.m_mul # Update initial_lr for the new cycle\\n            \\n            # Calculate progress within the current cycle\\n            progress_in_cycle = (epoch - self.current_cycle_steps) / self.cycle_length\\n            \\n            # Cosine annealing formula\\n            cosine_decay = 0.5 * (1 + np.cos(np.pi * progress_in_cycle))\\n            current_lr = (self.initial_lr - self.initial_lr * self.alpha) * cosine_decay + self.initial_lr * self.alpha\\n        \\n        # Set the optimizer's learning rate using assign for tf.Variable\\n        self.model.optimizer.learning_rate.assign(current_lr)\\n        self.lrs.append(current_lr)\\n        # print(f\\"Epoch {epoch+1}: Learning rate set to {current_lr:.6f}\\") # For debugging\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Consolidates temporal feature extraction using a 1D CNN for the short context.\\n    - Relies on 1D CNN temporal features, enhanced rolling window statistics, a learned global brain state,\\n      neuron ID embeddings, DELTA FEATURES, AND NOW RAW CONTEXT FEATURES to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n        # 1D CNN layer to process the raw temporal context for each neuron\\n        self.conv1d_context_processor = keras.layers.Conv1D(\\n            filters=64, # Increased output filters for 1D convolution\\n            kernel_size=self.NUM_TIMESTEPS_CONTEXT, # Use kernel size equal to context length to capture full window\\n            activation='relu',\\n            name='conv1d_context_processor_in_extractor',\\n        )\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A single tensor combining all extracted features, shaped (B*N, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Reshape for Conv1D input: (batch_size * num_neurons, timesteps_context, 1)\\n        raw_neuron_context_for_temporal_layer = tf.expand_dims(\\n            tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT]), axis=-1\\n        ) # Shape: (B*N, C, 1)\\n\\n        # Process raw context with 1D CNN\\n        # input shape for conv1d_context_processor will be (B*N, C, 1)\\n        temporal_conv_features = self.conv1d_context_processor(raw_neuron_context_for_temporal_layer) # Shape: (B*N, 1, filters)\\n        # Squeeze the redundant dimension, as kernel_size=C effectively performs pooling\\n        processed_temporal_features = tf.squeeze(temporal_conv_features, axis=1) # Shape: (B*N, filters)\\n\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data, axis=2) # Sort along the context dimension\\n        # Note: For C=4, indices are 0, 1, 2, 3. Middle two are at index 1 and 2.\\n        # This is (C/2 - 1) and (C/2).\\n        neuron_medians = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0 # Shape (B, N)\\n        neuron_medians = tf.expand_dims(neuron_medians, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n        # Calculate delta (difference) features for the context window\\n        # For C timesteps, there are C-1 delta values: (t1-t0), (t2-t1), ..., (t_C-1 - t_C-2)\\n        neuron_deltas = raw_neuron_data[:, :, 1:] - raw_neuron_data[:, :, :-1] # Shape: (B, N, C-1)\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n        neuron_deltas_flat = tf.reshape(neuron_deltas, [-1, self.NUM_TIMESTEPS_CONTEXT - 1]) # Shape: (B*N, C-1)\\n\\n        # NEW: Flatten the raw context data for direct inclusion as features\\n        raw_context_flat = tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT]) # Shape: (B*N, C)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Combine all features (1D CNN temporal features, statistics, DELTA FEATURES, RAW CONTEXT, global context, neuron embedding)\\n        combined_features_for_mlp = tf.concat([\\n            processed_temporal_features, # (B*N, 64) - Temporal features from 1D CNN\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            neuron_deltas_flat,      # (B*N, C-1) - Delta features\\n            raw_context_flat,        # (B*N, C) - NEW: Raw context features\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 64 + 8 + (C-1) + C + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return combined_features_for_mlp\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model now uses a refined FeatureExtractor that consolidates 1D CNN temporal processing\\n  alongside rich statistical, delta, and global brain state features, neuron ID embeddings,\\n  and now includes the raw context directly as features.\\n  The extracted features are processed by a deep residual network.\\n  Training is enhanced with a Cosine Annealing Warm Restarts learning rate schedule.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n\\n    # Initial batch normalization and projection layer\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    # This layer will now take the concatenated features directly from the FeatureExtractor.\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks - Adjusted dropout rate\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block4')\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='softplus', # Retained softplus activation\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the refined FeatureExtractor\\n    to provide all combined features.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate all combined features\\n    combined_features_for_mlp = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features_for_mlp, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    # Set num_epochs=1 for grain.IndexSampler, and tf.data.Dataset.from_generator handles\\n    # restarting the data source for each epoch provided to model.fit.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        # EarlyStopping remains\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Add Cosine Annealing Warm Restarts Learning Rate Callback\\n    initial_lr_for_schedule = 1e-3 # Starting LR for the first cycle\\n    first_decay_epochs = 10 # Length of the first LR cycle in epochs\\n    callbacks.append(CosineAnnealingWarmRestarts(\\n        initial_lr=initial_lr_for_schedule, \\n        first_decay_steps=first_decay_epochs, \\n        t_mul=2.0, # Double cycle length each restart\\n        m_mul=1.0 # Keep max LR constant each restart\\n    ))\\n\\n    # Compile the model with Adam optimizer and MAE loss.\\n    # The learning rate is now managed by the CosineAnnealingWarmRestarts callback.\\n    self.compile(optimizer=keras.optimizers.Adam(learning_rate=initial_lr_for_schedule), loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# Dynamically infer the necessary constants from the data source for model instantiation.\\nNUM_NEURONS_INFERRED = source[0]['series_input'].shape[1]\\nPREDICTION_WINDOW_LENGTH_INFERRED = constants.PREDICTION_WINDOW_LENGTH\\nNUM_TIMESTEPS_CONTEXT_INFERRED = num_timesteps_context\\n\\n# Define the number of epochs to train for model.fit\\nepochs_to_train_for_model_fit = 120\\n\\n# Setup train data loader: ensure num_epochs is 1 for consistent tf.data.Dataset generation per epoch\\nindex_sampler = grain.IndexSampler(\\n    num_records=len(source),\\n    num_epochs=num_epochs, # This should be 1 as defined globally initially\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=shuffle,\\n    seed=101\\n)\\n\\ndata_loader = grain.DataLoader(\\n    data_source=source,\\n    sampler=index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Setup validation data loader: ensure num_epochs is 1\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation split\\n  config_val = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Crucially, use 'val' split here\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config_val, prefetch=True))\\n\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_batch_size = batch_size # Use same batch size for validation\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # This should be 1\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101 # Consistent seed\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=val_batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\nprint(f'\\\\nValidation data loader created with {len(val_source)} records and batch size {val_batch_size}.')\\n\\n\\n# Instantiate the model by passing the required constants.\\nmodel = Model(\\n    num_timesteps_context=NUM_TIMESTEPS_CONTEXT_INFERRED,\\n    num_neurons=NUM_NEURONS_INFERRED,\\n    prediction_window_length=PREDICTION_WINDOW_LENGTH_INFERRED\\n)\\n\\n# Perform a dummy forward pass using the actual batch_size to build the model\\n# and ensure all layer shapes are determined, including those within FeatureExtractor.\\ndummy_input_for_build = tf.zeros((batch_size, NUM_TIMESTEPS_CONTEXT_INFERRED, NUM_NEURONS_INFERRED), dtype=tf.float32)\\n_ = model(dummy_input_for_build) # Call the model once to build its layers\\n\\nprint(\\"\\\\nShared-Weight MLP Model Summary (with encapsulated and consolidated FeatureExtractor and Residual Blocks):\\")\\nmodel.summary()\\n\\ntotal_params = model.count_params()\\n# The input dimension to the MLP is now determined by the FeatureExtractor's output\\n# It's (conv1d_filters + 8 stats + (C-1) deltas + C raw context + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\ntotal_mlp_input_dim = (model.feature_extractor.conv1d_context_processor.filters +\\n                       8 + # number of static features (mean, std, min, max, last, first, median, range)\\n                       (model.NUM_TIMESTEPS_CONTEXT - 1) + # number of delta features\\n                       model.NUM_TIMESTEPS_CONTEXT + # NEW: Raw context features\\n                       model.feature_extractor.GLOBAL_STATE_DIM +\\n                       model.feature_extractor.EMBEDDING_DIM)\\n\\nprint(f\\"\\\\nTotal combined model parameters: {total_params:,}.\\")\\nprint(f\\"This compact model shares its weights across {model.NUM_NEURONS} neurons to predict their future activity.\\")\\nprint(f\\"This design is critical for managing the vast number of outputs ({model.NUM_NEURONS} neurons * {model.PREDICTION_WINDOW_LENGTH} steps), providing both local and global context, and neuron identity, with a total input dimension to the MLP of {total_mlp_input_dim}.\\")\\n\\n\\n# Now using the encapsulated 'fit' method with validation data.\\nmodel.fit(train_data_loader=data_loader, val_data_loader=val_data_loader, epochs=epochs_to_train_for_model_fit)",
  "new_index": 426,
  "new_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\n\\n# Custom Learning Rate Scheduler for Cosine Annealing with Warm Restarts\\nclass CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n    def __init__(self, initial_lr, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0):\\n        super().__init__()\\n        self.initial_lr = initial_lr\\n        self.first_decay_steps = first_decay_steps\\n        self.t_mul = t_mul\\n        self.m_mul = m_mul\\n        self.alpha = alpha\\n        self.current_cycle_steps = 0\\n        self.cycle_length = float(first_decay_steps)\\n        self.lrs = [] # To store learning rates per epoch\\n\\n    def on_epoch_begin(self, epoch, logs=None):\\n        if not hasattr(self.model, 'optimizer') or self.model.optimizer is None:\\n            raise ValueError('Model optimizer is not set. Ensure model.compile() is called before fit().')\\n        \\n        # Access the learning_rate attribute, which is a tf.Variable for Adam\\n        # This is more robust than 'lr' which might be an alias or not always present.\\n        if not hasattr(self.model.optimizer, 'learning_rate'):\\n            raise ValueError('Optimizer must have a \\"learning_rate\\" attribute.')\\n\\n        # Calculate the learning rate for the current epoch\\n        if epoch == 0:\\n            current_lr = self.initial_lr\\n        else:\\n            # Check if we finished a cycle and need to restart\\n            if (epoch - self.current_cycle_steps) >= self.cycle_length:\\n                self.current_cycle_steps += self.cycle_length\\n                self.cycle_length *= self.t_mul\\n                self.initial_lr *= self.m_mul # Update initial_lr for the new cycle\\n            \\n            # Calculate progress within the current cycle\\n            progress_in_cycle = (epoch - self.current_cycle_steps) / self.cycle_length\\n            \\n            # Cosine annealing formula\\n            cosine_decay = 0.5 * (1 + np.cos(np.pi * progress_in_cycle))\\n            current_lr = (self.initial_lr - self.initial_lr * self.alpha) * cosine_decay + self.initial_lr * self.alpha\\n        \\n        # Set the optimizer's learning rate using assign for tf.Variable\\n        self.model.optimizer.learning_rate.assign(current_lr)\\n        self.lrs.append(current_lr)\\n        # print(f\\"Epoch {epoch+1}: Learning rate set to {current_lr:.6f}\\") # For debugging\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Consolidates temporal feature extraction using a 1D CNN for the short context.\\n    - Relies on 1D CNN temporal features, enhanced rolling window statistics, a learned global brain state,\\n      neuron ID embeddings, and DELTA FEATURES.\\n    - Removed RAW CONTEXT FEATURES to avoid redundancy and potential noise.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n        # 1D CNN layer to process the raw temporal context for each neuron\\n        self.conv1d_context_processor = keras.layers.Conv1D(\\n            filters=64, # Increased output filters for 1D convolution\\n            kernel_size=self.NUM_TIMESTEPS_CONTEXT, # Use kernel size equal to context length to capture full window\\n            activation='relu',\\n            name='conv1d_context_processor_in_extractor',\\n        )\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A single tensor combining all extracted features, shaped (B*N, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Reshape for Conv1D input: (batch_size * num_neurons, timesteps_context, 1)\\n        raw_neuron_context_for_temporal_layer = tf.expand_dims(\\n            tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT]), axis=-1\\n        ) # Shape: (B*N, C, 1)\\n\\n        # Process raw context with 1D CNN\\n        # input shape for conv1d_context_processor will be (B*N, C, 1)\\n        temporal_conv_features = self.conv1d_context_processor(raw_neuron_context_for_temporal_layer) # Shape: (B*N, 1, filters)\\n        # Squeeze the redundant dimension, as kernel_size=C effectively performs pooling\\n        processed_temporal_features = tf.squeeze(temporal_conv_features, axis=1) # Shape: (B*N, filters)\\n\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data, axis=2) # Sort along the context dimension\\n        # Note: For C=4, indices are 0, 1, 2, 3. Middle two are at index 1 and 2.\\n        # This is (C/2 - 1) and (C/2).\\n        neuron_medians = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0 # Shape (B, N)\\n        neuron_medians = tf.expand_dims(neuron_medians, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n        # Calculate delta (difference) features for the context window\\n        # For C timesteps, there are C-1 delta values: (t1-t0), (t2-t1), ..., (t_C-1 - t_C-2)\\n        neuron_deltas = raw_neuron_data[:, :, 1:] - raw_neuron_data[:, :, :-1] # Shape: (B, N, C-1)\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n        neuron_deltas_flat = tf.reshape(neuron_deltas, [-1, self.NUM_TIMESTEPS_CONTEXT - 1]) # Shape: (B*N, C-1)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Combine all features (1D CNN temporal features, statistics, DELTA FEATURES, global context, neuron embedding)\\n        combined_features_for_mlp = tf.concat([\\n            processed_temporal_features, # (B*N, 64) - Temporal features from 1D CNN\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            neuron_deltas_flat,      # (B*N, C-1) - Delta features\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 64 + 8 + (C-1) + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return combined_features_for_mlp\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model now uses a refined FeatureExtractor that consolidates 1D CNN temporal processing\\n  alongside rich statistical, delta, and global brain state features, neuron ID embeddings.\\n  Raw context features have been removed to avoid redundancy.\\n  The extracted features are processed by a deep residual network.\\n  Training is enhanced with a Cosine Annealing Warm Restarts learning rate schedule.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n\\n    # Initial batch normalization and projection layer\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    # This layer will now take the concatenated features directly from the FeatureExtractor.\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks - Adjusted dropout rate\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block4')\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='softplus', # Retained softplus activation\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the refined FeatureExtractor\\n    to provide all combined features.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate all combined features\\n    combined_features_for_mlp = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features_for_mlp, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    # Set num_epochs=1 for grain.IndexSampler, and tf.data.Dataset.from_generator handles\\n    # restarting the data source for each epoch provided to model.fit.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        # EarlyStopping remains\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Add Cosine Annealing Warm Restarts Learning Rate Callback\\n    initial_lr_for_schedule = 1e-3 # Starting LR for the first cycle\\n    first_decay_epochs = 10 # Length of the first LR cycle in epochs\\n    callbacks.append(CosineAnnealingWarmRestarts(\\n        initial_lr=initial_lr_for_schedule, \\n        first_decay_steps=first_decay_epochs, \\n        t_mul=2.0, # Double cycle length each restart\\n        m_mul=1.0 # Keep max LR constant each restart\\n    ))\\n\\n    # Compile the model with Adam optimizer and MAE loss.\\n    # The learning rate is now managed by the CosineAnnealingWarmRestarts callback.\\n    self.compile(optimizer=keras.optimizers.Adam(learning_rate=initial_lr_for_schedule), loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# Dynamically infer the necessary constants from the data source for model instantiation.\\nNUM_NEURONS_INFERRED = source[0]['series_input'].shape[1]\\nPREDICTION_WINDOW_LENGTH_INFERRED = constants.PREDICTION_WINDOW_LENGTH\\nNUM_TIMESTEPS_CONTEXT_INFERRED = num_timesteps_context\\n\\n# Define the number of epochs to train for model.fit\\nepochs_to_train_for_model_fit = 120\\n\\n# Setup train data loader: ensure num_epochs is 1 for consistent tf.data.Dataset generation per epoch\\nindex_sampler = grain.IndexSampler(\\n    num_records=len(source),\\n    num_epochs=num_epochs, # This should be 1 as defined globally initially\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=shuffle,\\n    seed=101\\n)\\n\\ndata_loader = grain.DataLoader(\\n    data_source=source,\\n    sampler=index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Setup validation data loader: ensure num_epochs is 1\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation split\\n  config_val = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Crucially, use 'val' split here\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config_val, prefetch=True))\\n\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_batch_size = batch_size # Use same batch size for validation\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # This should be 1\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101 # Consistent seed\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=val_batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\nprint(f'\\\\nValidation data loader created with {len(val_source)} records and batch size {val_batch_size}.')\\n\\n\\n# Instantiate the model by passing the required constants.\\nmodel = Model(\\n    num_timesteps_context=NUM_TIMESTEPS_CONTEXT_INFERRED,\\n    num_neurons=NUM_NEURONS_INFERRED,\\n    prediction_window_length=PREDICTION_WINDOW_LENGTH_INFERRED\\n)\\n\\n# Perform a dummy forward pass using the actual batch_size to build the model\\n# and ensure all layer shapes are determined, including those within FeatureExtractor.\\ndummy_input_for_build = tf.zeros((batch_size, NUM_TIMESTEPS_CONTEXT_INFERRED, NUM_NEURONS_INFERRED), dtype=tf.float32)\\n_ = model(dummy_input_for_build) # Call the model once to build its layers\\n\\nprint(\\"\\\\nShared-Weight MLP Model Summary (with encapsulated and consolidated FeatureExtractor and Residual Blocks):\\")\\nmodel.summary()\\n\\ntotal_params = model.count_params()\\n# The input dimension to the MLP is now determined by the FeatureExtractor's output\\n# It's (conv1d_filters + 8 stats + (C-1) deltas + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\ntotal_mlp_input_dim = (model.feature_extractor.conv1d_context_processor.filters +\\n                       8 + # number of static features (mean, std, min, max, last, first, median, range)\\n                       (model.NUM_TIMESTEPS_CONTEXT - 1) + # number of delta features\\n                       model.feature_extractor.GLOBAL_STATE_DIM +\\n                       model.feature_extractor.EMBEDDING_DIM)\\n\\nprint(f\\"\\\\nTotal combined model parameters: {total_params:,}.\\")\\nprint(f\\"This compact model shares its weights across {model.NUM_NEURONS} neurons to predict their future activity.\\")\\nprint(f\\"This design is critical for managing the vast number of outputs ({model.NUM_NEURONS} neurons * {model.PREDICTION_WINDOW_LENGTH} steps), providing both local and global context, and neuron identity, with a total input dimension to the MLP of {total_mlp_input_dim}.\\")\\n\\n\\n# Now using the encapsulated 'fit' method with validation data.\\nmodel.fit(train_data_loader=data_loader, val_data_loader=val_data_loader, epochs=epochs_to_train_for_model_fit)"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 21.0,
  "old_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\nimport math\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Relies on raw C-lagged values, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A tuple of tensors: (per_neuron_context_input, mlp_other_features_flat).\\n          per_neuron_context_input: (batch_size * NUM_NEURONS, C, 1) for temporal processing.\\n          mlp_other_features_flat: (batch_size * NUM_NEURONS, total_feature_dim_excluding_raw_context_for_temporal_model)\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # --- 1. Raw per-neuron context for temporal model ---\\n        # Reshape for temporal model (e.g., Conv1D): (B*N, C, 1)\\n        per_neuron_context_input = tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT, 1])\\n\\n\\n        # --- 2. Calculate rolling window statistics ---\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        # Use tf.cast to float32 before sorting to avoid potential issues if input is not float\\n        sorted_neuron_data = tf.sort(tf.cast(raw_neuron_data, tf.float32), axis=2) # Sort along the context dimension\\n        # Ensure correct indexing for median, handle cases where C is odd or even\\n        if self.NUM_TIMESTEPS_CONTEXT % 2 == 0:\\n            # Even number of timesteps, average middle two\\n            median_val = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0\\n        else:\\n            # Odd number of timesteps, take middle one\\n            median_val = sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2]\\n        neuron_medians = tf.expand_dims(median_val, axis=-1) # Shape (B, N, 1)\\n        \\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n\\n\\n        # --- 3. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 4. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # --- 5. Combine all other numeric and embedding features for the MLP input ---\\n        mlp_other_features_flat = tf.concat([\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 8 + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return per_neuron_context_input, mlp_other_features_flat\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model leverages a shared-weights approach, training a single MLP that learns to predict\\n  the future activity for one neuron based on its past activity, enriched with both\\n  its own raw context (processed by an LSTM), rolling statistics, distilled global brain activity context,\\n  and its unique ID.\\n  The feature engineering is now encapsulated and improved in a separate \`FeatureExtractor\` layer.\\n  Improvements:\\n  - More comprehensive feature engineering in FeatureExtractor.\\n  - Integration of Residual Blocks in the MLP for deeper and more stable training.\\n  - Replaced LSTM with Conv1D for temporal context, addressing OOM and improving efficiency.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    # NEW: Conv1D Temporal Extractor to process raw temporal context per neuron\\n    self.temporal_conv_filters = 64\\n    self.temporal_extractor_sequence = keras.Sequential([\\n        keras.layers.Conv1D(\\n            filters=self.temporal_conv_filters,\\n            kernel_size=min(self.NUM_TIMESTEPS_CONTEXT, 3), # Use kernel_size up to 3 or context length if smaller\\n            activation='relu',\\n            name='temporal_conv1d'\\n        ),\\n        keras.layers.GlobalAveragePooling1D(name='temporal_conv1d_pool')\\n    ], name='per_neuron_temporal_extractor') # This name is for the sequential block, the attribute is temporal_extractor_sequence\\n\\n    # MLP Architecture with Residual Blocks\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks\\n    # Each block's input and output dimensions are self.RESIDUAL_BLOCK_UNITS\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block4') # Added another block for depth\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='linear',\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the encapsulated FeatureExtractor\\n    and Residual Blocks, with Conv1D for temporal context.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate combined features\\n    per_neuron_context_input, mlp_other_features = self.feature_extractor(series_input)\\n\\n    # Process raw context with Conv1D\\n    # FIX: Corrected attribute name from self.per_neuron_temporal_extractor to self.temporal_extractor_sequence\\n    temporal_features = self.temporal_extractor_sequence(per_neuron_context_input) # Shape: (B*N, temporal_conv_filters)\\n\\n    # Concatenate temporal features with other features\\n    combined_features = tf.concat([temporal_features, mlp_other_features], axis=1)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    # CosineAnnealingWarmRestarts Callback\\n    class CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n        def __init__(self, T_0, T_mult=1, eta_max=1e-3, eta_min=1e-6, verbose=0):\\n            super().__init__()\\n            self.T_0 = T_0\\n            self.T_mult = T_mult\\n            self.eta_max = eta_max # Maximum LR\\n            self.eta_min = eta_min # Minimum LR\\n            self.verbose = verbose\\n            self.T_cur = 0\\n            self.T_i = T_0 # Current cycle length\\n\\n        def on_epoch_begin(self, epoch, logs=None):\\n            if not hasattr(self.model.optimizer, 'lr'):\\n                raise ValueError('Optimizer must have a \\"lr\\" attribute.')\\n\\n            self.T_cur = epoch % self.T_i # Calculate current epoch in cycle\\n\\n            lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (\\n                1 + math.cos(math.pi * self.T_cur / self.T_i)\\n            )\\n            keras.backend.set_value(self.model.optimizer.lr, lr)\\n            if self.verbose > 0:\\n                print(f'\\\\nEpoch {epoch+1}: CosineAnnealingWarmRestarts setting learning rate to {lr:.6f}.')\\n\\n        def on_epoch_end(self, epoch, logs=None):\\n            if (epoch + 1) % self.T_i == 0:\\n                # Restart cycle\\n                self.T_cur = 0\\n                self.T_i *= self.T_mult\\n                if self.verbose > 0:\\n                    print(f'\\\\nEpoch {epoch+1}: Restarting CosineAnnealing cycle. New T_i: {self.T_i}.')\\n\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        \\n        # Add CosineAnnealingWarmRestarts callback\\n        callbacks.append(CosineAnnealingWarmRestarts(\\n            T_0=15, # Cycle length in epochs\\n            T_mult=1, # No increase in cycle length\\n            eta_max=1e-3, # Max LR (default Adam LR)\\n            eta_min=1e-6, # Min LR\\n            verbose=1 # Print LR updates\\n        ))\\n        # Keep EarlyStopping\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Compile the model with Adam optimizer and MAE loss, as MAE is the benchmark metric.\\n    self.compile(optimizer='adam', loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# Dynamically infer the necessary constants from the data source for model instantiation.\\nNUM_NEURONS_INFERRED = source[0]['series_input'].shape[1]\\nPREDICTION_WINDOW_LENGTH_INFERRED = constants.PREDICTION_WINDOW_LENGTH\\nNUM_TIMESTEPS_CONTEXT_INFERRED = num_timesteps_context\\n\\n# Instantiate the model by passing the required constants.\\nmodel = Model(\\n    num_timesteps_context=NUM_TIMESTEPS_CONTEXT_INFERRED,\\n    num_neurons=NUM_NEURONS_INFERRED,\\n    prediction_window_length=PREDICTION_WINDOW_LENGTH_INFERRED\\n)\\n\\n# Perform a dummy forward pass using the actual batch_size to build the model\\n# and ensure all layer shapes are determined, including those within FeatureExtractor.\\ndummy_input_for_build = tf.zeros((batch_size, NUM_TIMESTEPS_CONTEXT_INFERRED, NUM_NEURONS_INFERRED), dtype=tf.float32)\\n_ = model(dummy_input_for_build) # Call the model once to build its layers\\n\\nprint(\\"\\\\nShared-Weight MLP Model Summary (with encapsulated FeatureExtractor and Residual Blocks):\\")\\nmodel.summary()\\n\\ntotal_params = model.count_params()\\n# Get the input dimension of the initial projection dense layer\\ntotal_mlp_input_dim = model.initial_projection_dense.kernel.shape[0]\\n\\nprint(f\\"\\\\nTotal combined model parameters: {total_params:,}.\\")\\nprint(f\\"This compact model shares its weights across {model.NUM_NEURONS} neurons to predict their future activity.\\")\\nprint(f\\"This design is critical for managing the vast number of outputs ({model.NUM_NEURONS} neurons * {model.PREDICTION_WINDOW_LENGTH} steps), providing both local and global context, and neuron identity, with a total input dimension to the MLP of {total_mlp_input_dim}.\\")\\n\\n\\n# --- Create Validation Data Loader for model.fit ---\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation split\\n  config_val = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Crucially, use 'val' split here\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config_val, prefetch=True))\\n\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_batch_size = batch_size # Use same batch size for validation\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # Can be 1 epoch for validation\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101 # Consistent seed\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=val_batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\nprint(f'\\\\nValidation data loader created with {len(val_source)} records and batch size {val_batch_size}.')\\n\\n\\n# Now using the encapsulated 'fit' method with validation data.\\n# The number of epochs can be increased for potentially better performance.\\nmodel.fit(train_data_loader=data_loader, val_data_loader=val_data_loader, epochs=100) # Increased epochs for EarlyStopping",
  "new_index": 29,
  "new_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\nimport math\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Relies on raw C-lagged values, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A tuple of tensors: (per_neuron_context_input, mlp_other_features_flat).\\n          per_neuron_context_input: (batch_size * NUM_NEURONS, C, 1) for temporal processing.\\n          mlp_other_features_flat: (batch_size * NUM_NEURONS, total_feature_dim_excluding_raw_context_for_temporal_model)\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # --- 1. Raw per-neuron context for temporal model ---\\n        # Reshape for temporal model (e.g., Conv1D): (B*N, C, 1)\\n        per_neuron_context_input = tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT, 1])\\n\\n\\n        # --- 2. Calculate rolling window statistics ---\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        # Use tf.cast to float32 before sorting to avoid potential issues if input is not float\\n        sorted_neuron_data = tf.sort(tf.cast(raw_neuron_data, tf.float32), axis=2) # Sort along the context dimension\\n        # Ensure correct indexing for median, handle cases where C is odd or even\\n        if self.NUM_TIMESTEPS_CONTEXT % 2 == 0:\\n            # Even number of timesteps, average middle two\\n            median_val = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0\\n        else:\\n            # Odd number of timesteps, take middle one\\n            median_val = sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2]\\n        neuron_medians = tf.expand_dims(median_val, axis=-1) # Shape (B, N, 1)\\n        \\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n\\n\\n        # --- 3. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 4. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # --- 5. Combine all other numeric and embedding features for the MLP input ---\\n        mlp_other_features_flat = tf.concat([\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 8 + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return per_neuron_context_input, mlp_other_features_flat\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model leverages a shared-weights approach, training a single MLP that learns to predict\\n  the future activity for one neuron based on its past activity, enriched with both\\n  its own raw context (processed by an LSTM), rolling statistics, distilled global brain activity context,\\n  and its unique ID.\\n  The feature engineering is now encapsulated and improved in a separate \`FeatureExtractor\` layer.\\n  Improvements:\\n  - More comprehensive feature engineering in FeatureExtractor.\\n  - Integration of Residual Blocks in the MLP for deeper and more stable training.\\n  - Replaced LSTM with Conv1D for temporal context, addressing OOM and improving efficiency.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    # NEW: Conv1D Temporal Extractor to process raw temporal context per neuron\\n    self.temporal_conv_filters = 64\\n    self.temporal_extractor_sequence = keras.Sequential([\\n        keras.layers.Conv1D(\\n            filters=self.temporal_conv_filters,\\n            kernel_size=min(self.NUM_TIMESTEPS_CONTEXT, 3), # Use kernel_size up to 3 or context length if smaller\\n            activation='relu',\\n            name='temporal_conv1d'\\n        ),\\n        keras.layers.GlobalAveragePooling1D(name='temporal_conv1d_pool')\\n    ], name='per_neuron_temporal_extractor') # This name is for the sequential block, the attribute is temporal_extractor_sequence\\n\\n    # MLP Architecture with Residual Blocks\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks\\n    # Each block's input and output dimensions are self.RESIDUAL_BLOCK_UNITS\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block4') # Added another block for depth\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='linear',\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the encapsulated FeatureExtractor\\n    and Residual Blocks, with Conv1D for temporal context.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate combined features\\n    per_neuron_context_input, mlp_other_features = self.feature_extractor(series_input)\\n\\n    # Process raw context with Conv1D\\n    temporal_features = self.temporal_extractor_sequence(per_neuron_context_input) # Shape: (B*N, temporal_conv_filters)\\n\\n    # Concatenate temporal features with other features\\n    combined_features = tf.concat([temporal_features, mlp_other_features], axis=1)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    # CosineAnnealingWarmRestarts Callback\\n    class CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n        def __init__(self, T_0, T_mult=1, eta_max=1e-3, eta_min=1e-6, verbose=0):\\n            super().__init__()\\n            self.T_0 = T_0\\n            self.T_mult = T_mult\\n            self.eta_max = eta_max # Maximum LR\\n            self.eta_min = eta_min # Minimum LR\\n            self.verbose = verbose\\n            self.T_cur = 0\\n            self.T_i = T_0 # Current cycle length\\n\\n        def on_epoch_begin(self, epoch, logs=None):\\n            # Check for 'learning_rate' attribute, which is common in TF2.x Keras optimizers\\n            if not hasattr(self.model.optimizer, 'learning_rate'):\\n                raise ValueError('Optimizer must have a \\"learning_rate\\" attribute.')\\n\\n            self.T_cur = epoch % self.T_i # Calculate current epoch in cycle\\n\\n            lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (\\n                1 + math.cos(math.pi * self.T_cur / self.T_i)\\n            )\\n            # Use keras.backend.set_value to update the learning rate tf.Variable\\n            keras.backend.set_value(self.model.optimizer.learning_rate, lr)\\n            if self.verbose > 0:\\n                print(f'\\\\nEpoch {epoch+1}: CosineAnnealingWarmRestarts setting learning rate to {lr:.6f}.')\\n\\n        def on_epoch_end(self, epoch, logs=None):\\n            if (epoch + 1) % self.T_i == 0:\\n                # Restart cycle\\n                self.T_cur = 0\\n                self.T_i *= self.T_mult\\n                if self.verbose > 0:\\n                    print(f'\\\\nEpoch {epoch+1}: Restarting CosineAnnealing cycle. New T_i: {self.T_i}.')\\n\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        \\n        # Add CosineAnnealingWarmRestarts callback\\n        callbacks.append(CosineAnnealingWarmRestarts(\\n            T_0=15, # Cycle length in epochs\\n            T_mult=1, # No increase in cycle length\\n            eta_max=1e-3, # Max LR (default Adam LR)\\n            eta_min=1e-6, # Min LR\\n            verbose=1 # Print LR updates\\n        ))\\n        # Keep EarlyStopping\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Compile the model with Adam optimizer and MAE loss, as MAE is the benchmark metric.\\n    self.compile(optimizer='adam', loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 748.0,
  "old_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\n\\n# Custom Learning Rate Scheduler for Cosine Annealing with Warm Restarts\\nclass CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n    def __init__(self, initial_lr, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0):\\n        super().__init__()\\n        self.initial_lr = initial_lr\\n        self.first_decay_steps = first_decay_steps\\n        self.t_mul = t_mul\\n        self.m_mul = m_mul\\n        self.alpha = alpha\\n        self.current_cycle_steps = 0\\n        self.cycle_length = float(first_decay_steps)\\n        self.lrs = [] # To store learning rates per epoch\\n\\n    def on_epoch_begin(self, epoch, logs=None):\\n        if not hasattr(self.model, 'optimizer') or self.model.optimizer is None:\\n            raise ValueError('Model optimizer is not set. Ensure model.compile() is called before fit().')\\n        \\n        # Access the learning_rate attribute, which is a tf.Variable for Adam\\n        # This is more robust than 'lr' which might be an alias or not always present.\\n        if not hasattr(self.model.optimizer, 'learning_rate'):\\n            raise ValueError('Optimizer must have a \\"learning_rate\\" attribute.')\\n\\n        # Calculate the learning rate for the current epoch\\n        if epoch == 0:\\n            current_lr = self.initial_lr\\n        else:\\n            # Check if we finished a cycle and need to restart\\n            if (epoch - self.current_cycle_steps) >= self.cycle_length:\\n                self.current_cycle_steps += self.cycle_length\\n                self.cycle_length *= self.t_mul\\n                self.initial_lr *= self.m_mul # Update initial_lr for the new cycle\\n            \\n            # Calculate progress within the current cycle\\n            progress_in_cycle = (epoch - self.current_cycle_steps) / self.cycle_length\\n            \\n            # Cosine annealing formula\\n            cosine_decay = 0.5 * (1 + np.cos(np.pi * progress_in_cycle))\\n            current_lr = (self.initial_lr - self.initial_lr * self.alpha) * cosine_decay + self.initial_lr * self.alpha\\n        \\n        # Set the optimizer's learning rate using assign for tf.Variable\\n        self.model.optimizer.learning_rate.assign(current_lr)\\n        self.lrs.append(current_lr)\\n        # print(f\\"Epoch {epoch+1}: Learning rate set to {current_lr:.6f}\\") # For debugging\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Consolidates temporal feature extraction using a 1D CNN for the short context.\\n    - Added explicit delta (rate of change) features.\\n    - Relies on 1D CNN temporal features, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n        # 1D CNN layer to process the raw temporal context for each neuron\\n        self.conv1d_context_processor = keras.layers.Conv1D(\\n            filters=64, # Increased output filters for 1D convolution\\n            kernel_size=self.NUM_TIMESTEPS_CONTEXT, # Use kernel size equal to context length to capture full window\\n            activation='relu',\\n            name='conv1d_context_processor_in_extractor',\\n        )\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A single tensor combining all extracted features, shaped (B*N, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Reshape for Conv1D input: (batch_size * num_neurons, timesteps_context, 1)\\n        raw_neuron_context_for_temporal_layer = tf.expand_dims(\\n            tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT]), axis=-1\\n        ) # Shape: (B*N, C, 1)\\n\\n        # Process raw context with 1D CNN\\n        # input shape for conv1d_context_processor will be (B*N, C, 1)\\n        temporal_conv_features = self.conv1d_context_processor(raw_neuron_context_for_temporal_layer) # Shape: (B*N, 1, filters)\\n        # Squeeze the redundant dimension, as kernel_size=C effectively performs pooling\\n        processed_temporal_features = tf.squeeze(temporal_conv_features, axis=1) # Shape: (B*N, filters)\\n\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data, axis=2) # Sort along the context dimension\\n        # Note: For C=4, indices are 0, 1, 2, 3. Middle two are at index 1 and 2.\\n        # This is (C/2 - 1) and (C/2).\\n        neuron_medians = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0 # Shape (B, N)\\n        neuron_medians = tf.expand_dims(neuron_medians, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n        # Calculate delta features (differences between consecutive timesteps)\\n        # For C=4, we'll have (t-0 - t-1), (t-1 - t-2), (t-2 - t-3)\\n        # raw_neuron_data shape: (B, N, C)\\n        # Deltas along the time dimension: (B, N, C-1)\\n        neuron_deltas = raw_neuron_data[:, :, 1:] - raw_neuron_data[:, :, :-1] # Shape (B, N, C-1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n        neuron_deltas_flat = tf.reshape(neuron_deltas, [-1, self.NUM_TIMESTEPS_CONTEXT - 1]) # Shape (B*N, C-1)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Combine all features (1D CNN temporal features, statistics, global context, neuron embedding, DELTAS)\\n        combined_features_for_mlp = tf.concat([\\n            processed_temporal_features, # (B*N, 64) - NEW: Temporal features from 1D CNN\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            neuron_deltas_flat,      # (B*N, C-1) - NEW: Delta features\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 64 + 8 + (C-1) + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return combined_features_for_mlp\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model now uses a refined FeatureExtractor that consolidates 1D CNN temporal processing\\n  alongside rich statistical, delta, and global brain state features, and neuron ID embeddings.\\n  The extracted features are processed by a deep residual network.\\n  Training is enhanced with a Cosine Annealing Warm Restarts learning rate schedule.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n\\n    # Initial batch normalization and projection layer\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    # This layer will now take the concatenated features directly from the FeatureExtractor.\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS,\\n        activation='relu',\\n        kernel_regularizer=keras.regularizers.l2(1e-5), # Added L2 regularization\\n        name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks - Adjusted dropout rate\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block4')\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='softplus', # Retained softplus activation\\n        kernel_regularizer=keras.regularizers.l2(1e-5), # Added L2 regularization\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the refined FeatureExtractor\\n    to provide all combined features.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate all combined features\\n    combined_features_for_mlp = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features_for_mlp, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    # Set num_epochs=1 for grain.IndexSampler, and tf.data.Dataset.from_generator handles\\n    # restarting the data source for each epoch provided to model.fit.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        # EarlyStopping remains\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Add Cosine Annealing Warm Restarts Learning Rate Callback\\n    initial_lr_for_schedule = 1.5e-3 # Starting LR for the first cycle - INCREASED\\n    first_decay_epochs = 10 # Length of the first LR cycle in epochs\\n    callbacks.append(CosineAnnealingWarmRestarts(\\n        initial_lr=initial_lr_for_schedule, \\n        first_decay_steps=first_decay_epochs, \\n        t_mul=2.0, # Double cycle length each restart\\n        m_mul=1.0 # Keep max LR constant each restart\\n    ))\\n\\n    # Compile the model with Adam optimizer and MAE loss.\\n    # The learning rate is now managed by the CosineAnnealingWarmRestarts callback.\\n    self.compile(optimizer=keras.optimizers.Adam(learning_rate=initial_lr_for_schedule), loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# Dynamically infer the necessary constants from the data source for model instantiation.\\nNUM_NEURONS_INFERRED = source[0]['series_input'].shape[1]\\nPREDICTION_WINDOW_LENGTH_INFERRED = constants.PREDICTION_WINDOW_LENGTH\\nNUM_TIMESTEPS_CONTEXT_INFERRED = num_timesteps_context\\n\\n# Define the number of epochs to train for model.fit\\nepochs_to_train_for_model_fit = 120\\n\\n# Setup train data loader: ensure num_epochs is 1 for consistent tf.data.Dataset generation per epoch\\nindex_sampler = grain.IndexSampler(\\n    num_records=len(source),\\n    num_epochs=num_epochs, # This should be 1 as defined globally initially\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=shuffle,\\n    seed=101\\n)\\n\\ndata_loader = grain.DataLoader(\\n    data_source=source,\\n    sampler=index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Setup validation data loader: ensure num_epochs is 1\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation split\\n  config_val = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Crucially, use 'val' split here\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config_val, prefetch=True))\\n\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_batch_size = batch_size # Use same batch size for validation\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # This should be 1\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101 # Consistent seed\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=val_batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\nprint(f'\\\\nValidation data loader created with {len(val_source)} records and batch size {val_batch_size}.')\\n\\n\\n# Instantiate the model by passing the required constants.\\nmodel = Model(\\n    num_timesteps_context=NUM_TIMESTEPS_CONTEXT_INFERRED,\\n    num_neurons=NUM_NEURONS_INFERRED,\\n    prediction_window_length=PREDICTION_WINDOW_LENGTH_INFERRED\\n)\\n\\n# Perform a dummy forward pass using the actual batch_size to build the model\\n# and ensure all layer shapes are determined, including those within FeatureExtractor.\\ndummy_input_for_build = tf.zeros((batch_size, NUM_TIMESTEPS_CONTEXT_INFERRED, NUM_NEURONS_INFERRED), dtype=tf.float32)\\n_ = model(dummy_input_for_build) # Call the model once to build its layers\\n\\nprint(\\"\\\\nShared-Weight MLP Model Summary (with encapsulated and consolidated FeatureExtractor and Residual Blocks):\\")\\nmodel.summary()\\n\\ntotal_params = model.count_params()\\n# The input dimension to the MLP is now determined by the FeatureExtractor's output\\n# It's (conv1d_filters + 8 stats + (C-1) deltas + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\ntotal_mlp_input_dim = (model.feature_extractor.conv1d_context_processor.filters +\\n                       8 + # number of static features (mean, std, min, max, last, first, median, range)\\n                       (model.NUM_TIMESTEPS_CONTEXT - 1) + # number of delta features\\n                       model.feature_extractor.GLOBAL_STATE_DIM +\\n                       model.feature_extractor.EMBEDDING_DIM)\\n\\nprint(f\\"\\\\nTotal combined model parameters: {total_params:,}.\\")\\nprint(f\\"This compact model shares its weights across {model.NUM_NEURONS} neurons to predict their future activity.\\")\\nprint(f\\"This design is critical for managing the vast number of outputs ({model.NUM_NEURONS} neurons * {model.PREDICTION_WINDOW_LENGTH} steps), providing both local and global context, neuron identity, and now rate of change, with a total input dimension to the MLP of {total_mlp_input_dim}.\\")\\n\\n\\n# Now using the encapsulated 'fit' method with validation data.\\nmodel.fit(train_data_loader=data_loader, val_data_loader=val_data_loader, epochs=epochs_to_train_for_model_fit)",
  "new_index": 756,
  "new_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain\\n\\n# Custom Learning Rate Scheduler for Cosine Annealing with Warm Restarts\\nclass CosineAnnealingWarmRestarts(keras.callbacks.Callback):\\n    def __init__(self, initial_lr, first_decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0):\\n        super().__init__()\\n        self.initial_lr = initial_lr\\n        self.first_decay_steps = first_decay_steps\\n        self.t_mul = t_mul\\n        self.m_mul = m_mul\\n        self.alpha = alpha\\n        self.current_cycle_steps = 0\\n        self.cycle_length = float(first_decay_steps)\\n        self.lrs = [] # To store learning rates per epoch\\n\\n    def on_epoch_begin(self, epoch, logs=None):\\n        if not hasattr(self.model, 'optimizer') or self.model.optimizer is None:\\n            raise ValueError('Model optimizer is not set. Ensure model.compile() is called before fit().')\\n        \\n        # Access the learning_rate attribute, which is a tf.Variable for Adam\\n        # This is more robust than 'lr' which might be an alias or not always present.\\n        if not hasattr(self.model.optimizer, 'learning_rate'):\\n            raise ValueError('Optimizer must have a \\"learning_rate\\" attribute.')\\n\\n        # Calculate the learning rate for the current epoch\\n        if epoch == 0:\\n            current_lr = self.initial_lr\\n        else:\\n            # Check if we finished a cycle and need to restart\\n            if (epoch - self.current_cycle_steps) >= self.cycle_length:\\n                self.current_cycle_steps += self.cycle_length\\n                self.cycle_length *= self.t_mul\\n                self.initial_lr *= self.m_mul # Update initial_lr for the new cycle\\n            \\n            # Calculate progress within the current cycle\\n            progress_in_cycle = (epoch - self.current_cycle_steps) / self.cycle_length\\n            \\n            # Cosine annealing formula\\n            cosine_decay = 0.5 * (1 + np.cos(np.pi * progress_in_cycle))\\n            current_lr = (self.initial_lr - self.initial_lr * self.alpha) * cosine_decay + self.initial_lr * self.alpha\\n        \\n        # Set the optimizer's learning rate using assign for tf.Variable\\n        self.model.optimizer.learning_rate.assign(current_lr)\\n        self.lrs.append(current_lr)\\n        # print(f\\"Epoch {epoch+1}: Learning rate set to {current_lr:.6f}\\") # For debugging\\n\\n# Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Added more rolling window statistics: median, range, first value.\\n    - Consolidates temporal feature extraction using a 1D CNN for the short context.\\n    - Added explicit delta (rate of change) features.\\n    - Relies on 1D CNN temporal features, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs):\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n        # 1D CNN layer to process the raw temporal context for each neuron\\n        self.conv1d_context_processor = keras.layers.Conv1D(\\n            filters=64, # Increased output filters for 1D convolution\\n            kernel_size=self.NUM_TIMESTEPS_CONTEXT, # Use kernel size equal to context length to capture full window\\n            activation='relu',\\n            name='conv1d_context_processor_in_extractor',\\n        )\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          A single tensor combining all extracted features, shaped (B*N, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # Transpose to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        raw_neuron_data = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Reshape for Conv1D input: (batch_size * num_neurons, timesteps_context, 1)\\n        raw_neuron_context_for_temporal_layer = tf.expand_dims(\\n            tf.reshape(raw_neuron_data, [-1, self.NUM_TIMESTEPS_CONTEXT]), axis=-1\\n        ) # Shape: (B*N, C, 1)\\n\\n        # Process raw context with 1D CNN\\n        # input shape for conv1d_context_processor will be (B*N, C, 1)\\n        temporal_conv_features = self.conv1d_context_processor(raw_neuron_context_for_temporal_layer) # Shape: (B*N, 1, filters)\\n        # Squeeze the redundant dimension, as kernel_size=C effectively performs pooling\\n        processed_temporal_features = tf.squeeze(temporal_conv_features, axis=1) # Shape: (B*N, filters)\\n\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        neuron_first_value = raw_neuron_data[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        \\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data, axis=2) # Sort along the context dimension\\n        # Note: For C=4, indices are 0, 1, 2, 3. Middle two are at index 1 and 2.\\n        # This is (C/2 - 1) and (C/2).\\n        neuron_medians = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0 # Shape (B, N)\\n        neuron_medians = tf.expand_dims(neuron_medians, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n        # Calculate delta features (differences between consecutive timesteps)\\n        # For C=4, we'll have (t-0 - t-1), (t-1 - t-2), (t-2 - t-3)\\n        # raw_neuron_data shape: (B, N, C)\\n        # Deltas along the time dimension: (B, N, C-1)\\n        neuron_deltas = raw_neuron_data[:, :, 1:] - raw_neuron_data[:, :, :-1] # Shape (B, N, C-1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n        neuron_deltas_flat = tf.reshape(neuron_deltas, [-1, self.NUM_TIMESTEPS_CONTEXT - 1]) # Shape (B*N, C-1)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Combine all features (1D CNN temporal features, statistics, global context, neuron embedding, DELTAS)\\n        combined_features_for_mlp = tf.concat([\\n            processed_temporal_features, # (B*N, 64) - NEW: Temporal features from 1D CNN\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            neuron_deltas_flat,      # (B*N, C-1) - NEW: Delta features\\n            global_context_final,    # (B*N, GLOBAL_STATE_DIM)\\n            neuron_embedding         # (B*N, EMBEDDING_DIM)\\n        ], axis=1) # Shape: (B*N, 64 + 8 + (C-1) + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\n\\n        return combined_features_for_mlp\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model now uses a refined FeatureExtractor that consolidates 1D CNN temporal processing\\n  alongside rich statistical, delta, and global brain state features, and neuron ID embeddings.\\n  The extracted features are processed by a deep residual network.\\n  Training is enhanced with a Cosine Annealing Warm Restarts learning rate schedule.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n\\n    # Initial batch normalization and projection layer\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    # This layer will now take the concatenated features directly from the FeatureExtractor.\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS,\\n        activation='relu',\\n        kernel_regularizer=keras.regularizers.l2(1e-5), # Added L2 regularization\\n        name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks - Adjusted dropout rate\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.3, name='res_block4')\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='softplus', # Retained softplus activation\\n        kernel_regularizer=keras.regularizers.l2(1e-5), # Added L2 regularization\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the refined FeatureExtractor\\n    to provide all combined features.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate all combined features\\n    combined_features_for_mlp = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features_for_mlp, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    # Set num_epochs=1 for grain.IndexSampler, and tf.data.Dataset.from_generator handles\\n    # restarting the data source for each epoch provided to model.fit.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n        # EarlyStopping remains\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1))\\n\\n    # Add Cosine Annealing Warm Restarts Learning Rate Callback\\n    initial_lr_for_schedule = 8e-4 # Starting LR for the first cycle - DECREASED\\n    first_decay_epochs = 10 # Length of the first LR cycle in epochs\\n    callbacks.append(CosineAnnealingWarmRestarts(\\n        initial_lr=initial_lr_for_schedule, \\n        first_decay_steps=first_decay_epochs, \\n        t_mul=2.0, # Double cycle length each restart\\n        m_mul=1.0 # Keep max LR constant each restart\\n    ))\\n\\n    # Compile the model with Adam optimizer and MAE loss.\\n    # The learning rate is now managed by the CosineAnnealingWarmRestarts callback.\\n    self.compile(optimizer=keras.optimizers.Adam(learning_rate=initial_lr_for_schedule), loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, NUM_NEURONS).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# Dynamically infer the necessary constants from the data source for model instantiation.\\nNUM_NEURONS_INFERRED = source[0]['series_input'].shape[1]\\nPREDICTION_WINDOW_LENGTH_INFERRED = constants.PREDICTION_WINDOW_LENGTH\\nNUM_TIMESTEPS_CONTEXT_INFERRED = num_timesteps_context\\n\\n# Define the number of epochs to train for model.fit\\nepochs_to_train_for_model_fit = 120\\n\\n# Setup train data loader: ensure num_epochs is 1 for consistent tf.data.Dataset generation per epoch\\nindex_sampler = grain.IndexSampler(\\n    num_records=len(source),\\n    num_epochs=num_epochs, # This should be 1 as defined globally initially\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=shuffle,\\n    seed=101\\n)\\n\\ndata_loader = grain.DataLoader(\\n    data_source=source,\\n    sampler=index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Setup validation data loader: ensure num_epochs is 1\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation split\\n  config_val = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Crucially, use 'val' split here\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config_val, prefetch=True))\\n\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_batch_size = batch_size # Use same batch size for validation\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # This should be 1\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101 # Consistent seed\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=val_batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\nprint(f'\\\\nValidation data loader created with {len(val_source)} records and batch size {val_batch_size}.')\\n\\n\\n# Instantiate the model by passing the required constants.\\nmodel = Model(\\n    num_timesteps_context=NUM_TIMESTEPS_CONTEXT_INFERRED,\\n    num_neurons=NUM_NEURONS_INFERRED,\\n    prediction_window_length=PREDICTION_WINDOW_LENGTH_INFERRED\\n)\\n\\n# Perform a dummy forward pass using the actual batch_size to build the model\\n# and ensure all layer shapes are determined, including those within FeatureExtractor.\\ndummy_input_for_build = tf.zeros((batch_size, NUM_TIMESTEPS_CONTEXT_INFERRED, NUM_NEURONS_INFERRED), dtype=tf.float32)\\n_ = model(dummy_input_for_build) # Call the model once to build its layers\\n\\nprint(\\"\\\\nShared-Weight MLP Model Summary (with encapsulated and consolidated FeatureExtractor and Residual Blocks):\\")\\nmodel.summary()\\n\\ntotal_params = model.count_params()\\n# The input dimension to the MLP is now determined by the FeatureExtractor's output\\n# It's (conv1d_filters + 8 stats + (C-1) deltas + GLOBAL_STATE_DIM + EMBEDDING_DIM)\\ntotal_mlp_input_dim = (model.feature_extractor.conv1d_context_processor.filters +\\n                       8 + # number of static features (mean, std, min, max, last, first, median, range)\\n                       (model.NUM_TIMESTEPS_CONTEXT - 1) + # number of delta features\\n                       model.feature_extractor.GLOBAL_STATE_DIM +\\n                       model.feature_extractor.EMBEDDING_DIM)\\n\\nprint(f\\"\\\\nTotal combined model parameters: {total_params:,}.\\")\\nprint(f\\"This compact model shares its weights across {model.NUM_NEURONS} neurons to predict their future activity.\\")\\nprint(f\\"This design is critical for managing the vast number of outputs ({model.NUM_NEURONS} neurons * {model.PREDICTION_WINDOW_LENGTH} steps), providing both local and global context, neuron identity, and now rate of change, with a total input dimension to the MLP of {total_mlp_input_dim}.\\")\\n\\n\\n# Now using the encapsulated 'fit' method with validation data.\\nmodel.fit(train_data_loader=data_loader, val_data_loader=val_data_loader, epochs=epochs_to_train_for_model_fit)"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

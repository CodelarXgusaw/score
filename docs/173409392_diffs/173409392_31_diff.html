<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 25.0,
  "old_code": "# TODO: Improve the model pipeline.\\n\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain # Import grain for the data loaders\\n\\n# NEW: Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n        # Ensure that if input_shape is known at build time, self.dense.build is called.\\n        # This is implicitly handled by Keras when layers are connected.\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Reverted LSTM usage to avoid OOM; instead, raw context values are flattened and concatenated.\\n    - Added more rolling window statistics: median, range, first value.\\n    - Relies on raw context values, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs): # Removed lstm_units\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          Combined feature tensor for the MLP, shape (batch_size * NUM_NEURONS, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # --- 1. Per-neuron Context Processing (Direct Concatenation of raw values) ---\\n        # Reshape series_input from (B, C, N) to (B*N, C) for direct concatenation\\n        # Each neuron's C-length context becomes independent features.\\n        raw_context_features = tf.reshape(\\n            tf.transpose(series_input, perm=[0, 2, 1]), # (B, N, C)\\n            [batch_size * num_neurons, self.NUM_TIMESTEPS_CONTEXT] # (B*N, C)\\n        )\\n\\n        # Transpose original series_input to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        # This raw data is used for statistical features.\\n        raw_neuron_data_for_stats = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data_for_stats[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        # NEW FEATURES:\\n        neuron_first_value = raw_neuron_data_for_stats[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data_for_stats, axis=2) # Sort along the context dimension\\n        # Handle cases where C is odd or even for median calculation\\n        if self.NUM_TIMESTEPS_CONTEXT % 2 == 0:\\n            median_val = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0\\n        else:\\n            median_val = sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2]\\n        neuron_medians = tf.expand_dims(median_val, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        # NEW FEATURES - flattened\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Combine all numeric features (including raw context) ---\\n        combined_numeric_features = tf.concat([\\n            raw_context_features,    # Raw context values (B*N, C)\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            # NEW FEATURES\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            global_context_final     # (B*N, GLOBAL_STATE_DIM)\\n        ], axis=1) # Shape: (B*N, C + 8 + GLOBAL_STATE_DIM)\\n\\n\\n        # --- 4. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Concatenate all features (numeric features + neuron embedding)\\n        combined_features = tf.concat([combined_numeric_features, neuron_embedding], axis=1)\\n        return combined_features\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model leverages a shared-weights approach, training a single MLP that learns to predict\\n  the future activity for one neuron based on its past activity, enriched with both\\n  its own raw temporal context values, rolling statistics, distilled global brain activity context,\\n  and its unique ID.\\n  The feature engineering is now encapsulated and improved in a separate \`FeatureExtractor\` layer.\\n  Improvements:\\n  - Reverted LSTM usage in FeatureExtractor to resolve OOM errors, using direct concatenation of raw context.\\n  - More comprehensive statistical feature engineering in FeatureExtractor.\\n  - Integration of Residual Blocks in the MLP for deeper and more stable training.\\n  - Dynamic learning rate scheduling using Cosine Annealing with Warm Restarts.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_LENGTH\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    # MLP Architecture with Residual Blocks\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks\\n    # Each block's input and output dimensions are self.RESIDUAL_BLOCK_UNITS\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block4') # Added another block for depth\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='linear',\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the encapsulated FeatureExtractor\\n    and Residual Blocks.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate combined features\\n    combined_features = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    # Configure optimizer and callbacks\\n    steps_per_epoch = len(train_data_loader)\\n    first_decay_steps = 15 * steps_per_epoch # Initial cycle length of 15 epochs\\n\\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\\n        initial_learning_rate=1e-3, # Starting learning rate\\n        first_decay_steps=first_decay_steps,\\n        t_mul=2.0, # Multiply the cycle length by 2 each restart\\n        m_mul=0.9, # Multiply the learning rate by 0.9 each restart\\n        alpha=0.01 # Minimum learning rate as a fraction of initial_learning_rate\\n    )\\n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)) # Increased patience\\n\\n    # Compile the model with the chosen optimizer and MAE loss.\\n    self.compile(optimizer=optimizer, loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, N).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# --- Model Instantiation and Training ---\\n# Get num_neurons from constants\\nNUM_NEURONS_GLOBAL = constants.NUM_NEURONS # Use the constant value for num_neurons\\n\\nmodel = Model(\\n    num_timesteps_context=num_timesteps_context,\\n    num_neurons=NUM_NEURONS_GLOBAL,\\n    prediction_window_length=constants.PREDICTION_WINDOW_LENGTH\\n)\\n\\n# Create validation data loader\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation as per problem statement\\n  config = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Use 'val' split\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config, prefetch=True))\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # Use same number of epochs\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Train the model\\nmodel.fit(train_data_loader, val_data_loader=val_data_loader, epochs=100) # Increased epochs for better training",
  "new_index": 31,
  "new_code": "import tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport pandas as pd\\nfrom zapbench import constants\\nfrom zapbench.ts_forecasting import data_source\\nimport grain.python as grain # Import grain for the data loaders\\n\\n# NEW: Custom Residual Block Layer\\nclass ResidualBlock(keras.layers.Layer):\\n    \\"\\"\\"\\n    A simple residual block for MLP.\\n    Consists of Dense, BatchNormalization, Dropout, and a skip connection.\\n    The output dimension of the Dense layer must match the input dimension for the skip connection.\\n    \\"\\"\\"\\n    def __init__(self, units, dropout_rate, activation='relu', **kwargs):\\n        super().__init__(**kwargs)\\n        self.dense = keras.layers.Dense(units, activation=None) # Activation applied after addition\\n        self.bn = keras.layers.BatchNormalization()\\n        self.dropout = keras.layers.Dropout(dropout_rate)\\n        self.activation_fn = keras.activations.get(activation)\\n        # Ensure that if input_shape is known at build time, self.dense.build is called.\\n        # This is implicitly handled by Keras when layers are connected.\\n\\n    def call(self, inputs, training=False):\\n        x = self.dense(inputs)\\n        x = self.bn(x, training=training)\\n        x = self.dropout(x, training=training)\\n        # Add residual connection: original input + transformed input\\n        output = self.activation_fn(x + inputs) # Assuming inputs and x have same shape (units)\\n        return output\\n\\nclass FeatureExtractor(keras.layers.Layer):\\n    \\"\\"\\"\\n    Custom Keras Layer to encapsulate all feature engineering for the ZapBench model.\\n    Takes raw neural activity and generates rich features for a shared-weight MLP.\\n    Improvements for C=4 context:\\n    - Reverted LSTM usage to avoid OOM; instead, raw context values are flattened and concatenated.\\n    - Added more rolling window statistics: median, range, first value.\\n    - Relies on raw context values, enhanced rolling window statistics, a learned global brain state,\\n      and neuron ID embeddings to form comprehensive features.\\n    \\"\\"\\"\\n    def __init__(self, num_timesteps_context: int, num_neurons: int, **kwargs): # Removed lstm_units\\n        super().__init__(**kwargs)\\n        self.NUM_NEURONS = num_neurons\\n        self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n        # Index for the most recent past activity value (t-0)\\n        self.last_value_index = num_timesteps_context - 1\\n        # Index for the first value in context\\n        self.first_value_index = 0\\n\\n        # Learned Global Brain State Encoder (Improved Aggregation)\\n        # Increased dimension for potentially richer global context\\n        self.GLOBAL_STATE_DIM = 64\\n        self.global_state_encoder = keras.layers.TimeDistributed(\\n            keras.layers.Dense(self.GLOBAL_STATE_DIM, activation='relu', name='global_state_dense'),\\n            name='global_state_encoder_in_extractor'\\n        )\\n        self.global_state_aggregator_pooling = keras.layers.GlobalAveragePooling1D(\\n            name='global_state_aggregator_pooling_in_extractor'\\n        )\\n\\n        # Neuron ID Embedding Layer\\n        self.EMBEDDING_DIM = 64 # Increased dimensionality for the neuron ID embedding\\n        self.neuron_embedding_layer = keras.layers.Embedding(\\n            input_dim=self.NUM_NEURONS,\\n            output_dim=self.EMBEDDING_DIM,\\n            name='neuron_embedding_in_extractor'\\n        )\\n        self.flatten_embedding = keras.layers.Flatten()\\n\\n    def call(self, series_input: tf.Tensor) -> tf.Tensor:\\n        \\"\\"\\"\\n        Performs the forward pass of the feature extractor.\\n\\n        Args:\\n          series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n\\n        Returns:\\n          Combined feature tensor for the MLP, shape (batch_size * NUM_NEURONS, total_feature_dim).\\n        \\"\\"\\"\\n        batch_size = tf.shape(series_input)[0]\\n        num_neurons = tf.shape(series_input)[2] # Use dynamic num_neurons from input tensor\\n\\n        # --- 1. Per-neuron Context Processing (Direct Concatenation of raw values) ---\\n        # Reshape series_input from (B, C, N) to (B*N, C) for direct concatenation\\n        # Each neuron's C-length context becomes independent features.\\n        raw_context_features = tf.reshape(\\n            tf.transpose(series_input, perm=[0, 2, 1]), # (B, N, C)\\n            [batch_size * num_neurons, self.NUM_TIMESTEPS_CONTEXT] # (B*N, C)\\n        )\\n\\n        # Transpose original series_input to (batch_size, NUM_NEURONS, C) for easier per-neuron context processing.\\n        # This raw data is used for statistical features.\\n        raw_neuron_data_for_stats = tf.transpose(series_input, perm=[0, 2, 1]) # Shape: (B, N, C)\\n\\n        # Calculate mean, std, min, max, last_value, first_value, median, range for each neuron's C context window.\\n        neuron_means = tf.reduce_mean(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_stds = tf.math.reduce_std(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape: (B, N, 1)\\n        neuron_mins = tf.reduce_min(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_maxs = tf.reduce_max(raw_neuron_data_for_stats, axis=2, keepdims=True) # Shape (B, N, 1)\\n        neuron_last_value = raw_neuron_data_for_stats[:, :, self.last_value_index:self.last_value_index+1] # Shape (B, N, 1)\\n        # NEW FEATURES:\\n        neuron_first_value = raw_neuron_data_for_stats[:, :, self.first_value_index:self.first_value_index+1] # Shape (B, N, 1)\\n        # For median on C=4, sort and take average of middle two\\n        sorted_neuron_data = tf.sort(raw_neuron_data_for_stats, axis=2) # Sort along the context dimension\\n        # Handle cases where C is odd or even for median calculation\\n        if self.NUM_TIMESTEPS_CONTEXT % 2 == 0:\\n            median_val = (sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 -1 ] +\\n                          sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2 ]) / 2.0\\n        else:\\n            median_val = sorted_neuron_data[:, :, self.NUM_TIMESTEPS_CONTEXT // 2]\\n        neuron_medians = tf.expand_dims(median_val, axis=-1) # Shape (B, N, 1)\\n        neuron_ranges = neuron_maxs - neuron_mins # Shape (B, N, 1)\\n\\n\\n        # Flatten these to match the (B*N) batching\\n        neuron_means_flat = tf.reshape(neuron_means, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_stds_flat = tf.reshape(neuron_stds, [-1, 1]) # Shape: (B*N, 1)\\n        neuron_mins_flat = tf.reshape(neuron_mins, [-1, 1]) # Shape (B*N, 1)\\n        neuron_maxs_flat = tf.reshape(neuron_maxs, [-1, 1]) # Shape (B*N, 1)\\n        neuron_last_value_flat = tf.reshape(neuron_last_value, [-1, 1]) # Shape (B*N, 1)\\n        # NEW FEATURES - flattened\\n        neuron_first_value_flat = tf.reshape(neuron_first_value, [-1, 1]) # Shape (B*N, 1)\\n        neuron_medians_flat = tf.reshape(neuron_medians, [-1, 1]) # Shape (B*N, 1)\\n        neuron_ranges_flat = tf.reshape(neuron_ranges, [-1, 1]) # Shape (B*N, 1)\\n\\n\\n        # --- 2. Global Time-Dependent Aggregates (Learned) ---\\n        # Input series_input (B, C, N) -> TimeDistributed Dense -> (B, C, GLOBAL_STATE_DIM)\\n        global_brain_state_embeddings = self.global_state_encoder(series_input) # Shape: (B, C, GLOBAL_STATE_DIM)\\n\\n        # Aggregate the learned global states across the context timesteps using GlobalAveragePooling1D\\n        # (B, C, GLOBAL_STATE_DIM) -> GlobalAveragePooling1D -> (B, GLOBAL_STATE_DIM)\\n        global_context_aggregated = self.global_state_aggregator_pooling(global_brain_state_embeddings) # Shape: (B, GLOBAL_STATE_DIM)\\n\\n        # Tile this aggregated global context to match (B*N) for concatenation with other features\\n        global_context_final = tf.reshape(\\n            tf.tile(global_context_aggregated[:, tf.newaxis, :], [1, num_neurons, 1]),\\n            [-1, self.GLOBAL_STATE_DIM]\\n        ) # Shape: (B*N, GLOBAL_STATE_DIM)\\n\\n\\n        # --- 3. Combine all numeric features (including raw context) ---\\n        combined_numeric_features = tf.concat([\\n            raw_context_features,    # Raw context values (B*N, C)\\n            neuron_means_flat,       # (B*N, 1)\\n            neuron_stds_flat,        # (B*N, 1)\\n            neuron_mins_flat,        # (B*N, 1)\\n            neuron_maxs_flat,        # (B*N, 1)\\n            neuron_last_value_flat,  # (B*N, 1)\\n            # NEW FEATURES\\n            neuron_first_value_flat, # (B*N, 1)\\n            neuron_medians_flat,     # (B*N, 1)\\n            neuron_ranges_flat,      # (B*N, 1)\\n            global_context_final     # (B*N, GLOBAL_STATE_DIM)\\n        ], axis=1) # Shape: (B*N, C + 8 + GLOBAL_STATE_DIM)\\n\\n\\n        # --- 4. Generate Neuron IDs dynamically for the current batch and embed ---\\n        neuron_ids_base = tf.range(self.NUM_NEURONS, dtype=tf.int32) # Shape: (N,)\\n        # Repeat neuron IDs for each sample in the batch to match the (B*N) flattened structure\\n        X_neuron_ids_final = tf.tile(neuron_ids_base, [batch_size]) # Shape: (B*N,)\\n        X_neuron_ids_final = tf.reshape(X_neuron_ids_final, [-1, 1]) # Shape: (B*N, 1)\\n\\n        neuron_embedding = self.neuron_embedding_layer(X_neuron_ids_final) # Shape: (B*N, 1, EMBEDDING_DIM)\\n        neuron_embedding = self.flatten_embedding(neuron_embedding) # Shape: (B*N, EMBEDDING_DIM)\\n\\n        # Concatenate all features (numeric features + neuron embedding)\\n        combined_features = tf.concat([combined_numeric_features, neuron_embedding], axis=1)\\n        return combined_features\\n\\n\\nclass Model(keras.Model):\\n  \\"\\"\\"\\n  An enhanced Multi-Layer Perceptron (MLP) model for multivariate time series forecasting.\\n  This model leverages a shared-weights approach, training a single MLP that learns to predict\\n  the future activity for one neuron based on its past activity, enriched with both\\n  its own raw temporal context values, rolling statistics, distilled global brain activity context,\\n  and its unique ID.\\n  The feature engineering is now encapsulated and improved in a separate \`FeatureExtractor\` layer.\\n  Improvements:\\n  - Reverted LSTM usage in FeatureExtractor to resolve OOM errors, using direct concatenation of raw context.\\n  - More comprehensive statistical feature engineering in FeatureExtractor.\\n  - Integration of Residual Blocks in the MLP for deeper and more stable training.\\n  - Dynamic learning rate scheduling using Cosine Annealing with Warm Restarts.\\n  \\"\\"\\"\\n  def __init__(self, num_timesteps_context: int, num_neurons: int, prediction_window_length: int):\\n    super().__init__() # Initialize the base Keras Model class\\n\\n    # Store constants passed during instantiation.\\n    self.NUM_NEURONS = num_neurons\\n    self.PREDICTION_WINDOW_LENGTH = prediction_window_length # Corrected typo here\\n    self.NUM_TIMESTEPS_CONTEXT = num_timesteps_context\\n\\n    # Encapsulate all feature engineering into a dedicated layer\\n    self.feature_extractor = FeatureExtractor(\\n        num_timesteps_context=num_timesteps_context,\\n        num_neurons=num_neurons,\\n        name='feature_extractor_layer'\\n    )\\n\\n    # MLP Architecture with Residual Blocks\\n    self.initial_batchnorm = keras.layers.BatchNormalization(name='initial_bn')\\n\\n    # Initial projection layer to match the dimension of subsequent residual blocks\\n    self.RESIDUAL_BLOCK_UNITS = 256 # Consistent hidden layer size for residual blocks\\n    self.initial_projection_dense = keras.layers.Dense(\\n        self.RESIDUAL_BLOCK_UNITS, activation='relu', name='initial_projection_dense'\\n    )\\n\\n    # Residual Blocks\\n    # Each block's input and output dimensions are self.RESIDUAL_BLOCK_UNITS\\n    self.res_block1 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block1')\\n    self.res_block2 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block2')\\n    self.res_block3 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block3')\\n    self.res_block4 = ResidualBlock(self.RESIDUAL_BLOCK_UNITS, dropout_rate=0.25, name='res_block4') # Added another block for depth\\n\\n    # Final output layer\\n    self.final_output_dense = keras.layers.Dense(\\n        self.PREDICTION_WINDOW_LENGTH,\\n        activation='linear',\\n        name='final_output_dense'\\n    )\\n\\n  def call(self, series_input: tf.Tensor, training: bool = False) -> tf.Tensor:\\n    \\"\\"\\"\\n    Performs the forward pass of the model, now using the encapsulated FeatureExtractor\\n    and Residual Blocks.\\n\\n    Args:\\n      series_input: Raw neural activity data, shape (batch_size, C, NUM_NEURONS).\\n      training: Boolean indicating whether the model is in training mode. Used by BatchNormalization and Dropout.\\n\\n    Returns:\\n      Predicted neural activity, shape (batch_size * NUM_NEURONS, H).\\n    \\"\\"\\"\\n    # Use the encapsulated feature extractor layer to generate combined features\\n    combined_features = self.feature_extractor(series_input)\\n\\n    # Apply initial batch normalization\\n    x = self.initial_batchnorm(combined_features, training=training)\\n\\n    # Project features to the dimension of residual blocks\\n    x = self.initial_projection_dense(x)\\n\\n    # Pass through residual blocks\\n    x = self.res_block1(x, training=training)\\n    x = self.res_block2(x, training=training)\\n    x = self.res_block3(x, training=training)\\n    x = self.res_block4(x, training=training)\\n\\n    # Final prediction layer\\n    output = self.final_output_dense(x) # Shape: (B*N, H)\\n    return output\\n\\n  def _raw_data_generator(self, data_loader_instance):\\n    \\"\\"\\"\\n    A Python generator to yield raw (series_input, series_output) pairs\\n    from a grain.DataLoader. These are NumPy arrays as provided by grain.\\n    \\"\\"\\"\\n    for element in data_loader_instance:\\n      yield element['series_input'], element['series_output']\\n\\n  @tf.function # Decorate for graph mode execution\\n  def _prepare_targets_for_training(self, series_input_batch: tf.Tensor, series_output_batch: tf.Tensor):\\n    \\"\\"\\"\\n    Prepares the target tensor (y_true) for training by reshaping it\\n    to match the model's output shape (batch_size * NUM_NEURONS, H).\\n    The series_input_batch is returned as is, for the model's \`call\` method.\\n    \\"\\"\\"\\n    # Reshape the output batch (targets) from (B, H, N) to (B, N, H)\\n    y_target = tf.transpose(series_output_batch, perm=[0, 2, 1]) # Shape: (B, N, H)\\n    y_target = tf.reshape(y_target, [-1, self.PREDICTION_WINDOW_LENGTH]) # Shape: (B*N, H)\\n    return series_input_batch, y_target\\n\\n  def fit(self, train_data_loader, val_data_loader=None, epochs=10):\\n    \\"\\"\\"\\n    Trains the shared-weight MLP model using the provided data loaders via Keras's Model.fit().\\n\\n    Args:\\n      train_data_loader: A grain.DataLoader instance for training data.\\n      val_data_loader: An optional grain.DataLoader instance for validation data.\\n      epochs: Number of epochs to train for.\\n    \\"\\"\\"\\n    print(f\\"\\\\nStarting training for {epochs} epochs using Keras model.fit()...\\")\\n\\n    # Create tf.data.Dataset from generators for efficient data pipeline.\\n    train_dataset = tf.data.Dataset.from_generator(\\n        lambda: self._raw_data_generator(train_data_loader),\\n        output_signature=(\\n            tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n            tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n        )\\n    ).map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n    validation_dataset = None\\n    callbacks = []\\n\\n    # Configure optimizer and callbacks\\n    steps_per_epoch = len(train_data_loader)\\n    first_decay_steps = 15 * steps_per_epoch # Initial cycle length of 15 epochs\\n\\n    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\\n        initial_learning_rate=1e-3, # Starting learning rate\\n        first_decay_steps=first_decay_steps,\\n        t_mul=2.0, # Multiply the cycle length by 2 each restart\\n        m_mul=0.9, # Multiply the learning rate by 0.9 each restart\\n        alpha=0.01 # Minimum learning rate as a fraction of initial_learning_rate\\n    )\\n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\\n\\n    if val_data_loader:\\n        validation_dataset = tf.data.Dataset.from_generator(\\n            lambda: self._raw_data_generator(val_data_loader),\\n            output_signature=(\\n                tf.TensorSpec(shape=(None, self.NUM_TIMESTEPS_CONTEXT, self.NUM_NEURONS), dtype=tf.float32),\\n                tf.TensorSpec(shape=(None, self.PREDICTION_WINDOW_LENGTH, self.NUM_NEURONS), dtype=tf.float32)\\n            )\\n        )\\n        validation_dataset = validation_dataset.map(self._prepare_targets_for_training, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\\n\\n        callbacks.append(keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)) # Increased patience\\n\\n    # Compile the model with the chosen optimizer and MAE loss.\\n    self.compile(optimizer=optimizer, loss='mae')\\n\\n    # Use the parent's fit method\\n    super().fit(\\n        train_dataset,\\n        epochs=epochs,\\n        validation_data=validation_dataset,\\n        callbacks=callbacks,\\n        verbose=1 # Show Keras training progress bar\\n    )\\n    print(f\\"Training completed after {epochs} epochs (or early stopping).\\")\\n\\n  def predict_non_batched(self, series_input: np.ndarray) -> np.ndarray:\\n    \\"\\"\\"Predict the series_output for a single timestep given the series_input.\\n\\n    Args:\\n      series_input: np.ndarray of shape (context, neuron)\\n\\n    Returns:\\n      np.ndarray of shape (steps_ahead, neuron)\\n    \\"\\"\\"\\n    # Convert input NumPy array to TensorFlow tensor and add a batch dimension.\\n    series_input_tensor = tf.expand_dims(tf.convert_to_tensor(series_input, dtype=tf.float32), axis=0) # Shape: (1, C, N)\\n\\n    # Make predictions using the model's \`call\` method.\\n    # The \`call\` method performs all feature engineering internally.\\n    # The output will be (1 * NUM_NEURONS, PREDICTION_WINDOW_LENGTH).\\n    predictions_flat = self(series_input_tensor, training=False) # Use self() to invoke the call method\\n\\n    # Reshape predictions back to (NUM_NEURONS, H) then transpose to (H, N).\\n    predictions_per_neuron = tf.reshape(predictions_flat, [self.NUM_NEURONS, self.PREDICTION_WINDOW_LENGTH]) # Shape: (N, H)\\n    prediction = tf.transpose(predictions_per_neuron, perm=[1, 0]) # Shape: (H, N)\\n\\n    return prediction.numpy()\\n\\n# --- Model Instantiation and Training ---\\n# Dynamically get num_neurons from the data source\\n# Assumes 'source' (training data loader's source) is already defined and pre-fetched\\nNUM_NEURONS_GLOBAL = source[0]['series_input'].shape[2] \\n\\nmodel = Model(\\n    num_timesteps_context=num_timesteps_context,\\n    num_neurons=NUM_NEURONS_GLOBAL,\\n    prediction_window_length=constants.PREDICTION_WINDOW_LENGTH\\n)\\n\\n# Create validation data loader\\nval_sources = []\\nfor condition_id in constants.CONDITIONS_TRAIN: # Use training conditions for validation as per problem statement\\n  config = data_source.TensorStoreTimeSeriesConfig(\\n      input_spec=data_utils.adjust_spec_for_condition_and_split(\\n          condition=condition_id,\\n          split='val', # Use 'val' split\\n          spec=data_utils.get_spec('240930_traces'),\\n          num_timesteps_context=num_timesteps_context),\\n      timesteps_input=num_timesteps_context,\\n      timesteps_output=constants.PREDICTION_WINDOW_LENGTH,\\n  )\\n  val_sources.append(data_source.TensorStoreTimeSeries(config, prefetch=True))\\nval_source = data_source.ConcatenatedTensorStoreTimeSeries(*val_sources)\\n\\nval_index_sampler = grain.IndexSampler(\\n    num_records=len(val_source),\\n    num_epochs=num_epochs, # Use same number of epochs\\n    shard_options=grain.ShardOptions(\\n        shard_index=0, shard_count=1, drop_remainder=True),\\n    shuffle=False, # No need to shuffle validation data\\n    seed=101\\n)\\n\\nval_data_loader = grain.DataLoader(\\n    data_source=val_source,\\n    sampler=val_index_sampler,\\n    operations=[\\n        grain.Batch(\\n            batch_size=batch_size, drop_remainder=True)\\n    ],\\n    worker_count=0\\n)\\n\\n# Train the model\\nmodel.fit(train_data_loader, val_data_loader=val_data_loader, epochs=100) # Increased epochs for better training"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

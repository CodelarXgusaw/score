<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 111.0,
  "old_code": "# TODO: Implement the \`quadrature()\` function to numerically evaluate integrals.\\n\\nimport math\\nfrom typing import Any, Callable, Tuple\\nimport numpy as np\\nimport scipy.integrate\\nimport scipy.optimize\\n\\ndef quadrature(\\n    integrand: Callable[float, float],\\n    lower_limit: float,\\n    upper_limit: float,\\n) -> Tuple[float, float]:\\n    \\"\\"\\"\\n    Estimate the numeric value of the definite integral, especially\\n    when scipy.integrate.quad() fails to converge or returns a large error\\n    estimate or NaN.\\n    \\"\\"\\"\\n\\n    # --- Step 1: Initial Attempt with scipy.integrate.quad() ---\\n    try:\\n        # Push scipy.integrate.quad a bit harder with higher limits and absolute tolerance\\n        # Increased limit and tightened epsilons even further for the primary attempt.\\n        answer, error_estimate = scipy.integrate.quad(\\n            integrand, lower_limit, upper_limit,\\n            limit=1000,      # Max subintervals, increased from 200/500\\n            epsabs=1e-15,   # Absolute error tolerance, tightened from 1e-12/1e-14\\n            epsrel=1e-15    # Relative error tolerance, tightened from 1e-12/1e-14\\n        )\\n\\n        # Check for NaN or Inf results/errors from quad, or if error is too high\\n        # Adjusted heuristic for \\"unreliable\\": if relative error is > 0.5% or absolute error > 5e-8 for small answers\\n        if (np.isnan(answer) or np.isinf(answer) or\\n            np.isnan(error_estimate) or np.isinf(error_estimate) or\\n            (abs(answer) > 1e-8 and error_estimate / abs(answer) > 0.005) or # Tighter relative error check (0.5%)\\n            (abs(answer) <= 1e-8 and error_estimate > 5e-8)): # Tighter absolute error check for tiny results\\n            raise ValueError(\\"scipy.integrate.quad failed to converge reliably or error is too high.\\")\\n            \\n        return answer, error_estimate\\n    except Exception:\\n        # If quad fails or raises an error, fall back to a more robust method for oscillatory integrals\\n        pass # Continue to specialized method\\n\\n    # --- Step 2: Handle Infinite Limits for Specialized Method ---\\n    # Convert all integrals to the form [a, inf) for uniform processing.\\n    if lower_limit == -np.inf and upper_limit == np.inf:\\n        # Split into two infinite integrals: (-inf, 0] and [0, inf)\\n        # Recursively call quadrature for the split parts\\n        val1, err1 = quadrature(integrand, -np.inf, 0)\\n        val2, err2 = quadrature(integrand, 0, np.inf)\\n        \\n        # If either sub-integral failed, propagate failure.\\n        if np.isnan(val1) or np.isnan(val2) or np.isinf(err1) or np.isinf(err2):\\n            return np.nan, np.inf\\n        \\n        return val1 + val2, err1 + err2\\n    elif lower_limit == -np.inf and upper_limit != np.inf: # Case: (-inf, b]\\n        # Transform x -> -t, dx -> -dt. Integral becomes int_{-b}^{inf} f(-t) dt\\n        def transformed_integrand_neg_inf(t):\\n            return integrand(-t)\\n        return quadrature(transformed_integrand_neg_inf, -upper_limit, np.inf)\\n    elif upper_limit != np.inf: # Case: [a, b] where quad failed.\\n        # This implies a very difficult finite integral. Try again with aggressive quad, then Romberg.\\n        try:\\n            # Even more aggressive quad parameters for the finite interval fallback\\n            answer, error_estimate = scipy.integrate.quad(\\n                integrand, lower_limit, upper_limit,\\n                limit=2000, epsabs=1e-16, epsrel=1e-16 # Increased limit and tightened epsilons\\n            )\\n            # Stricter error check for finite integrals here.\\n            if (np.isnan(answer) or np.isinf(answer) or\\n                np.isnan(error_estimate) or np.isinf(error_estimate) or\\n                (abs(answer) > 1e-8 and error_estimate / abs(answer) > 0.001) or # Tighter relative error check (0.1%)\\n                (abs(answer) <= 1e-8 and error_estimate > 1e-8)): # Tighter absolute error check\\n                raise ValueError(\\"Aggressive quad also failed for finite interval.\\")\\n            return answer, error_estimate\\n        except Exception:\\n            try:\\n                # Romberg is good for smooth functions over finite intervals.\\n                # It may struggle with oscillations or singularities.\\n                # Increase \`divmax\` for more iterations.\\n                # Romberg's return value for error is not standard, so this is a heuristic.\\n                ans_romberg = scipy.integrate.romberg(integrand, lower_limit, upper_limit, divmax=35) # Increased divmax from 25/30\\n                # A more robust error estimation for Romberg can be the difference from the previous iteration.\\n                # Romberg function usually performs \`divmax\` iterations. We estimate error using the last and second to last.\\n                # This requires a trickier approach since romberg doesn't expose all intermediate results easily.\\n                # For simplicity, we just run it one level less and take the difference, though this is not ideal.\\n                ans_romberg_prev_level = scipy.integrate.romberg(integrand, lower_limit, upper_limit, divmax=max(1, ans_romberg._results.shape[0] - 2)) if hasattr(ans_romberg, '_results') else None\\n\\n                if ans_romberg_prev_level is None: # Fallback if _results is not available or too short\\n                    ans_romberg_prev_level = scipy.integrate.romberg(integrand, lower_limit, upper_limit, divmax=max(1, 34))\\n\\n                err_romberg = abs(ans_romberg - ans_romberg_prev_level) * 10 # Scaled for rough estimate\\n\\n                # If Romberg result is suspicious (e.g., zero with non-zero previous level diff, or huge error)\\n                if abs(ans_romberg) < 1e-12 and err_romberg > 1e-8: # If result is tiny but error is not\\n                    return np.nan, np.inf # Indicate failure\\n                    \\n                return ans_romberg, err_romberg\\n            except Exception:\\n                return np.nan, np.inf # Indicate failure\\n\\n    # --- Step 3: Specialized Fallback for Oscillatory Integrals on [a, inf) ---\\n    # This method is for [a, inf) where 'a' is finite.\\n    # It assumes the integrand oscillates and decays.\\n\\n    start_oscillation_point = lower_limit\\n    segment_length = math.pi  # Heuristic for half-period of common oscillations (sin(x), cos(x))\\n    \\n    # Store segment integrals for Euler transformation\\n    segment_integrals = []\\n    \\n    # Accumulate initial terms by direct integration over segments\\n    num_segments_to_try = 1000 # Increased number of segments to compute significantly (from 200/500)\\n    min_segments_for_euler = 10 # Minimum terms required for Euler transformation to be meaningful (increased from 5/7)\\n\\n    prev_seg_val = None\\n    consistent_sign_count = 0 # Count consecutive segments with the same sign (for non-alternating check)\\n    initial_oscillation_check_terms = 5 # Number of initial terms to check for basic oscillation before more stringent checks\\n\\n    for i in range(num_segments_to_try):\\n        seg_start = start_oscillation_point + i * segment_length\\n        seg_end = start_oscillation_point + (i + 1) * segment_length\\n\\n        try:\\n            # Use more aggressive quad parameters for segment integration as well\\n            seg_val, seg_err = scipy.integrate.quad(\\n                integrand, seg_start, seg_end,\\n                limit=1000, epsabs=1e-16, epsrel=1e-16 # Even tighter epsilons for segments\\n            )\\n            \\n            # If segment integral fails to converge to a small error, this whole method might be unsuitable\\n            # Also, if the segment value is NaN/Inf, that's a failure.\\n            if np.isnan(seg_val) or np.isinf(seg_val) or seg_err > 1e-12: # A more stringent error threshold for segments\\n                raise ValueError(f\\"Segment integration failed at {i}-th segment with val={seg_val}, err={seg_err}\\")\\n\\n            segment_integrals.append(seg_val)\\n\\n            # --- Convergence and Oscillation Checks ---\\n            if i > 0 and prev_seg_val is not None:\\n                # Check for alternating signs (crucial for Euler transformation) or consistent sign indicating non-oscillatory decay\\n                if (seg_val * prev_seg_val >= 0) and abs(seg_val) > 1e-16: # Not strictly alternating and non-zero\\n                    consistent_sign_count += 1\\n                    # If signs stop alternating *consistently* (e.g., 3+ times) after initial oscillation phase\\n                    if consistent_sign_count >= 3 and i >= initial_oscillation_check_terms: \\n                        # This suggests Euler transformation is not suitable (series is not alternating)\\n                        # print(f\\"Segment signs stopped alternating consistently at iteration {i}. Summing directly.\\")\\n                        return np.sum(segment_integrals), np.inf # High error, as Euler won't help much\\n                else:\\n                    consistent_sign_count = 0 # Reset if signs alternate\\n            \\n            # Check for decay: if terms are consistently very small, we might have converged\\n            # This decay check is more about the absolute value of the term itself.\\n            if i >= min_segments_for_euler and abs(seg_val) < 1e-17: # Extremely small absolute term\\n                # If sum of previous terms is also small, indicates overall convergence.\\n                if abs(np.sum(segment_integrals[:-1])) < 1e-12: # Sum is already very small\\n                    # print(f\\"Total sum is already tiny and segments decaying. Breaking at iteration {i}.\\")\\n                    break \\n                # If current term is very small relative to the last one, and previous one was not tiny, also break.\\n                if i > 0 and abs(segment_integrals[-2]) > 1e-10 and abs(seg_val) < 1e-10 * abs(segment_integrals[-2]):\\n                    # print(f\\"Segment terms decaying quickly relatively at iteration {i}. Breaking.\\")\\n                    break\\n            \\n            # If terms start to grow or fluctuate erratically, break as convergence is unlikely\\n            # If a term is significantly larger than the previous one, and not near zero.\\n            if i > initial_oscillation_check_terms and abs(seg_val) > abs(segment_integrals[-2]) * 10 and abs(segment_integrals[-2]) > 1e-12: # More aggressive growth check (factor 10)\\n                # This suggests the integral is not converging, or the segment_length is wrong.\\n                # print(f\\"Terms not decaying, or growing significantly at iteration {i}. Direct sum so far.\\")\\n                return np.sum(segment_integrals), np.inf # Return current sum with high error\\n            \\n            prev_seg_val = seg_val\\n\\n        except Exception as e:\\n            # If any segment integration fails, return the sum of successful segments\\n            # with a high error to indicate uncertainty.\\n            # print(f\\"scipy.integrate.quad failed for segment {seg_start}-{seg_end}: {e}\\")\\n            return np.sum(segment_integrals), np.inf\\n\\n    # --- Step 4: Apply Series Acceleration (Euler's Transformation) ---\\n    # This implementation uses the Euler-Knopp summation method (iterated arithmetic mean)\\n    # which is suitable for alternating series or series that converge slowly.\\n    \\n    N = len(segment_integrals)\\n    if N < min_segments_for_euler:\\n        # Not enough terms collected for Euler transformation, return simple sum\\n        return np.sum(segment_integrals), np.inf # High uncertainty\\n\\n    euler_sums = np.array(segment_integrals, dtype=float)\\n    \\n    # Store the 'diagonal' elements of the Euler transformation table for convergence check\\n    euler_diagonal_values = [euler_sums[0]] \\n\\n    try:\\n        # The Euler transformation involves repeatedly averaging adjacent terms.\\n        # We perform N-1 levels of transformation at most, as each level reduces term count by 1.\\n        max_euler_levels = N - 1\\n\\n        for m in range(max_euler_levels): \\n            if len(euler_sums) < 2: # Need at least two terms to average\\n                break\\n            \\n            # Compute the next level of sums by averaging adjacent elements\\n            new_euler_sums = (euler_sums[:-1] + euler_sums[1:]) / 2.0\\n            euler_sums = new_euler_sums\\n            \\n            euler_diagonal_values.append(euler_sums[0]) # The first element of the new level\\n            \\n            # Check for convergence of the Euler sum by comparing the last few diagonal elements\\n            # A common heuristic: if the difference between successive terms is very small relative to the term itself.\\n            # Check for stabilization of the Euler diagonal\\n            if m > 0:\\n                diff = abs(euler_diagonal_values[-1] - euler_diagonal_values[-2])\\n                # Converged if difference is very small relative to current sum or absolutely very small\\n                if (abs(euler_diagonal_values[-1]) > 1e-12 and diff < 1e-10 * abs(euler_diagonal_values[-1])) or \\\\\\n                   (abs(euler_diagonal_values[-1]) <= 1e-12 and diff < 1e-12):\\n                    break\\n                \\n            # Additional check: If the values on the diagonal start to diverge or oscillate wildly again\\n            if m > 2 and abs(euler_diagonal_values[-1]) > abs(euler_diagonal_values[-2]) * 100: # Very aggressive check for divergence in Euler\\n                # This suggests the Euler transformation is not converging or diverging.\\n                # Revert to the last stable sum or simple sum, with high error.\\n                # print(f\\"Euler diagonal values diverging at level {m}. Reverting to previous estimate.\\")\\n                final_answer = euler_diagonal_values[-2] if len(euler_diagonal_values) >= 2 else np.sum(segment_integrals)\\n                return final_answer, np.inf\\n\\n    except Exception as e:\\n        # print(f\\"Euler transformation encountered an error: {e}\\")\\n        pass\\n\\n    final_answer = euler_diagonal_values[-1] if len(euler_diagonal_values) > 0 else np.sum(segment_integrals)\\n    \\n    # Estimate error: difference between the last two diagonal terms (if available)\\n    # A more robust error estimate is often the last term that was significantly different,\\n    # or the last calculated \`a_k\` before acceleration stabilized.\\n    if len(euler_diagonal_values) >= 2:\\n        # The error is approximately the difference between the last two computed Euler sums.\\n        # Add a small buffer / scaling to be conservative for error estimate.\\n        error_estimate = abs(euler_diagonal_values[-1] - euler_diagonal_values[-2]) * 20 # Increased scaling for conservatism\\n    elif len(segment_integrals) > 0:\\n        # If only one Euler term was produced, use the magnitude of the last segment integral\\n        # as a rough indicator of remaining error, scaled up.\\n        error_estimate = abs(segment_integrals[-1]) * 100 # Even more conservative if only one Euler term\\n    else:\\n        error_estimate = np.inf # No segments successfully integrated\\n\\n    # Final check for NaN or Inf\\n    if np.isnan(final_answer) or np.isinf(final_answer):\\n        return np.nan, np.inf\\n\\n    return final_answer, error_estimate",
  "new_index": 124,
  "new_code": "import math\\nfrom typing import Any, Callable, Tuple\\nimport numpy as np\\nimport scipy.integrate\\nimport scipy.optimize\\n\\ndef _wynn_epsilon_accelerate(s_n_partial_sums: np.ndarray, tolerance: float = 1e-12, max_levels: int = 50) -> Tuple[float, float]:\\n    \\"\\"\\"\\n    Accelerates the convergence of a series using Wynn's epsilon algorithm.\\n    Args:\\n        s_n_partial_sums: A numpy array of partial sums S_k = a_0 + ... + a_k.\\n        tolerance: Relative/absolute tolerance for convergence.\\n        max_levels: Maximum number of acceleration levels to attempt.\\n    Returns:\\n        A tuple (accelerated_sum, estimated_error).\\n    \\"\\"\\"\\n    if len(s_n_partial_sums) == 0:\\n        return 0.0, 0.0\\n    if len(s_n_partial_sums) == 1:\\n        return s_n_partial_sums[0], np.inf # Not enough terms to accelerate\\n\\n    n_terms = len(s_n_partial_sums)\\n    \\n    # Initialize epsilon table\\n    # epsilon_table[j][k] stores epsilon_j^(k)\\n    # j: index along the sequence (row)\\n    # k: level of acceleration (column)\\n    # The table is dynamically sized or can be pre-allocated.\\n    # We only need enough rows to process the available partial sums.\\n    # We will use two columns (prev_eps_col and curr_eps_col) to optimize memory.\\n    \\n    # Initialize the first column (level 0) with the partial sums\\n    curr_eps_col = np.copy(s_n_partial_sums).astype(float)\\n    \\n    # Initialize prev_eps_col for k=1 computation. These are conceptual epsilon_j^(-1) = 0.0.\\n    prev_eps_col = np.zeros(n_terms, dtype=float)\\n    \\n    best_accelerated_sum = s_n_partial_sums[-1] # Default to the last partial sum\\n    min_estimated_error = np.inf\\n\\n    # Store a history of accelerated values to estimate error and detect explicit convergence\\n    accelerated_values_history = [s_n_partial_sums[0]] # epsilon_0^(0) is S_0\\n    \\n    for k_level in range(1, max_levels + 1):\\n        if k_level >= n_terms: # Cannot compute more levels than available terms\\n            break\\n        \\n        # Buffer for the next column of epsilon values\\n        next_eps_col = np.zeros(n_terms - k_level, dtype=float)\\n        \\n        for j_index in range(n_terms - k_level):\\n            # Recurrence relation: epsilon_j^(k) = epsilon_{j+1}^(k-2) + 1 / (epsilon_{j+1}^(k-1) - epsilon_j^(k-1))\\n            \\n            # The epsilon_{j+1}^(k-2) term needs values from two levels back.\\n            # This is \`epsilon_table[j_index+1, k_level-2]\`.\\n            # In our optimized space, this would be \`prev_prev_eps_col[j_index+1]\` if we tracked it explicitly.\\n            # For k_level=1, this term is conceptually 0.\\n            term_prev_prev = 0.0 if k_level == 1 else prev_prev_eps_col[j_index+1]\\n            \\n            # Denominator: (epsilon_{j+1}^(k-1) - epsilon_j^(k-1))\\n            # These values come from \`prev_eps_col\` (which stores epsilon_j^(k_level-1))\\n            denominator = prev_eps_col[j_index+1] - prev_eps_col[j_index]\\n            \\n            # Check for near-zero denominator (indicates instability or constant series)\\n            if abs(denominator) < tolerance * abs(prev_eps_col[j_index+1]):\\n                next_eps_col[j_index] = np.inf # Mark as unstable\\n            else:\\n                next_eps_col[j_index] = term_prev_prev + 1.0 / denominator\\n\\n            # Check for NaN/Inf in computed epsilon values; mark as unstable\\n            if np.isnan(next_eps_col[j_index]) or np.isinf(next_eps_col[j_index]):\\n                next_eps_col[j_index] = np.inf \\n        \\n        # If the first element of next_eps_col is unstable, then break the acceleration process\\n        if np.isnan(next_eps_col[0]) or np.isinf(next_eps_col[0]):\\n            break\\n\\n        # The accelerated sum for this level is epsilon_0^(k_level) (the first element of the new column)\\n        current_accel_value = next_eps_col[0]\\n        accelerated_values_history.append(current_accel_value)\\n        \\n        # Update columns for the next iteration:\\n        # The current prev_eps_col becomes the prev_prev_eps_col for the next step.\\n        # The current next_eps_col becomes the prev_eps_col for the next step.\\n        prev_prev_eps_col = prev_eps_col.copy()\\n        prev_eps_col = next_eps_col.copy()\\n        \\n        # Estimate error: difference from the previous valid accelerated value in history\\n        if k_level >= 1: # We need at least two terms in history to compare\\n            # The current error estimate is the absolute difference between epsilon_0^(k_level) and epsilon_0^(k_level-1)\\n            current_error = abs(accelerated_values_history[-1] - accelerated_values_history[-2])\\n            \\n            # Convergence check: if error is sufficiently small (absolute or relative)\\n            if (abs(current_accel_value) > tolerance and current_error < tolerance * abs(current_accel_value)) or \\\\\\n               (abs(current_accel_value) <= tolerance and current_error < tolerance):\\n                best_accelerated_sum = current_accel_value\\n                min_estimated_error = current_error\\n                return best_accelerated_sum, min_estimated_error # Explicit convergence!\\n            \\n            # Update best found result if this one has a smaller error so far\\n            if current_error < min_estimated_error:\\n                min_estimated_error = current_error\\n                best_accelerated_sum = current_accel_value\\n        else:\\n            # For the very first accelerated value (k_level=1), we can't compare it yet.\\n            # If it's stable, we set it as the best current result with infinite error.\\n            if min_estimated_error == np.inf and not (np.isnan(current_accel_value) or np.isinf(current_accel_value)):\\n                best_accelerated_sum = current_accel_value\\n\\n    # If no explicit convergence, return the best value found based on error criteria.\\n    return best_accelerated_sum, min_estimated_error\\n\\n\\ndef quadrature(\\n    integrand: Callable[float, float],\\n    lower_limit: float,\\n    upper_limit: float,\\n) -> Tuple[float, float]:\\n    \\"\\"\\"\\n    Estimate the numeric value of the definite integral, especially\\n    when scipy.integrate.quad() fails to converge or returns a large error\\n    estimate or NaN.\\n    \\"\\"\\"\\n\\n    # --- Step 1: Initial Attempt with scipy.integrate.quad() ---\\n    try:\\n        # Push scipy.integrate.quad with higher limits and tighter absolute/relative tolerance.\\n        # Only consider it failed if it returns NaN/Inf or explicitly raises an error.\\n        answer, error_estimate = scipy.integrate.quad(\\n            integrand, lower_limit, upper_limit,\\n            limit=1000,      # Max subintervals\\n            epsabs=1e-15,   # Absolute error tolerance\\n            epsrel=1e-15    # Relative error tolerance\\n        )\\n\\n        # If quad returns a finite answer and error, we trust it,\\n        # as its internal error estimate is generally reliable.\\n        if not (np.isnan(answer) or np.isinf(answer) or np.isnan(error_estimate) or np.isinf(error_estimate)):\\n            return answer, error_estimate\\n        else:\\n            # If NaN or Inf, it's a true failure, proceed to fallback.\\n            raise ValueError(\\"scipy.integrate.quad returned NaN or Inf.\\")\\n            \\n    except Exception as e:\\n        # If quad fails or raises an error, fall back to a more robust method for oscillatory integrals.\\n        # print(f\\"Initial quad failed for {lower_limit} to {upper_limit}: {e}. Trying specialized method.\\")\\n        pass # Continue to specialized method\\n\\n    # --- Step 2: Handle Infinite Limits for Specialized Method ---\\n    # Convert all integrals to the form [a, inf) for uniform processing.\\n    if lower_limit == -np.inf and upper_limit == np.inf:\\n        # Split into two infinite integrals: (-inf, 0] and [0, inf)\\n        val1, err1 = quadrature(integrand, -np.inf, 0)\\n        val2, err2 = quadrature(integrand, 0, np.inf)\\n        \\n        # If either sub-integral failed, propagate failure.\\n        if np.isnan(val1) or np.isnan(val2) or np.isinf(err1) or np.isinf(err2):\\n            return np.nan, np.inf\\n        \\n        return val1 + val2, err1 + err2\\n    elif lower_limit == -np.inf and upper_limit != np.inf: # Case: (-inf, b]\\n        # Transform x -> -t, dx -> -dt. Integral becomes int_{-upper_limit}^{inf} f(-t) dt\\n        def transformed_integrand_neg_inf(t):\\n            return integrand(-t)\\n        return quadrature(transformed_integrand_neg_inf, -upper_limit, np.inf)\\n    elif upper_limit != np.inf: # Case: [a, b] where initial quad failed.\\n        # This implies a very difficult finite integral. Try again with even more aggressive quad, then Romberg.\\n        try:\\n            # Even more aggressive quad parameters for the finite interval fallback\\n            answer, error_estimate = scipy.integrate.quad(\\n                integrand, lower_limit, upper_limit,\\n                limit=2000, epsabs=1e-16, epsrel=1e-16\\n            )\\n            if not (np.isnan(answer) or np.isinf(answer) or np.isnan(error_estimate) or np.isinf(error_estimate)):\\n                return answer, error_estimate\\n            else:\\n                raise ValueError(\\"Aggressive quad also returned NaN or Inf for finite interval.\\")\\n        except Exception as e:\\n            # print(f\\"Aggressive quad failed for finite interval: {e}. Trying Romberg.\\")\\n            try:\\n                # Romberg is good for smooth functions over finite intervals.\\n                # It may struggle with oscillations or singularities. Increase \`divmax\` for more iterations.\\n                ans_romberg = scipy.integrate.romberg(integrand, lower_limit, upper_limit, divmax=35)\\n                # Romberg doesn't provide a direct error estimate, so we use a heuristic based on its internal results\\n                # if available, or a fallback of the last difference if successful.\\n                # This is a bit tricky; a simpler heuristic for error: if it converged, assume small error or difference from previous iteration.\\n                # If romberg's internal \`_results\` table is available, its last element gives us some info.\\n                # The _results are usually the D_k values from the Romberg table.\\n                if hasattr(ans_romberg, '_results') and ans_romberg._results.shape[0] > 1:\\n                    last_two_results = ans_romberg._results[-2:]\\n                    err_romberg = abs(last_two_results[-1] - last_two_results[0]) # Difference between last two steps\\n                else: # Fallback if _results is not available or too short\\n                    err_romberg = abs(ans_romberg) * 1e-6 # A very conservative guess\\n                    \\n                # If Romberg result is suspicious (e.g., NaN/Inf or huge error), propagate failure.\\n                if np.isnan(ans_romberg) or np.isinf(ans_romberg) or np.isinf(err_romberg):\\n                    raise ValueError(\\"Romberg returned NaN or Inf or huge error.\\")\\n                \\n                return ans_romberg, err_romberg\\n            except Exception as e:\\n                # print(f\\"Romberg failed for finite interval: {e}.\\")\\n                return np.nan, np.inf # Indicate failure for this difficult finite integral\\n\\n    # --- Step 3: Specialized Fallback for Oscillatory Integrals on [a, inf) ---\\n    # This method is for [a, inf) where 'a' is finite.\\n    # It assumes the integrand oscillates and decays.\\n\\n    start_oscillation_point = lower_limit\\n    segment_length = math.pi  # Heuristic for half-period of common oscillations (sin(x), cos(x))\\n    \\n    # Store segment integrals for series acceleration\\n    segment_integrals_terms = []\\n    \\n    num_segments_to_try = 1500 # Increased number of segments to compute (from 1000) for more terms for Wynn's\\n    min_segments_for_wynn = 5 # Minimum terms required for Wynn transformation to be meaningful\\n\\n    # Heuristics for direct sum convergence or divergence\\n    small_term_threshold = 1e-15 # Absolute value indicating a very small segment\\n    consecutive_small_terms = 5 # How many consecutive small terms to consider for direct sum convergence\\n    growth_factor_threshold = 100 # How much a term can grow before considering divergence\\n    num_terms_for_growth_check = 5 # Minimum terms to check for growth/divergence\\n\\n    consecutive_small_count = 0\\n    \\n    for i in range(num_segments_to_try):\\n        seg_start = start_oscillation_point + i * segment_length\\n        seg_end = start_oscillation_point + (i + 1) * segment_length\\n\\n        try:\\n            # Use aggressive quad parameters for segment integration\\n            seg_val, seg_err = scipy.integrate.quad(\\n                integrand, seg_start, seg_end,\\n                limit=1000, epsabs=1e-16, epsrel=1e-16\\n            )\\n            \\n            # If segment integral returns NaN/Inf or very high error, mark it as unstable\\n            # but try to continue collecting other segments. Wynn's can sometimes tolerate some issues.\\n            if np.isnan(seg_val) or np.isinf(seg_val) or seg_err > 1e-10: # A slightly more lenient error threshold for segments\\n                segment_integrals_terms.append(np.nan) # Mark as unstable\\n            else:\\n                segment_integrals_terms.append(seg_val)\\n\\n            # --- Convergence and Divergence Checks ---\\n            if i >= num_terms_for_growth_check and not np.isnan(seg_val):\\n                last_good_terms = [t for t in segment_integrals_terms[-num_terms_for_growth_check:] if not np.isnan(t)]\\n                if len(last_good_terms) >= 2:\\n                    prev_seg_val = last_good_terms[-2]\\n\\n                    # Check for absolute decay: if terms are consistently very small\\n                    if abs(seg_val) < small_term_threshold:\\n                        consecutive_small_count += 1\\n                        if consecutive_small_count >= consecutive_small_terms:\\n                            # If total sum of good terms is also small, indicates overall convergence.\\n                            good_terms = [t for t in segment_integrals_terms if not np.isnan(t)]\\n                            if abs(np.sum(good_terms)) < 1e-10: \\n                                # print(f\\"Total sum is already tiny and segments decaying. Breaking at iteration {i}.\\")\\n                                break # Converged enough by direct sum\\n\\n                    else:\\n                        consecutive_small_count = 0 # Reset if term is not small\\n\\n                    # Check for divergence: if terms start to grow significantly and are not near zero\\n                    if abs(prev_seg_val) > small_term_threshold and abs(seg_val) > abs(prev_seg_val) * growth_factor_threshold:\\n                        # This suggests the integral is not converging, or the segment_length is wrong.\\n                        # print(f\\"Terms not decaying, or growing significantly at iteration {i}. Returning direct sum.\\")\\n                        good_terms = [t for t in segment_integrals_terms if not np.isnan(t)]\\n                        return np.sum(good_terms), np.inf # Return current sum with high error\\n\\n        except Exception as e:\\n            # If any segment integration itself raises an error, return the sum of successful segments\\n            # with a high error to indicate uncertainty.\\n            # print(f\\"scipy.integrate.quad raised error for segment {seg_start}-{seg_end}: {e}. Returning direct sum.\\")\\n            good_terms = [t for t in segment_integrals_terms if not np.isnan(t)]\\n            return np.sum(good_terms), np.inf\\n\\n    # --- Step 4: Apply Series Acceleration (Wynn's Epsilon Transformation) ---\\n    \\n    # Filter out NaN/Inf values before passing to Wynn's, as it doesn't handle them well.\\n    filtered_terms = np.array([term for term in segment_integrals_terms if not (np.isnan(term) or np.isinf(term))])\\n\\n    if len(filtered_terms) < min_segments_for_wynn:\\n        # Not enough stable terms collected for Wynn transformation, return simple sum\\n        # print(f\\"Not enough stable terms ({len(filtered_terms)}) for Wynn's. Returning direct sum.\\")\\n        return np.sum(filtered_terms), np.inf # High uncertainty\\n\\n    try:\\n        # Call Wynn's epsilon algorithm on the partial sums of the collected terms\\n        accelerated_sum, wynn_error = _wynn_epsilon_accelerate(np.cumsum(filtered_terms), tolerance=1e-12, max_levels=50)\\n\\n        # Final check for NaN or Inf from Wynn's\\n        if np.isnan(accelerated_sum) or np.isinf(accelerated_sum) or np.isinf(wynn_error) or wynn_error > 1.0: # If Wynn's error is too large, it likely failed.\\n            # print(f\\"Wynn's returned NaN/Inf or huge error. Reverting to direct sum of good terms.\\")\\n            return np.sum(filtered_terms), np.inf\\n        \\n        return accelerated_sum, wynn_error\\n\\n    except Exception as e:\\n        # If Wynn's algorithm itself encounters an error, revert to simple sum\\n        # print(f\\"Wynn's algorithm encountered an error: {e}. Reverting to direct sum of good terms.\\")\\n        return np.sum(filtered_terms), np.inf"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

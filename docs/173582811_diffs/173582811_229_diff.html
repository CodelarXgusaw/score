<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 193.0,
  "old_code": "# [rewrite_cell]\\nfrom typing import Any\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom scipy.sparse import csr_matrix, coo_matrix, lil_matrix\\nimport numpy as np\\nimport pandas as pd # Added for efficient de-duplication\\nimport scanpy as sc\\nimport anndata as ad\\nimport heapq # For efficiently getting top K elements from merged lists\\n\\n# Define parameters for the config.\\n# Adjusted values:\\n# - n_pca_components: Increased to capture more variance in potentially complex datasets.\\n# - n_neighbors_per_batch: Increased to consider a larger local neighborhood within each batch.\\n# - total_k_neighbors: Increased to ensure a robust overall integrated graph.\\nconfig = {\\n    'n_pca_components': 100,  # Recommended: 50-200. Increased for potentially richer embedding.\\n    'n_neighbors_per_batch': 10,  # Recommended: 5-15. Increased for more robust batch-local neighborhoods.\\n    'total_k_neighbors': 50,  # Recommended: 15-100. Increased for denser, more connected global graph.\\n}\\n\\n\\ndef eliminate_batch_effect_fn(\\n    adata: ad.AnnData, config: dict[str, Any]\\n) -> ad.AnnData:\\n  # Create a copy to avoid modifying the original input adata in place if not desired,\\n  # though the problem setup implies modifications are fine.\\n  adata_integrated = adata.copy()\\n\\n  # Handle edge case: very small datasets where PCA or neighbor finding might fail.\\n  # If n_obs is 0 or 1, no meaningful graph can be built.\\n  if adata_integrated.n_obs <= 1:\\n      # For a single cell, there are no neighbors or meaningful embedding.\\n      # Return a placeholder to avoid errors in metrics.\\n      adata_integrated.obsm['X_emb'] = np.zeros((adata_integrated.n_obs, 1)) # Placeholder embedding\\n      adata_integrated.obsp['connectivities'] = csr_matrix((adata_integrated.n_obs, adata_integrated.n_obs))\\n      adata_integrated.obsp['distances'] = csr_matrix((adata_integrated.n_obs, adata_integrated.n_obs))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0, # No meaningful neighbors\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  # 1. Preprocessing: Normalize, log-transform, scale, and perform PCA\\n  # Using adata.X which contains raw counts.\\n  # Ensure X is dense for scanpy preprocessing if it's not already.\\n  if isinstance(adata_integrated.X, csr_matrix):\\n      adata_integrated.X = adata_integrated.X.toarray()\\n\\n  sc.pp.normalize_total(adata_integrated, target_sum=1e4)\\n  sc.pp.log1p(adata_integrated)\\n  sc.pp.scale(adata_integrated, max_value=10) # Clip values to avoid extreme outliers\\n\\n  # Perform PCA\\n  n_pca_components = config.get('n_pca_components', 50)\\n  # Handle cases where n_vars < n_pca_components or n_obs-1 < n_pca_components.\\n  # PCA components cannot exceed min(n_pca_components, n_vars, n_obs - 1).\\n  # min(n_obs - 1, n_vars) is the maximum possible number of components.\\n  actual_n_pca_components = min(n_pca_components, adata_integrated.n_vars, adata_integrated.n_obs - 1)\\n\\n\\n  if actual_n_pca_components <= 0:\\n      # If PCA is not possible (e.g., too few dimensions after HVG selection or too few cells),\\n      # return a trivial embedding and empty graph to avoid errors in metrics.\\n      adata_integrated.obsm['X_emb'] = np.zeros((adata_integrated.n_obs, 1)) # Placeholder embedding\\n      adata_integrated.obsp['connectivities'] = csr_matrix((adata_integrated.n_obs, adata_integrated.n_obs))\\n      adata_integrated.obsp['distances'] = csr_matrix((adata_integrated.n_obs, adata_integrated.n_obs))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0,\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  sc.tl.pca(adata_integrated, n_comps=actual_n_pca_components, svd_solver='arpack')\\n  X_pca_original = adata_integrated.obsm['X_pca'] # Store original PCA for correction\\n\\n  # Batch Correction on PCA embedding: Apply mean-centering batch correction\\n  X_pca_corrected = X_pca_original.copy()\\n  batches = adata_integrated.obs['batch'].values\\n  unique_batches = np.unique(batches)\\n\\n  # Calculate global mean of PCA components\\n  global_pca_mean = X_pca_original.mean(axis=0)\\n\\n  # Perform batch correction: subtract batch mean and add global mean\\n  for batch_id in unique_batches:\\n      batch_mask = (batches == batch_id)\\n      # Ensure there are cells in this batch to avoid issues with empty slices\\n      if np.sum(batch_mask) > 0:\\n          batch_pca_mean = X_pca_original[batch_mask, :].mean(axis=0)\\n          # Correct: X_corrected = X_original - X_batch_mean + X_global_mean\\n          X_pca_corrected[batch_mask, :] = X_pca_original[batch_mask, :] - batch_pca_mean + global_pca_mean\\n\\n  # Set X_emb to the batch-corrected PCA embedding.\\n  adata_integrated.obsm['X_emb'] = X_pca_corrected\\n  X_emb_data = adata_integrated.obsm['X_emb']\\n  n_cells = adata_integrated.n_obs\\n\\n\\n  # 2. Custom Batch-Aware Nearest Neighbors Graph Construction (using the batch-corrected X_emb)\\n  # This implementation follows the expert advice while significantly improving efficiency.\\n  k_batch_neighbors = config.get('n_neighbors_per_batch', 10)\\n  total_k_neighbors = config.get('total_k_neighbors', 50)\\n\\n  # Adjust total_k_neighbors if it's too large for the dataset size\\n  total_k_neighbors = min(total_k_neighbors, n_cells - 1)\\n  if total_k_neighbors <= 0: # If dataset has 1 cell, or total_k_neighbors somehow became 0\\n      adata_integrated.obsp['connectivities'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.obsp['distances'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0,\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  # Group cell indices by batch for efficient querying\\n  batch_to_indices = {b: np.where(batches == b)[0] for b in unique_batches}\\n\\n  # Fit NearestNeighbors model for each batch's data once (using corrected PCA)\\n  # Store the fitted models for efficient querying later.\\n  batch_nn_models = {}\\n  for b_id in unique_batches:\\n    batch_cell_indices = batch_to_indices[b_id]\\n    if len(batch_cell_indices) > 1: # Need at least 2 points to find neighbors\\n      # Ensure n_neighbors doesn't exceed the number of points in the batch minus 1.\\n      k_effective_for_batch = min(k_batch_neighbors, len(batch_cell_indices) - 1)\\n      if k_effective_for_batch > 0:\\n        nn_model = NearestNeighbors(n_neighbors=k_effective_for_batch, metric='euclidean', algorithm='auto')\\n        # Fit on the batch-corrected data (adata_integrated.obsm['X_emb']) for cells in this batch\\n        nn_model.fit(X_emb_data[batch_cell_indices])\\n        batch_nn_models[b_id] = nn_model\\n\\n  # Collect all candidate neighbors efficiently by making vectorized queries\\n  all_collected_candidate_tuples = []\\n\\n  for target_batch_id in unique_batches:\\n      if target_batch_id not in batch_nn_models:\\n          continue # Skip batches for which no NN model was fitted (e.g., single-cell batches)\\n\\n      nn_model = batch_nn_models[target_batch_id]\\n      target_global_indices = batch_to_indices[target_batch_id]\\n\\n      # Perform vectorized query: find neighbors for ALL cells against this TARGET batch\\n      distances_matrix, local_indices_matrix = nn_model.kneighbors(\\n          X_emb_data, n_neighbors=nn_model.n_neighbors, return_distance=True\\n      ) # distances_matrix: (n_cells, k_effective), local_indices_matrix: (n_cells, k_effective)\\n\\n      # Convert local indices to global indices\\n      global_indices_matrix = target_global_indices[local_indices_matrix] # (n_cells, k_effective)\\n\\n      # Prepare flattened arrays for collecting triplets\\n      query_indices_flat = np.repeat(np.arange(n_cells), nn_model.n_neighbors) # (n_cells * k_effective,)\\n      distances_flat = distances_matrix.ravel() # (n_cells * k_effective,)\\n      global_indices_flat = global_indices_matrix.ravel() # (n_cells * k_effective,)\\n\\n      # Filter out self-loops (a cell querying itself from its own batch). A cell should not be its own neighbor.\\n      mask = (query_indices_flat != global_indices_flat)\\n\\n      # Extend the list with (query_idx, neighbor_idx, distance) tuples for candidates\\n      all_collected_candidate_tuples.extend(\\n          zip(query_indices_flat[mask], global_indices_flat[mask], distances_flat[mask])\\n      )\\n\\n  # Process candidates to get unique (row, col) with minimum distance\\n  if not all_collected_candidate_tuples: # No neighbors found at all\\n      distances_matrix_final = csr_matrix((n_cells, n_cells), dtype=float)\\n  else:\\n      df_candidates = pd.DataFrame(all_collected_candidate_tuples, columns=['row', 'col', 'dist'])\\n      # Group by (row, col) and take the minimum distance to handle duplicates\\n      df_unique_min_dist = df_candidates.groupby(['row', 'col'])['dist'].min().reset_index()\\n\\n      # Create an initial CSR matrix from these unique, minimum-distance candidates\\n      initial_csr = csr_matrix(\\n          (df_unique_min_dist['dist'], (df_unique_min_dist['row'], df_unique_min_dist['col'])),\\n          shape=(n_cells, n_cells),\\n          dtype=float\\n      )\\n\\n      # Select total_k_neighbors for each row from initial_csr\\n      final_rows_selected = []\\n      final_cols_selected = []\\n      final_data_selected = []\\n\\n      for i in range(n_cells):\\n          # Get neighbors and distances for the current cell from the initial CSR matrix\\n          row_start = initial_csr.indptr[i]\\n          row_end = initial_csr.indptr[i+1]\\n          current_row_neighbors = initial_csr.indices[row_start:row_end]\\n          current_row_distances = initial_csr.data[row_start:row_end]\\n\\n          # Create a list of (distance, neighbor_idx) tuples for heapq\\n          candidate_neighbors_for_cell = list(zip(current_row_distances, current_row_neighbors))\\n\\n          # Select the top total_k_neighbors closest unique neighbors for the current cell\\n          selected_neighbors = heapq.nsmallest(total_k_neighbors, candidate_neighbors_for_cell, key=lambda x: x[0])\\n\\n          for dist, neighbor_idx in selected_neighbors:\\n              final_rows_selected.append(i)\\n              final_cols_selected.append(neighbor_idx)\\n              final_data_selected.append(dist)\\n      \\n      # Construct the final distances matrix from the selected neighbors\\n      if not final_rows_selected:\\n          distances_matrix_final = csr_matrix((n_cells, n_cells), dtype=float)\\n      else:\\n          distances_matrix_final = coo_matrix(\\n              (final_data_selected, (final_rows_selected, final_cols_selected)),\\n              shape=(n_cells, n_cells),\\n              dtype=float\\n          ).tocsr() # Convert to CSR for efficient symmetric operation\\n\\n  # Symmetrize the distance matrix by taking the maximum distance if an edge exists in either direction.\\n  # This ensures if i sees j, j also sees i, taking the max distance to be conservative.\\n  distances_matrix_final = distances_matrix_final.maximum(distances_matrix_final.T)\\n  distances_matrix_final.eliminate_zeros() # Remove any explicit zeros\\n\\n  # Create connectivities matrix (binary, 1 for connected, 0 otherwise)\\n  connectivities_matrix = distances_matrix_final.copy()\\n  connectivities_matrix.data[:] = 1.0  # Set all non-zero entries to 1.0\\n  connectivities_matrix.eliminate_zeros()\\n  connectivities_matrix = connectivities_matrix.astype(float) # Ensure float type for consistency\\n\\n  # Store the custom graph in adata.obsp. This is crucial for scib metrics.\\n  adata_integrated.obsp['connectivities'] = connectivities_matrix\\n  adata_integrated.obsp['distances'] = distances_matrix_final\\n\\n  # Store params in adata.uns['neighbors'] for completeness and scanpy compatibility\\n  adata_integrated.uns['neighbors'] = {\\n      'params': {\\n          'n_neighbors': total_k_neighbors,\\n          'method': 'custom_batch_aware_corrected_pca', # Update method name to reflect improvement\\n          'metric': 'euclidean',\\n          'n_pcs': actual_n_pca_components,\\n          'n_neighbors_per_batch': k_batch_neighbors,\\n          'pca_batch_correction': 'mean_sub_global_add', # Indicate that PCA was batch-corrected\\n      },\\n      'connectivities_key': 'connectivities', # Specify key in obsp\\n      'distances_key': 'distances', # Specify key in obsp\\n  }\\n\\n  # Return the AnnData object with the embedding and custom graph\\n  return adata_integrated",
  "new_index": 229,
  "new_code": "# [rewrite_cell]\\nfrom typing import Any\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom scipy.sparse import csr_matrix, coo_matrix\\nimport numpy as np\\nimport pandas as pd\\nimport scanpy as sc\\nimport anndata as ad\\nimport heapq\\n\\n# Define parameters for the config.\\n# Adjusted values based on common practices and competition objectives:\\n# - n_pca_components: Increased to capture more variance and provide a richer embedding.\\n# - n_neighbors_per_batch: Increased for more robust local neighborhoods within each batch.\\n# - total_k_neighbors: Increased to ensure a denser, more connected overall graph.\\nconfig = {\\n    'n_pca_components': 100,  # Recommended: 50-200. Increased for potentially richer embedding.\\n    'n_neighbors_per_batch': 10,  # Recommended: 5-15. Increased for more robust batch-local neighborhoods.\\n    'total_k_neighbors': 50,  # Recommended: 15-100. Increased for denser, more connected global graph.\\n}\\n\\n\\ndef eliminate_batch_effect_fn(\\n    adata: ad.AnnData, config: dict[str, Any]\\n) -> ad.AnnData:\\n  adata_integrated = adata.copy()\\n\\n  n_cells = adata_integrated.n_obs\\n  n_genes = adata_integrated.n_vars\\n\\n  # --- Robustness Checks for Small Datasets ---\\n  # If n_obs is 0 or 1, no meaningful graph can be built or PCA performed.\\n  # Return a placeholder to avoid errors in metrics.\\n  if n_cells <= 1:\\n      print(f\\"Warning: Dataset has {n_cells} cells. Returning empty embedding and graph.\\")\\n      adata_integrated.obsm['X_emb'] = np.zeros((n_cells, 1)) # Placeholder embedding\\n      adata_integrated.obsp['connectivities'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.obsp['distances'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0, # No meaningful neighbors\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  # --- 1. Preprocessing: Normalize, log-transform, scale, and perform PCA ---\\n  # Ensure X is dense for scanpy preprocessing if max_value is used in scale, as it might\\n  # convert internally or expect dense for certain operations.\\n  if isinstance(adata_integrated.X, csr_matrix):\\n      adata_integrated.X = adata_integrated.X.toarray()\\n\\n  sc.pp.normalize_total(adata_integrated, target_sum=1e4)\\n  sc.pp.log1p(adata_integrated)\\n  sc.pp.scale(adata_integrated, max_value=10) # Clip values to avoid extreme outliers\\n\\n  # Perform PCA\\n  n_pca_components = config.get('n_pca_components', 50)\\n  # PCA components cannot exceed min(n_pca_components, n_vars, n_obs - 1).\\n  # min(n_obs - 1, n_vars) is the maximum possible number of components.\\n  actual_n_pca_components = min(n_pca_components, n_genes, n_cells - 1)\\n\\n  if actual_n_pca_components <= 0:\\n      # If PCA is not possible (e.g., too few dimensions after HVG selection or too few cells),\\n      # return a trivial embedding and empty graph to avoid errors in metrics.\\n      print(f\\"Warning: PCA not possible with {actual_n_pca_components} components. Returning empty embedding and graph.\\")\\n      adata_integrated.obsm['X_emb'] = np.zeros((n_cells, 1)) # Placeholder embedding\\n      adata_integrated.obsp['connectivities'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.obsp['distances'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0,\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  sc.tl.pca(adata_integrated, n_comps=actual_n_pca_components, svd_solver='arpack')\\n  X_pca_original = adata_integrated.obsm['X_pca']\\n\\n  # --- Batch Correction on PCA embedding: Apply mean-centering batch correction ---\\n  X_pca_corrected = X_pca_original.copy()\\n  batches = adata_integrated.obs['batch'].values\\n  unique_batches = np.unique(batches)\\n\\n  # Calculate global mean of PCA components\\n  global_pca_mean = X_pca_original.mean(axis=0)\\n\\n  # Perform batch correction: subtract batch mean and add global mean\\n  for batch_id in unique_batches:\\n      batch_mask = (batches == batch_id)\\n      # Ensure there are cells in this batch to avoid issues with empty slices\\n      if np.sum(batch_mask) > 0:\\n          batch_pca_mean = X_pca_original[batch_mask, :].mean(axis=0)\\n          # Correct: X_corrected = X_original - X_batch_mean + X_global_mean\\n          X_pca_corrected[batch_mask, :] = X_pca_original[batch_mask, :] - batch_pca_mean + global_pca_mean\\n\\n  # Set X_emb to the batch-corrected PCA embedding.\\n  adata_integrated.obsm['X_emb'] = X_pca_corrected\\n  X_emb_data = adata_integrated.obsm['X_emb']\\n\\n\\n  # --- 2. Custom Batch-Aware Nearest Neighbors Graph Construction ---\\n  k_batch_neighbors = config.get('n_neighbors_per_batch', 10)\\n  total_k_neighbors = config.get('total_k_neighbors', 50)\\n\\n  # Adjust total_k_neighbors if it's too large for the dataset size or 0\\n  total_k_neighbors = min(total_k_neighbors, n_cells - 1)\\n  if total_k_neighbors <= 0: # If dataset has 1 cell, or total_k_neighbors somehow became 0\\n      print(f\\"Warning: Cannot find meaningful neighbors with total_k_neighbors={total_k_neighbors}. Returning empty graph.\\")\\n      adata_integrated.obsp['connectivities'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.obsp['distances'] = csr_matrix((n_cells, n_cells))\\n      adata_integrated.uns['neighbors'] = {\\n          'params': {\\n              'n_neighbors': 0,\\n              'method': 'degenerate',\\n          },\\n          'connectivities_key': 'connectivities',\\n          'distances_key': 'distances',\\n      }\\n      return adata_integrated\\n\\n  # Group cell indices by batch for efficient querying\\n  batch_to_indices = {b: np.where(batches == b)[0] for b in unique_batches}\\n\\n  # Fit NearestNeighbors model for each batch's data once (using corrected PCA)\\n  batch_nn_models = {}\\n  for b_id in unique_batches:\\n    batch_cell_indices = batch_to_indices[b_id]\\n    # Need at least k_batch_neighbors + 1 points to find k_batch_neighbors.\\n    # NearestNeighbors itself requires at least 2 points to fit a tree for non-trivial n_neighbors.\\n    if len(batch_cell_indices) > 1:\\n      # Ensure n_neighbors doesn't exceed the number of points in the batch minus 1.\\n      k_effective_for_batch = min(k_batch_neighbors, len(batch_cell_indices) - 1)\\n      if k_effective_for_batch > 0: # Only fit if meaningful neighbors can be found\\n        nn_model = NearestNeighbors(n_neighbors=k_effective_for_batch, metric='euclidean', algorithm='auto')\\n        nn_model.fit(X_emb_data[batch_cell_indices])\\n        batch_nn_models[b_id] = nn_model\\n\\n  # Collect all candidate neighbors efficiently by making vectorized queries\\n  all_collected_candidate_tuples = []\\n\\n  # Iterate through fitted NN models for each batch\\n  for target_batch_id, nn_model in batch_nn_models.items():\\n      target_global_indices = batch_to_indices[target_batch_id]\\n\\n      # Perform vectorized query: find neighbors for ALL cells against this TARGET batch\\n      # distances_matrix: (n_cells, k_effective), local_indices_matrix: (n_cells, k_effective)\\n      distances_matrix, local_indices_matrix = nn_model.kneighbors(\\n          X_emb_data, n_neighbors=nn_model.n_neighbors, return_distance=True\\n      )\\n\\n      # Convert local indices within the target batch to global indices\\n      global_indices_matrix = target_global_indices[local_indices_matrix]\\n\\n      # Prepare flattened arrays for collecting triplets (query_idx, neighbor_idx, distance)\\n      query_indices_flat = np.repeat(np.arange(n_cells), nn_model.n_neighbors)\\n      distances_flat = distances_matrix.ravel()\\n      global_indices_flat = global_indices_matrix.ravel()\\n\\n      # Filter out self-loops (a cell querying itself from any batch).\\n      # This is crucial for graph integrity and to avoid a cell being its own neighbor.\\n      mask = (query_indices_flat != global_indices_flat)\\n\\n      # Extend the list with (query_idx, neighbor_idx, distance) tuples for valid candidates\\n      all_collected_candidate_tuples.extend(\\n          zip(query_indices_flat[mask], global_indices_flat[mask], distances_flat[mask])\\n      )\\n\\n  # Process candidates to get unique (row, col) with minimum distance\\n  if not all_collected_candidate_tuples: # No neighbors found at all\\n      print(\\"Warning: No candidate neighbors found. Returning empty graph.\\")\\n      distances_matrix_final = csr_matrix((n_cells, n_cells), dtype=float)\\n  else:\\n      # Use pandas DataFrame for efficient de-duplication and minimum distance selection\\n      df_candidates = pd.DataFrame(all_collected_candidate_tuples, columns=['row', 'col', 'dist'])\\n      df_unique_min_dist = df_candidates.groupby(['row', 'col'])['dist'].min().reset_index()\\n\\n      # Create an initial CSR matrix from these unique, minimum-distance candidates\\n      initial_csr = coo_matrix(\\n          (df_unique_min_dist['dist'], (df_unique_min_dist['row'], df_unique_min_dist['col'])),\\n          shape=(n_cells, n_cells),\\n          dtype=float\\n      ).tocsr() # Convert to CSR for efficient row-wise access later\\n\\n      # Select total_k_neighbors for each row from initial_csr\\n      final_rows_selected = []\\n      final_cols_selected = []\\n      final_data_selected = []\\n\\n      for i in range(n_cells):\\n          # Get neighbors and distances for the current cell from the initial CSR matrix\\n          row_start = initial_csr.indptr[i]\\n          row_end = initial_csr.indptr[i+1]\\n          current_row_neighbors_indices = initial_csr.indices[row_start:row_end]\\n          current_row_neighbors_distances = initial_csr.data[row_start:row_end]\\n\\n          # Create a list of (distance, neighbor_idx) tuples for heapq\\n          candidate_neighbors_for_cell = list(zip(current_row_neighbors_distances, current_row_neighbors_indices))\\n\\n          # Select the top total_k_neighbors closest unique neighbors for the current cell\\n          # heapq.nsmallest is efficient for this purpose.\\n          selected_neighbors = heapq.nsmallest(total_k_neighbors, candidate_neighbors_for_cell, key=lambda x: x[0])\\n\\n          for dist, neighbor_idx in selected_neighbors:\\n              final_rows_selected.append(i)\\n              final_cols_selected.append(neighbor_idx)\\n              final_data_selected.append(dist)\\n\\n      # Construct the final distances matrix from the selected neighbors\\n      if not final_rows_selected:\\n          print(\\"Warning: No final neighbors selected after total_k_neighbors filtering. Returning empty graph.\\")\\n          distances_matrix_final = csr_matrix((n_cells, n_cells), dtype=float)\\n      else:\\n          distances_matrix_final = coo_matrix(\\n              (final_data_selected, (final_rows_selected, final_cols_selected)),\\n              shape=(n_cells, n_cells),\\n              dtype=float\\n          ).tocsr()\\n\\n  # Symmetrize the distance matrix by taking the maximum distance if an edge exists in either direction.\\n  # This ensures if i sees j, j also sees i, taking the max distance to be conservative.\\n  # This avoids issues where graph metrics might fail if the graph is not symmetric.\\n  distances_matrix_final = distances_matrix_final.maximum(distances_matrix_final.T)\\n  distances_matrix_final.eliminate_zeros() # Remove any explicit zeros\\n\\n  # Create connectivities matrix (binary, 1 for connected, 0 otherwise)\\n  connectivities_matrix = distances_matrix_final.copy()\\n  connectivities_matrix.data[:] = 1.0  # Set all non-zero entries to 1.0\\n  connectivities_matrix.eliminate_zeros()\\n  connectivities_matrix = connectivities_matrix.astype(float) # Ensure float type for consistency\\n\\n  # Store the custom graph in adata.obsp. This is crucial for scib metrics.\\n  adata_integrated.obsp['connectivities'] = connectivities_matrix\\n  adata_integrated.obsp['distances'] = distances_matrix_final\\n\\n  # Store params in adata.uns['neighbors'] for completeness and scanpy compatibility\\n  adata_integrated.uns['neighbors'] = {\\n      'params': {\\n          'n_neighbors': total_k_neighbors,\\n          'method': 'custom_batch_aware_corrected_pca', # Update method name to reflect improvement\\n          'metric': 'euclidean',\\n          'n_pcs': actual_n_pca_components,\\n          'n_neighbors_per_batch': k_batch_neighbors,\\n          'pca_batch_correction': 'mean_sub_global_add', # Indicate that PCA was batch-corrected\\n      },\\n      'connectivities_key': 'connectivities', # Specify key in obsp\\n      'distances_key': 'distances', # Specify key in obsp\\n  }\\n\\n  # Return the AnnData object with the embedding and custom graph\\n  return adata_integrated"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "421",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any, Dict, List, Union\\n\\n# Define quantiles globally as they are constant for the competition\\nQUANTILES = [\\n    0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n]\\nTARGET_COL = 'Total COVID-19 Admissions'\\nDATE_COL = 'target_end_date'\\nLOCATION_COL = 'location'\\nPOPULATION_COL = 'population'\\nHORIZON_COL = 'horizon'\\n# New column to store admissions per million before any complex transformation\\nRAW_TARGET_PER_MILLION_COL = 'admissions_per_million'\\nEPSILON = 1e-6 # A small epsilon for numerical stability to prevent issues like log(0) or sqrt(negative)\\n\\n# --- Helper functions for target transformations ---\\ndef transform_target(series: pd.Series, transform_type: str) -> pd.Series:\\n    \\"\\"\\"Applies a specified transformation to the series.\\"\\"\\"\\n    if transform_type == 'log1p':\\n        return np.log1p(series)\\n    elif transform_type == 'sqrt':\\n        return np.sqrt(series + EPSILON)\\n    elif transform_type == 'fourth_root':\\n        return np.power(series + EPSILON, 0.25)\\n    else: # 'raw' or unknown transform type\\n        return series\\n\\ndef inverse_transform_target(series: np.ndarray, transform_type: str) -> np.ndarray:\\n    \\"\\"\\"Applies the inverse of a specified transformation to the numpy array.\\"\\"\\"\\n    if transform_type == 'log1p':\\n        return np.expm1(series)\\n    elif transform_type == 'sqrt':\\n        # Ensure values are non-negative before squaring, as model noise might make them slightly negative\\n        return np.power(np.maximum(0, series), 2) - EPSILON\\n    elif transform_type == 'fourth_root':\\n        # Ensure values are non-negative before powering\\n        return np.power(np.maximum(0, series), 4) - EPSILON\\n    else: # 'raw' or unknown transform type\\n        return series\\n\\ndef add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n    \\"\\"\\"Adds common date-based and temporal trend features.\\"\\"\\"\\n    df = df_input.copy()\\n    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n    \\n    df['year'] = df[DATE_COL].dt.year\\n    df['month'] = df[DATE_COL].dt.month\\n    # Using ISO calendar week for consistency across years\\n    df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n    \\n    # Cyclical features for week of year to capture seasonality\\n    df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n    df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n    # Weeks since the start of the entire dataset to capture overall trend\\n    df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n    \\n    return df\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Make probabilistic predictions for test_x by modeling train_x to train_y,\\n    using an ensemble of LightGBM models. Each ensemble member can have different\\n    target transformations and feature sets. The history for iterative prediction\\n    is updated using the ensemble's median forecast.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n                                  Can contain 'ensemble_members' (list of sub-configs)\\n                                  and 'default_lgbm_params'.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n\\n    # --- 1. Initial Data Preparation (Common for all ensemble members) ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate raw admissions per million, which serves as the base for all transformations.\\n    # Handle division by zero for population and ensure non-negativity.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    df_train_full[RAW_TARGET_PER_MILLION_COL] = pd.Series(admissions_per_million, index=df_train_full.index)\\n    df_train_full[RAW_TARGET_PER_MILLION_COL] = df_train_full[RAW_TARGET_PER_MILLION_COL].apply(lambda x: max(0.0, x))\\n\\n    # Determine global min date for 'weeks_since_start' feature consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    # Add base features to both training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n    \\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n    \\n    # Ensure 'horizon' column exists in training data (historical data has horizon 0)\\n    if HORIZON_COL not in df_train_full.columns:\\n        df_train_full[HORIZON_COL] = 0 \\n    \\n    # Collect all unique categories for consistent categorical encoding across train and test sets\\n    all_location_categories = pd.unique(pd.concat([df_train_full[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    train_horizon_categories = df_train_full[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    \\n    # --- 2. Ensemble Member Training Setup ---\\n    # Retrieve ensemble configurations; if 'ensemble_members' is not specified, run as a single model.\\n    ensemble_members_configs = config.get('ensemble_members', [config])\\n    trained_models_per_member = [] # Stores trained models and their configurations\\n\\n    # Loop through each ensemble member's configuration\\n    for member_config_idx, member_config in enumerate(ensemble_members_configs):\\n        # Extract member-specific parameters, merging with default LGBM params if provided\\n        default_lgbm_params = config.get('default_lgbm_params', {\\n            'objective': 'quantile', 'metric': 'quantile', 'n_estimators': 200, 'learning_rate': 0.03,\\n            'num_leaves': 25, 'max_depth': 5, 'min_child_samples': 20, 'random_state': 42,\\n            'n_jobs': -1, 'verbose': -1, 'colsample_bytree': 0.8, 'subsample': 0.8,\\n            'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        })\\n        lgbm_params = {**default_lgbm_params, **member_config.get('lgbm_params', {})}\\n        target_transform_type = member_config.get('target_transform', 'log1p')\\n        lag_weeks = member_config.get('lag_weeks', [1, 2, 4, 8, 12, 26, 52])\\n        rolling_windows = member_config.get('rolling_windows', [2, 4, 8, 16])\\n\\n        # --- Member-specific Feature Engineering (Training Data) ---\\n        member_train_df = df_train_full.copy()\\n        # Apply this member's specific target transformation to the raw admissions per million\\n        member_train_df[f'transformed_admissions_for_model_{member_config_idx}'] = \\\\\\n            transform_target(member_train_df[RAW_TARGET_PER_MILLION_COL], target_transform_type)\\n\\n        # Generate lagged features based on the member's transformed target\\n        for lag in lag_weeks:\\n            member_train_df[f'lag_{lag}_wk_m{member_config_idx}'] = \\\\\\n                member_train_df.groupby(LOCATION_COL)[f'transformed_admissions_for_model_{member_config_idx}'].shift(lag)\\n\\n        # Generate rolling mean features, shifted to avoid data leakage\\n        for window in rolling_windows:\\n            member_train_df[f'rolling_mean_{window}_wk_m{member_config_idx}'] = \\\\\\n                member_train_df.groupby(LOCATION_COL)[f'transformed_admissions_for_model_{member_config_idx}'].transform(\\n                    lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n                )\\n        \\n        # Compile all features used by this member's model\\n        member_specific_features = [f'lag_{lag}_wk_m{member_config_idx}' for lag in lag_weeks] + \\\\\\n                                   [f'rolling_mean_{window}_wk_m{member_config_idx}' for window in rolling_windows]\\n        \\n        X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL] + member_specific_features\\n        \\n        X_train_model_member = member_train_df[X_train_model_cols].copy()\\n        y_train_model_member = member_train_df[f'transformed_admissions_for_model_{member_config_idx}']\\n\\n        # Fill NaNs created by lagging/rolling at the start of series (ffill then 0.0)\\n        for col in member_specific_features:\\n            if X_train_model_member[col].isnull().any():\\n                X_train_model_member[col] = X_train_model_member.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n                X_train_model_member[col] = X_train_model_member[col].fillna(0.0) \\n\\n        # Drop rows where target or selected features are NaN (e.g., very early in time series)\\n        train_combined_member = X_train_model_member.copy()\\n        train_combined_member[f'transformed_admissions_for_model_{member_config_idx}'] = y_train_model_member\\n        train_combined_member.dropna(subset=X_train_model_cols + [f'transformed_admissions_for_model_{member_config_idx}'], inplace=True)\\n        \\n        X_train_model_member = train_combined_member.drop(columns=[f'transformed_admissions_for_model_{member_config_idx}'])\\n        y_train_model_member = train_combined_member[f'transformed_admissions_for_model_{member_config_idx}']\\n\\n        # Set categorical types for LightGBM\\n        X_train_model_member[LOCATION_COL] = X_train_model_member[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_train_model_member[HORIZON_COL] = X_train_model_member[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # --- Train Models for this Ensemble Member ---\\n        member_models = {}\\n        categorical_features_lgbm = [LOCATION_COL, HORIZON_COL]\\n        for q in QUANTILES:\\n            model_params = lgbm_params.copy()\\n            model_params['alpha'] = q # Set quantile for LGBMRegressor\\n            model = LGBMRegressor(**model_params)\\n            model.fit(X_train_model_member, y_train_model_member,\\n                      categorical_feature=categorical_features_lgbm)\\n            member_models[q] = model\\n        \\n        # Store trained models and their associated config for prediction phase\\n        trained_models_per_member.append({\\n            'models': member_models,\\n            'transform_type': target_transform_type,\\n            'lag_weeks': lag_weeks,\\n            'rolling_windows': rolling_windows,\\n            'member_idx': member_config_idx # Keep index for unique feature names\\n        })\\n\\n    # --- 3. Iterative Prediction for Test Data (Ensemble Averaging) ---\\n    # Store original test_x index to map predictions back correctly\\n    original_test_x_index = test_x.index\\n    test_x_processed['original_index'] = test_x_processed.index\\n    # Sort test data by location and date to enable proper iterative feature generation\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize DataFrame to store final ensemble predictions\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES], dtype=float)\\n\\n    # Initialize history for each location using full training data's RAW_TARGET_PER_MILLION_COL values.\\n    # This history will be updated iteratively with ensemble's median predictions.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[RAW_TARGET_PER_MILLION_COL].apply(list).to_dict()\\n\\n    # Iterate through each row of the test set to predict sequentially\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n        population_val = row[POPULATION_COL]\\n\\n        # List to hold raw (Total COVID-19 Admissions) predictions from each ensemble member for the current row\\n        all_member_raw_preds_for_this_row_by_quantile = {q: [] for q in QUANTILES}\\n        \\n        # Get the current raw history for this location (updated from previous time steps/training data)\\n        current_loc_raw_hist = location_history_data.get(current_loc, [])\\n\\n        # For each ensemble member, generate features and predict\\n        for member_info in trained_models_per_member:\\n            member_models = member_info['models']\\n            member_transform_type = member_info['transform_type']\\n            member_lag_weeks = member_info['lag_weeks']\\n            member_rolling_windows = member_info['rolling_windows']\\n            member_config_idx = member_info['member_idx'] # Use index to form unique feature names\\n\\n            # Transform the raw history according to this specific member's transformation\\n            # This ensures lags/rolling means are in the correct scale for the member's model\\n            transformed_hist_for_member = transform_target(\\n                pd.Series(current_loc_raw_hist), member_transform_type\\n            ).values\\n\\n            # Build feature dictionary for the current test row\\n            current_features_dict = {col: row[col] for col in BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in row}\\n\\n            # Generate dynamic lag and rolling features using the member-specific transformed history\\n            for lag in member_lag_weeks:\\n                lag_col_name = f'lag_{lag}_wk_m{member_config_idx}'\\n                if len(transformed_hist_for_member) >= lag:\\n                    lag_value = transformed_hist_for_member[-lag]\\n                elif len(transformed_hist_for_member) > 0: # Use most recent if not enough data for full lag\\n                    lag_value = transformed_hist_for_member[-1]\\n                else:\\n                    lag_value = 0.0 # No history at all for this location\\n                current_features_dict[lag_col_name] = lag_value\\n            \\n            for window in member_rolling_windows:\\n                rolling_col_name = f'rolling_mean_{window}_wk_m{member_config_idx}'\\n                if len(transformed_hist_for_member) >= window:\\n                    rolling_mean_val = np.mean(transformed_hist_for_member[-window:])\\n                elif len(transformed_hist_for_member) > 0: # Use what's available\\n                    rolling_mean_val = np.mean(transformed_hist_for_member)\\n                else:\\n                    rolling_mean_val = 0.0\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            # Prepare X_test_row for prediction: ensure same columns and order as training data\\n            X_test_row_cols_for_member = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL] + \\\\\\n                                         [f'lag_{lag}_wk_m{member_config_idx}' for lag in member_lag_weeks] + \\\\\\n                                         [f'rolling_mean_{window}_wk_m{member_config_idx}' for window in member_rolling_windows]\\n            \\n            X_test_row = pd.DataFrame([current_features_dict])\\n            X_test_row = X_test_row.reindex(columns=X_test_row_cols_for_member, fill_value=0.0)\\n            \\n            # Re-cast categorical features with proper types\\n            X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n            X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n            \\n            # Make predictions for this member across all quantiles\\n            member_predictions_transformed = np.array([model.predict(X_test_row)[0] for q, model in member_models.items()])\\n\\n            # Inverse transform predictions from this member back to admissions per million\\n            member_inv_preds_admissions_per_million = inverse_transform_target(member_predictions_transformed, member_transform_type)\\n            \\n            # Convert from admissions per million back to total admissions (counts)\\n            if population_val == 0:\\n                member_final_preds_total_admissions = np.zeros_like(member_inv_preds_admissions_per_million)\\n            else:\\n                member_final_preds_total_admissions = member_inv_preds_admissions_per_million * population_val / 1_000_000\\n            \\n            # Ensure predictions are non-negative at this stage (before averaging)\\n            member_final_preds_total_admissions[member_final_preds_total_admissions < 0] = 0\\n            \\n            # Store predictions from this member, by quantile, to be averaged later\\n            for i, q in enumerate(QUANTILES):\\n                all_member_raw_preds_for_this_row_by_quantile[q].append(member_final_preds_total_admissions[i])\\n        \\n        # --- Ensemble Averaging for the current row ---\\n        ensemble_preds_total_admissions = []\\n        for q in QUANTILES:\\n            # Average the predictions for each quantile across all ensemble members\\n            avg_pred = np.mean(all_member_raw_preds_for_this_row_by_quantile[q])\\n            ensemble_preds_total_admissions.append(avg_pred)\\n        \\n        ensemble_preds_total_admissions = np.array(ensemble_preds_total_admissions)\\n        \\n        # Store ensemble predictions in the final DataFrame for this row\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = ensemble_preds_total_admissions[i]\\n\\n        # --- Update the global history with the ensemble's median prediction ---\\n        # This ensemble median prediction (in Total Admissions scale) will be used for future lags\\n        ensemble_median_pred_total_admissions = predictions_df.loc[original_idx, f'quantile_{0.5}']\\n\\n        # Convert back to raw admissions per million for consistent history storage\\n        if population_val == 0:\\n            value_to_add_to_history = 0.0\\n        else:\\n            value_to_add_to_history = ensemble_median_pred_total_admissions / population_val * 1_000_000\\n        \\n        # Append the ensemble's median prediction to the history for the current location\\n        # This updates the \\"known past\\" for subsequent iterations of forecasting for this location.\\n        location_history_data.setdefault(current_loc, []).append(max(0.0, value_to_add_to_history))\\n\\n    # --- Final Post-processing: Rounding to Integer and Ensuring Monotonicity ---\\n    # Convert predictions to integers (counts) and ensure non-negativity\\n    predictions_df = predictions_df.apply(lambda x: np.round(np.maximum(0, x)).astype(int))\\n\\n    # Ensure monotonicity of quantiles across each row (q0.01 <= q0.025 <= ... <= q0.99)\\n    # This is a critical step for valid probabilistic forecasts\\n    predictions_array = predictions_df.values.astype(float) # Convert to float for robust sorting\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    {\\n        # Ensemble 1: Combines the two best performing target transformations from prior trial\\n        # using a common set of default LightGBM hyperparameters.\\n        'default_lgbm_params': { \\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'ensemble_members': [\\n            { # Member 1: fourth_root transformation for target, with lags and rolling windows\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 4, 8, 16, 26, 52], \\n                'rolling_windows': [8, 16, 26]      \\n            },\\n            { # Member 2: log1p transformation for target, with slightly different lags and rolling windows\\n                'target_transform': 'log1p',\\n                'lag_weeks': [1, 2, 4, 8, 12, 26, 52], \\n                'rolling_windows': [2, 4, 8, 16]         \\n            }\\n        ]\\n    },\\n    {\\n        # Ensemble 2: A slightly more diverse ensemble, including a 'raw' (no transformation) member\\n        # and slightly adjusted overall LightGBM parameters.\\n        'default_lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 180,      # Slightly fewer estimators\\n            'learning_rate': 0.035,   # Slightly higher learning rate\\n            'num_leaves': 24,         \\n            'max_depth': 4,           # Reduced depth for slightly simpler trees\\n            'min_child_samples': 22,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.75, # Slightly more feature subsampling\\n            'subsample': 0.75,        # Slightly more data subsampling\\n            'reg_alpha': 0.15,        # Increased L1 regularization\\n            'reg_lambda': 0.15        # Increased L2 regularization\\n        },\\n        'ensemble_members': [\\n            { # Member 1: fourth_root transform\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 4, 8, 16, 26, 52], \\n                'rolling_windows': [8, 16, 26]      \\n            },\\n            { # Member 2: log1p transform\\n                'target_transform': 'log1p',\\n                'lag_weeks': [1, 2, 4, 8, 12, 26, 52], \\n                'rolling_windows': [2, 4, 8, 16]         \\n            },\\n            { # Member 3: Raw target (admissions per million) without further transformation\\n                'target_transform': 'raw', \\n                'lag_weeks': [1, 4, 8, 16, 26], # Fewer lags for a simpler model\\n                'rolling_windows': [4, 8, 16]   # Fewer rolling windows\\n            }\\n        ]\\n    },\\n    {\\n        # Single Model Baseline: This config is identical to the best performing one\\n        # from the previous trial's output. Used for direct comparison against ensembles.\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 220,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], \\n        'rolling_windows': [8, 16, 26]      \\n    }\\n]",
  "new_index": "451",
  "new_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\n# Define quantiles globally as they are constant for the competition\\nQUANTILES = [\\n    0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n]\\nTARGET_COL = 'Total COVID-19 Admissions'\\nDATE_COL = 'target_end_date'\\nLOCATION_COL = 'location'\\nPOPULATION_COL = 'population'\\n# New column to store admissions per million before any complex transformation\\nRAW_TARGET_PER_MILLION_COL = 'admissions_per_million'\\nEPSILON = 1e-6 # A small epsilon for numerical stability to prevent issues like log(0) or sqrt(negative)\\n\\n# --- Helper functions for target transformations ---\\ndef transform_target(series: pd.Series, transform_type: str) -> pd.Series:\\n    \\"\\"\\"Applies a specified transformation to the series.\\"\\"\\"\\n    if transform_type == 'log1p':\\n        return np.log1p(series)\\n    elif transform_type == 'sqrt':\\n        return np.sqrt(series + EPSILON)\\n    elif transform_type == 'fourth_root':\\n        return np.power(series + EPSILON, 0.25)\\n    else: # 'raw' or unknown transform type\\n        return series\\n\\ndef inverse_transform_target(series: np.ndarray, transform_type: str) -> np.ndarray:\\n    \\"\\"\\"Applies the inverse of a specified transformation to the numpy array.\\"\\"\\"\\n    if transform_type == 'log1p':\\n        return np.expm1(series)\\n    elif transform_type == 'sqrt':\\n        # Ensure values are non-negative before squaring, as model noise might make them slightly negative\\n        return np.power(np.maximum(0, series), 2) - EPSILON\\n    elif transform_type == 'fourth_root':\\n        # Ensure values are non-negative before powering\\n        return np.power(np.maximum(0, series), 4) - EPSILON\\n    else: # 'raw' or unknown transform type\\n        return series\\n\\ndef add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n    \\"\\"\\"Adds common date-based and temporal trend features.\\"\\"\\"\\n    df = df_input.copy()\\n    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n    \\n    df['year'] = df[DATE_COL].dt.year\\n    df['month'] = df[DATE_COL].dt.month\\n    # Using ISO calendar week for consistency across years\\n    df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n    \\n    # Cyclical features for week of year to capture seasonality\\n    df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n    df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n    # Weeks since the start of the entire dataset to capture overall trend\\n    df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n    \\n    return df\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Make probabilistic predictions for test_x by modeling train_x to train_y,\\n    using a single LightGBM model configuration. Iterative prediction is used\\n    where the history is updated using the model's median forecast.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n                                  Contains 'lgbm_params', 'target_transform',\\n                                  'lag_weeks', and 'rolling_windows'.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n\\n    # --- 1. Initial Data Preparation ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate raw admissions per million, which serves as the base for all transformations.\\n    # Handle division by zero for population and ensure non-negativity.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    df_train_full[RAW_TARGET_PER_MILLION_COL] = pd.Series(admissions_per_million, index=df_train_full.index)\\n    df_train_full[RAW_TARGET_PER_MILLION_COL] = df_train_full[RAW_TARGET_PER_MILLION_COL].apply(lambda x: max(0.0, x))\\n\\n    # Determine global min date for 'weeks_since_start' feature consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Add base features to both training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n    \\n    # Define base features and categorical features. Removed 'horizon' for simplicity\\n    # and to avoid issues with unseen categories during prediction.\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # Collect all unique categories for consistent categorical encoding across train and test sets\\n    all_location_categories = pd.unique(pd.concat([df_train_full[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    \\n    # --- 2. Model Training Setup ---\\n    # Extract model-specific parameters from config\\n    lgbm_params = config.get('lgbm_params', {\\n        'objective': 'quantile', 'metric': 'quantile', 'n_estimators': 150, 'learning_rate': 0.05,\\n        'num_leaves': 20, 'max_depth': 4, 'min_child_samples': 25, 'random_state': 42,\\n        'n_jobs': -1, 'verbose': -1, 'colsample_bytree': 0.7, 'subsample': 0.7,\\n        'reg_alpha': 0.2, 'reg_lambda': 0.2 \\n    })\\n    target_transform_type = config.get('target_transform', 'fourth_root') \\n    lag_weeks = config.get('lag_weeks', [1, 4, 8, 16]) \\n    rolling_windows = config.get('rolling_windows', [4, 8]) \\n\\n    # --- Feature Engineering (Training Data) ---\\n    df_train_processed = df_train_full.copy()\\n    # Apply the target transformation to the raw admissions per million\\n    df_train_processed['transformed_admissions'] = \\\\\\n        transform_target(df_train_processed[RAW_TARGET_PER_MILLION_COL], target_transform_type)\\n\\n    # Generate lagged features\\n    for lag in lag_weeks:\\n        df_train_processed[f'lag_{lag}_wk'] = \\\\\\n            df_train_processed.groupby(LOCATION_COL)['transformed_admissions'].shift(lag)\\n\\n    # Generate rolling mean features, shifted to avoid data leakage\\n    for window in rolling_windows:\\n        df_train_processed[f'rolling_mean_{window}_wk'] = \\\\\\n            df_train_processed.groupby(LOCATION_COL)['transformed_admissions'].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n    \\n    # Compile all features used by the model\\n    model_specific_features = [f'lag_{lag}_wk' for lag in lag_weeks] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in rolling_windows]\\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + model_specific_features\\n    \\n    X_train_model = df_train_processed[X_train_model_cols].copy()\\n    y_train_model = df_train_processed['transformed_admissions']\\n\\n    # Fill NaNs created by lagging/rolling at the start of series (ffill then 0.0)\\n    for col in model_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    # Drop rows where target or selected features are NaN (e.g., very early in time series)\\n    train_combined = X_train_model.copy()\\n    train_combined['transformed_admissions'] = y_train_model\\n    train_combined.dropna(subset=X_train_model_cols + ['transformed_admissions'], inplace=True)\\n    \\n    X_train_model = train_combined.drop(columns=['transformed_admissions'])\\n    y_train_model = train_combined['transformed_admissions']\\n\\n    # Set categorical types for LightGBM\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # --- Train Models for each Quantile ---\\n    trained_models = {}\\n    categorical_features_lgbm = [LOCATION_COL] \\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set quantile for LGBMRegressor\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        trained_models[q] = model\\n    \\n    # --- 3. Iterative Prediction for Test Data ---\\n    # Store original test_x index to map predictions back correctly\\n    original_test_x_index = test_x.index\\n    test_x_processed['original_index'] = test_x_processed.index\\n    # Sort test data by location and date to enable proper iterative feature generation\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize DataFrame to store final predictions\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES], dtype=float)\\n\\n    # Initialize history for each location using full training data's RAW_TARGET_PER_MILLION_COL values.\\n    # This history will be updated iteratively with model's median forecasts.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[RAW_TARGET_PER_MILLION_COL].apply(list).to_dict()\\n\\n    # Iterate through each row of the test set to predict sequentially\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n        population_val = row[POPULATION_COL]\\n\\n        # Get the current raw history for this location (updated from previous time steps/training data)\\n        current_loc_raw_hist = location_history_data.get(current_loc, [])\\n\\n        # Transform the raw history according to the model's transformation\\n        transformed_hist_for_model = transform_target(\\n            pd.Series(current_loc_raw_hist), target_transform_type\\n        ).values\\n\\n        # Build feature dictionary for the current test row\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES if col in row}\\n\\n        # Generate dynamic lag and rolling features using the transformed history\\n        for lag in lag_weeks:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(transformed_hist_for_model) >= lag:\\n                lag_value = transformed_hist_for_model[-lag]\\n            elif len(transformed_hist_for_model) > 0: # Use most recent if not enough data for full lag\\n                lag_value = transformed_hist_for_model[-1]\\n            else:\\n                lag_value = 0.0 # No history at all for this location\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for window in rolling_windows:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(transformed_hist_for_model) >= window:\\n                rolling_mean_val = np.mean(transformed_hist_for_model[-window:])\\n            elif len(transformed_hist_for_model) > 0: # Use what's available\\n                rolling_mean_val = np.mean(transformed_hist_for_model)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Prepare X_test_row for prediction: ensure same columns and order as training data\\n        X_test_row_cols_for_model = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + model_specific_features\\n        \\n        X_test_row = pd.DataFrame([current_features_dict])\\n        X_test_row = X_test_row.reindex(columns=X_test_row_cols_for_model, fill_value=0.0)\\n        \\n        # Re-cast categorical features with proper types\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        \\n        # Make predictions across all quantiles\\n        model_predictions_transformed = np.array([model.predict(X_test_row)[0] for q, model in trained_models.items()])\\n\\n        # Inverse transform predictions back to admissions per million\\n        inv_preds_admissions_per_million = inverse_transform_target(model_predictions_transformed, target_transform_type)\\n        \\n        # Convert from admissions per million back to total admissions (counts)\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n        \\n        # Ensure predictions are non-negative\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        \\n        # Store predictions in the final DataFrame for this row\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # --- Update the global history with the model's median prediction ---\\n        # This median prediction (in Total Admissions scale) will be used for future lags\\n        median_pred_total_admissions = predictions_df.loc[original_idx, f'quantile_{0.5}']\\n\\n        # Convert back to raw admissions per million for consistent history storage\\n        if population_val == 0:\\n            value_to_add_to_history = 0.0\\n        else:\\n            value_to_add_to_history = median_pred_total_admissions / population_val * 1_000_000\\n        \\n        # Append the model's median prediction to the history for the current location\\n        location_history_data.setdefault(current_loc, []).append(max(0.0, value_to_add_to_history))\\n\\n    # --- Final Post-processing: Rounding to Integer and Ensuring Monotonicity ---\\n    # Convert predictions to integers (counts) and ensure non-negativity\\n    predictions_df = predictions_df.apply(lambda x: np.round(np.maximum(0, x)).astype(int))\\n\\n    # Ensure monotonicity of quantiles across each row (q0.01 <= q0.025 <= ... <= q0.99)\\n    predictions_array = predictions_df.values.astype(float) \\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    {\\n        # Simpler model 1: fourth_root transformation with reduced feature set and simpler LGBM params\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 150,      # Reduced from 220\\n            'learning_rate': 0.05,    # Slightly higher LR for fewer trees\\n            'num_leaves': 20,         # Reduced from 26\\n            'max_depth': 4,           # Reduced from 5\\n            'min_child_samples': 25,  # Increased from 20 for more robustness\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,  # Reduced subsampling\\n            'subsample': 0.7,         # Reduced subsampling\\n            'reg_alpha': 0.2,         # Increased L1 regularization\\n            'reg_lambda': 0.2         # Increased L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16],   # Fewer, simpler lags\\n        'rolling_windows': [4, 8]     # Fewer, simpler rolling windows\\n    },\\n    {\\n        # Simpler model 2: fourth_root, slightly more complex than Simpler model 1\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 180,      # More estimators than Simpler 1\\n            'learning_rate': 0.035,   # Closer to original LR\\n            'num_leaves': 22,         # Slightly more leaves\\n            'max_depth': 4,           # Same depth\\n            'min_child_samples': 22,  # Slightly less min_child_samples\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly more subsampling\\n            'subsample': 0.75,        # Slightly more subsampling\\n            'reg_alpha': 0.15,        # Moderate regularization\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26], # Moderate number of lags\\n        'rolling_windows': [8, 16]      # Moderate number of rolling windows\\n    },\\n    {\\n        # Simpler model 3: log1p transformation with simplified features and parameters\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 150,\\n            'learning_rate': 0.05,\\n            'num_leaves': 20,\\n            'max_depth': 4,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'log1p', # Different transformation type\\n        'lag_weeks': [1, 2, 4, 8],    # Lags optimized for log1p behavior\\n        'rolling_windows': [2, 4]     # Rolling windows optimized for log1p behavior\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

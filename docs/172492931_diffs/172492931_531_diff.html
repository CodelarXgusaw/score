<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "489",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy\\n    for the test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version explicitly aims to handle seasonality and trends through comprehensive\\n    feature engineering including cyclical date features, \\"weeks since start\\" for trend,\\n    and various short-to-long term lags and rolling means. Special attention is given\\n    to time-series specific missing data handling for generated features.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = config.get('target_transform', 'log1p')\\n\\n    # A small epsilon for numerical stability in transformations (e.g., sqrt/log1p of zero)\\n    epsilon = 1e-6\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people\\n    # Handle potential zero population to prevent division errors.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation (admissions cannot be negative)\\n    admissions_per_million[admissions_per_million < 0] = 0\\n\\n    # Apply chosen target transformation (log1p, sqrt, fourth_root, or raw)\\n    # Adding epsilon to prevent issues with log(0) or sqrt(0) for very small values\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else:\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use ISO week number for consistency, which can range 1-53\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This helps model yearly patterns like flu seasons or holiday impacts.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since the start of the entire dataset, to capture overall trends.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n\\n        return df\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define lists of features used by the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # Lags capture auto-correlation and temporal dependencies.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target.\\n    # Rolling means smooth noise and capture local trends.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures the window includes data *before* the current date, avoiding future leakage.\\n        # \`min_periods=1\` allows calculation even if fewer than \`window\` points are available at series start.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS]\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This feature exists in test_x and can capture different dynamics for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs from shifting\\n    # (e.g., if a week is missing in the middle of a location's series).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining initial NaNs (at the very beginning of a location's data) with 0.0.\\n            # This is a common and reasonable approach for count data when no prior history exists,\\n            # indicating no admissions or no data to derive a feature from.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Drop rows from training data if any selected feature or target is still NaN after fillna.\\n    # This ensures no NaNs are passed to the LGBM model during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories in the model.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    categorical_features_lgbm = [LOCATION_COL]\\n\\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be extended with predictions during the iterative forecasting process.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions to the correct output format.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing: Keep original index and sort by location and date.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and required quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                # If not enough history for the specific lag, use the most recent available value.\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                # If no history at all for this location (e.g., completely new location), default to 0.0.\\n                lag_value = 0.0\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                # Use all available history if less than window size but some data exists.\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                # No history for this location.\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling any new/missing (static)\\n        # feature columns that might arise in test_x (though unlikely for this dataset) with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction to match training.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        # Ensure values are converted to a numpy array maintaining quantile order.\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            # Clamp negative predictions to 0 before inverse transform to avoid NaNs\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            # Clamp negative predictions to 0 before inverse transform.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4) - epsilon\\n        else: # If no specific transformation was applied\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Ensure all predictions are non-negative and round to integer counts.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed for consistency).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Inverse transform median prediction to admissions per million, clamping to non-negative\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n\\n        # Ensure non-negative before re-transforming for history\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n\\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# Configuration options to be evaluated by the scoring harness.\\n# These configs explore different target transformations, LightGBM hyperparameters,\\n# and choices for lagged/rolling features.\\nconfig_list = [\\n    { # Config 1: Best performing from previous trials with 'fourth_root' transformation.\\n      # Tuned LGBM params, comprehensive lags and rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 220,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    },\\n    { # Config 2: Second best from previous trials with 'log1p' transformation.\\n      # Similar LGBM parameters and feature sets, offering an alternative robust transformation.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 4, 8, 12, 26, 52],\\n        'rolling_windows': [2, 4, 8, 16]\\n    },\\n    { # Config 3: A slightly more regularized 'fourth_root' model.\\n      # Aims to prevent overfitting by reducing model complexity and increasing regularization strength.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 22,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    }\\n]",
  "new_index": "531",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression, known for its efficiency and\\n    performance in tabular data. It incorporates advanced time-series feature\\n    engineering, including cyclical date features, \\"weeks since start\\" for trend,\\n    lagged target values, lagged differences, and rolling mean/standard deviation.\\n    The target variable is first normalized by population and then transformed\\n    (e.g., using \`fourth_root\` or \`log1p\`) to stabilize variance and handle\\n    skewness, which is crucial for count data like hospital admissions.\\n\\n    The prediction strategy for the test set is iterative: predictions for earlier\\n    future time steps are used to generate features (lags, rolling statistics)\\n    for later future time steps. This ensures that the model always uses the most\\n    up-to-date available information (actuals from train, median predictions from test)\\n    for generating time-dependent features.\\n\\n    Special attention is given to:\\n    - Handling potential zero population to prevent division by zero.\\n    - Ensuring non-negative values for admissions after inverse transformation.\\n    - Robustly handling missing values in generated time-series features.\\n    - Enforcing monotonicity of quantile predictions across each row.\\n    - Using categorical features for location and forecast horizon to capture\\n      location-specific and horizon-specific patterns.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model, allowing\\n                                 customization of LGBM hyperparameters, target\\n                                 transformation type, and feature engineering\\n                                 parameters (lag weeks, rolling windows).\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'. The index of this\\n                      DataFrame matches the index of the input \`test_x\`.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = config.get('target_transform', 'log1p')\\n\\n    # A small epsilon for numerical stability in transformations (e.g., sqrt/log1p of zero)\\n    epsilon = 1e-6\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people\\n    # Handle potential zero population to prevent division errors.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation (admissions cannot be negative)\\n    admissions_per_million[admissions_per_million < 0] = 0\\n\\n    # Apply chosen target transformation (log1p, sqrt, fourth_root, or raw)\\n    # Adding epsilon to prevent issues with log(0) or sqrt(0) for very small values\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else:\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['day_of_year'] = df[DATE_COL].dt.dayofyear # New granular seasonality feature\\n\\n        # Add cyclical features for week of year and day of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n        # Using 365.25 for day of year to account for leap years over long periods\\n        df['sin_day_of_year'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\\n        df['cos_day_of_year'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\\n\\n        # Weeks since the start of the entire dataset, to capture overall trends.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n\\n        return df\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define lists of features used by the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'day_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'sin_day_of_year', 'cos_day_of_year',\\n                     'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n        # Add lagged differences for capturing trend/momentum if a prior lag exists\\n        if lag == 1: # Calculate lag_1_wk for future diffs\\n             # This is just to ensure lag_1_wk exists when other lags are > 1\\n             # and we compute diffs with lag_1_wk later.\\n            _ = train_features_df[f'lag_{lag}_wk'] # Ensure it's computed before calling it below\\n        elif lag > 1:\\n            # Current value (lag_0, conceptually) minus lagged value. Here we use lag_1_wk as \\"current\\"\\n            train_features_df[f'lag_diff_{lag}_wk'] = train_features_df[f'lag_1_wk'] - train_features_df[f'lag_{lag}_wk']\\n\\n    # Generate rolling mean and standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures the window includes data *before* the current date, avoiding future leakage.\\n        # \`min_periods=1\` allows calculation even if fewer than \`window\` points are available at series start.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n        # Add rolling standard deviation for volatility\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # std of a single point is NaN, fill with 0\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = []\\n    for lag in LAG_WEEKS:\\n        train_specific_features.append(f'lag_{lag}_wk')\\n        if lag > 1: # Only add diffs for lags > 1 (e.g., diff_2_wk, diff_3_wk etc. relative to lag_1_wk)\\n            train_specific_features.append(f'lag_diff_{lag}_wk')\\n    for window in ROLLING_WINDOWS:\\n        train_specific_features.append(f'rolling_mean_{window}_wk')\\n        train_specific_features.append(f'rolling_std_{window}_wk')\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This feature exists in test_x and can capture different dynamics for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs from shifting\\n    # (e.g., if a week is missing in the middle of a location's series).\\n    # Then fill any remaining initial NaNs (at the very beginning of a location's data) with 0.0.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0) # Fill NaNs at the start of series\\n\\n    # Drop rows from training data if any selected feature or target is still NaN after fillna.\\n    # This ensures no NaNs are passed to the LGBM model during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=X_train_model_cols + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories in the model.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    categorical_features_lgbm = [LOCATION_COL]\\n\\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be extended with predictions during the iterative forecasting process.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions to the correct output format.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing: Keep original index and sort by location and date.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and required quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        # Ensure there's enough history for lags and rolling calculations.\\n        # If not, use the most recent available value (for lags) or 0.0 (for no history).\\n        # Store computed lag_1_wk to use for diffs\\n        lag_1_wk_val = 0.0\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Use latest if not enough lag\\n            else:\\n                lag_value = 0.0 # Default if no history\\n            current_features_dict[lag_col_name] = lag_value\\n\\n            if lag == 1:\\n                lag_1_wk_val = lag_value\\n            elif lag > 1:\\n                # Calculate lagged differences relative to lag_1_wk\\n                lag_diff_col_name = f'lag_diff_{lag}_wk'\\n                current_features_dict[lag_diff_col_name] = lag_1_wk_val - lag_value\\n\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_mean_col_name = f'rolling_mean_{window}_wk'\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n\\n            if len(current_loc_hist) >= window:\\n                window_data = current_loc_hist[-window:]\\n            else:\\n                window_data = current_loc_hist # Use all available history if less than window size\\n\\n            if window_data:\\n                current_features_dict[rolling_mean_col_name] = np.mean(window_data)\\n                current_features_dict[rolling_std_col_name] = np.std(window_data)\\n            else:\\n                current_features_dict[rolling_mean_col_name] = 0.0\\n                current_features_dict[rolling_std_col_name] = 0.0\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling any new/missing\\n        # feature columns that might arise (though unlikely for this dataset) with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction to match training.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        # Ensure values are converted to a numpy array maintaining quantile order.\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            # Clamp negative predictions to 0 before inverse transform to avoid NaNs\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            # Clamp negative predictions to 0 before inverse transform.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4) - epsilon\\n        else: # If no specific transformation was applied\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Ensure all predictions are non-negative and round to integer counts.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed for consistency).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Inverse transform median prediction to admissions per million, clamping to non-negative\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n\\n        # Ensure non-negative before re-transforming for history\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n\\n        # Add to history, ensuring value is float for consistency.\\n        location_history_data[current_loc].append(float(value_to_add_to_history))\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # This step is crucial if the individual quantile models do not inherently guarantee monotonicity.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# Configuration options to be evaluated by the scoring harness.\\n# These configs explore different target transformations, LightGBM hyperparameters,\\n# and choices for lagged/rolling features, now with enhanced feature engineering\\n# (added \`day_of_year\` cyclical features, lagged differences, rolling standard deviation).\\nconfig_list = [\\n    { # Config 1: Aggressive 'fourth_root' transformation with enriched features.\\n      # Tuned LGBM params, comprehensive lags and rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 250, # Slightly increased estimators for more complexity\\n            'learning_rate': 0.025, # Slightly reduced learning rate for better convergence\\n            'num_leaves': 28, # Slightly increased leaves\\n            'max_depth': 6, # Slightly increased depth\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # More varied and relevant lags\\n        'rolling_windows': [4, 8, 16, 26] # More varied rolling windows\\n    },\\n    { # Config 2: Robust 'log1p' transformation with enriched features.\\n      # Similar LGBM parameters and feature sets, offering an alternative robust transformation.\\n        'lgbm_params': {\\n            'n_estimators': 220,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # Comprehensive lags\\n        'rolling_windows': [2, 4, 8, 16, 24] # Comprehensive rolling windows\\n    },\\n    { # Config 3: A more regularized 'fourth_root' model with enriched features.\\n      # Aims to prevent overfitting by reducing model complexity and increasing regularization strength.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 22,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Selected lags\\n        'rolling_windows': [8, 16, 26] # Selected rolling windows\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

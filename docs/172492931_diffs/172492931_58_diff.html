<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "28",
  "old_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\n# Define a list of quantiles as per competition requirements\\nQUANTILE_COLUMNS = [f'quantile_{q}' for q in [\\n    0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n    0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n]]\\n\\ndef create_features_for_all_data(full_df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates comprehensive features for a combined DataFrame (train + test).\\n    Handles time-based, population, location, horizon, and lagged target features.\\n    Lags for future dates (in test set) will appear as NaN and require imputation later.\\n\\n    Args:\\n        full_df (pd.DataFrame): A DataFrame containing both training and test data,\\n                                including 'Total COVID-19 Admissions' where known, and NaNs where unknown.\\n\\n    Returns:\\n        pd.DataFrame: The DataFrame with engineered features.\\n    \\"\\"\\"\\n    df_copy = full_df.copy()\\n\\n    # Convert target_end_date to datetime objects\\n    df_copy['target_end_date'] = pd.to_datetime(df_copy['target_end_date'])\\n\\n    # Ensure data is sorted by location and date for accurate lag calculations\\n    df_copy = df_copy.sort_values(by=['location', 'target_end_date']).reset_index(drop=True)\\n\\n    # --- Time-based features ---\\n    df_copy['year'] = df_copy['target_end_date'].dt.year\\n    df_copy['month'] = df_copy['target_end_date'].dt.month\\n    df_copy['week_of_year'] = df_copy['target_end_date'].dt.isocalendar().week.astype(int)\\n    df_copy['day_of_year'] = df_copy['target_end_date'].dt.dayofyear\\n    df_copy['day_of_week'] = df_copy['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for periodic patterns (e.g., yearly seasonality)\\n    df_copy['week_sin'] = np.sin(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['week_cos'] = np.cos(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['dayofyear_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 366) # 366 for leap years\\n    df_copy['dayofyear_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 366)\\n\\n    # --- Population features ---\\n    # Handle missing population values: Use median per location, then global median if still missing\\n    df_copy['population_imputed'] = df_copy.groupby('location')['population'].transform(lambda x: x.fillna(x.median()))\\n    df_copy['population_imputed'] = df_copy['population_imputed'].fillna(df_copy['population_imputed'].median())\\n    df_copy['population_log'] = np.log1p(df_copy['population_imputed'])\\n\\n    # --- Location and Horizon features ---\\n    df_copy['location_id'] = df_copy['location'].astype(int)\\n    # The 'horizon' column exists in test_x and will be NaN for train_x; fill train_x with 0.\\n    df_copy['horizon'] = df_copy['horizon'].fillna(0).astype(int)\\n\\n    # --- Time index feature ---\\n    # Calculated relative to the minimum 'target_end_date' in the current \`full_df\` slice.\\n    df_copy['time_idx'] = (df_copy['target_end_date'] - df_copy['target_end_date'].min()).dt.days // 7\\n\\n    # --- Lagged target features and rolling statistics ---\\n    # These are calculated based on 'Total COVID-19 Admissions'.\\n    # For rows where 'Total COVID-19 Admissions' is NaN (i.e., test data),\\n    # the corresponding lagged features will also be NaN if they fall beyond observed data.\\n    \\n    # Create temporary columns for lag calculations to avoid modifying original df_copy structure\\n    temp_admissions = df_copy['Total COVID-19 Admissions'].copy()\\n\\n    for loc in df_copy['location'].unique():\\n        loc_mask = df_copy['location'] == loc\\n        \\n        # Calculate various lags\\n        for lag in [1, 2, 3, 4, 5, 6, 7, 8]: # Lags up to 8 weeks prior\\n            df_copy.loc[loc_mask, f'admissions_lag_{lag}'] = temp_admissions.loc[loc_mask].shift(lag)\\n        \\n        # Rolling features based on past 4 weeks (shifted by 1 to avoid data leakage)\\n        df_copy.loc[loc_mask, 'admissions_rolling_mean_4w'] = temp_admissions.loc[loc_mask].rolling(window=4, min_periods=1).mean().shift(1)\\n        df_copy.loc[loc_mask, 'admissions_rolling_std_4w'] = temp_admissions.loc[loc_mask].rolling(window=4, min_periods=1).std().shift(1)\\n        \\n    return df_copy\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile\\n    and makes predictions on the test set, including time-series specific feature handling.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    quantiles = [float(col.split('_')[-1]) for col in QUANTILE_COLUMNS]\\n\\n    # Store original test_x index for final output alignment\\n    original_test_index = test_x.index\\n    \\n    # 1. Combine train_x, train_y, and test_x for unified feature engineering\\n    # Mark rows to distinguish train from test after feature engineering\\n    train_data_combined = train_x.copy()\\n    train_data_combined['Total COVID-19 Admissions'] = train_y # Add target for lag calculation\\n    train_data_combined['_is_test'] = False # Flag for train rows\\n\\n    test_data_combined = test_x.copy()\\n    test_data_combined['Total COVID-19 Admissions'] = np.nan # Target is unknown for test data\\n    test_data_combined['_is_test'] = True # Flag for test rows\\n\\n    # Concatenate train and test data for consistent feature creation\\n    full_df = pd.concat([train_data_combined, test_data_combined], ignore_index=True)\\n    \\n    # 2. Apply comprehensive feature engineering to the combined dataset\\n    full_features_df = create_features_for_all_data(full_df)\\n\\n    # 3. Split back into training and testing sets based on the '_is_test' flag\\n    train_features = full_features_df[full_features_df['_is_test'] == False].set_index(train_x.index)\\n    test_features = full_features_df[full_features_df['_is_test'] == True].set_index(original_test_index)\\n\\n    # Drop the temporary '_is_test' column and original target column from features\\n    train_features = train_features.drop(columns=['_is_test', 'Total COVID-19 Admissions'])\\n    test_features = test_features.drop(columns=['_is_test', 'Total COVID-19 Admissions'])\\n\\n    # 4. Define features for the model\\n    # Exclude 'population', 'target_end_date', 'location_name' etc., as engineered versions are used\\n    all_potential_features = [\\n        'year', 'month', 'week_of_year', 'day_of_year', 'day_of_week',\\n        'week_sin', 'week_cos', 'dayofyear_sin', 'dayofyear_cos',\\n        'population_log', 'location_id', 'horizon', 'time_idx'\\n    ]\\n    # Add engineered lag and rolling features\\n    lag_features = [f'admissions_lag_{lag}' for lag in [1, 2, 3, 4, 5, 6, 7, 8]] + \\\\\\n                   ['admissions_rolling_mean_4w', 'admissions_rolling_std_4w']\\n    all_potential_features.extend(lag_features)\\n    \\n    # Filter to only include columns that are actually present in *both* dataframes\\n    final_features = list(set(all_potential_features) & set(train_features.columns) & set(test_features.columns))\\n\\n    X_train = train_features[final_features]\\n    X_test = test_features[final_features]\\n    y_train = train_y.copy() # Ensure y_train is a copy\\n\\n    # 5. Handle any remaining NaNs in features\\n    # Missing lagged features (e.g., at the very start of the time series or for test set predictions beyond known data)\\n    # are imputed with 0. Other missing numerical features are imputed with their median from the training set.\\n    for col in final_features:\\n        # Impute NaNs in training data\\n        if X_train[col].isnull().any():\\n            if col in lag_features:\\n                X_train[col] = X_train[col].fillna(0) # Common for missing lags\\n            else:\\n                X_train[col] = X_train[col].fillna(X_train[col].median())\\n        \\n        # Impute NaNs in test data\\n        if X_test[col].isnull().any():\\n            if col in lag_features:\\n                X_test[col] = X_test[col].fillna(0) # Fill missing lags in test with 0\\n            else:\\n                # Use training data's median for imputation in test data\\n                X_test[col] = X_test[col].fillna(X_train[col].median() if not X_train[col].isnull().all() else 0)\\n\\n    # Define categorical features for LightGBM.\\n    # LightGBM can directly handle integer-encoded categorical features.\\n    categorical_features = ['location_id', 'year', 'month', 'week_of_year',\\n                            'day_of_year', 'day_of_week', 'horizon']\\n    # Filter to ensure only features present in the final dataset are used as categorical\\n    categorical_features = [f for f in categorical_features if f in final_features]\\n\\n    # Retrieve LightGBM model hyperparameters from the config dictionary,\\n    # or use default values if not specified.\\n    lgbm_params = config.get('lgbm_params', {\\n        'objective': 'quantile',  # Objective for quantile regression\\n        'metric': 'quantile',     # Evaluation metric for quantile regression\\n        'n_estimators': 400,      # Number of boosting rounds\\n        'learning_rate': 0.02,    # Step size shrinkage\\n        'num_leaves': 48,         # Max number of leaves in one tree\\n        'verbose': -1,            # Suppress verbose output during training\\n        'n_jobs': -1,             # Use all available CPU cores\\n        'seed': 42,               # Random seed for reproducibility\\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\\n        'lambda_l1': 0.05,        # L1 regularization\\n        'lambda_l2': 0.05,        # L2 regularization\\n        'feature_fraction': 0.7,  # Fraction of features considered at each split\\n        'bagging_fraction': 0.7,  # Fraction of data sampled for each tree\\n        'bagging_freq': 1         # Frequency for bagging\\n    })\\n\\n    # DataFrame to store all quantile predictions for the test set\\n    test_y_hat_quantiles = pd.DataFrame(index=original_test_index, columns=QUANTILE_COLUMNS)\\n\\n    # Train a separate LightGBM model for each quantile\\n    for q in quantiles:\\n        # Update the 'alpha' parameter for the current quantile\\n        current_lgbm_params = lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        # Initialize and train the LightGBM Regressor\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        model.fit(X_train, y_train, categorical_feature=categorical_features)\\n\\n        # Make predictions on the test set\\n        preds = model.predict(X_test)\\n        \\n        # Ensure predictions are non-negative, as admissions cannot be less than zero\\n        preds[preds < 0] = 0\\n        \\n        # Store predictions in the results DataFrame\\n        test_y_hat_quantiles[f'quantile_{q}'] = preds\\n\\n    # Enforce monotonicity for quantile predictions\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation metric.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        # For each row, set the current quantile's prediction to be at least the previous one's.\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    return test_y_hat_quantiles\\n\\n# These configurations will be used by the evaluation harness.\\n# A list of dictionaries, where each dictionary defines a set of parameters\\n# to be passed to the \`fit_and_predict_fn\`. The harness will iterate through\\n# each config, run the rolling window evaluation, and score the results,\\n# ultimately selecting the best performing configuration.\\nconfig_list = [\\n    {\\n        # Default/Optimized configuration with more estimators and adjusted params\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 400,\\n            'learning_rate': 0.02,\\n            'num_leaves': 48,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.05,\\n            'lambda_l2': 0.05,\\n            'feature_fraction': 0.7,\\n            'bagging_fraction': 0.7,\\n            'bagging_freq': 1\\n        }\\n    },\\n    {\\n        # Configuration with more estimators and deeper trees, potentially for higher complexity\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 600,\\n            'learning_rate': 0.015,\\n            'num_leaves': 64,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.01,\\n            'lambda_l2': 0.01,\\n            'feature_fraction': 0.6,\\n            'bagging_fraction': 0.6,\\n            'bagging_freq': 1\\n        }\\n    },\\n    {\\n        # Configuration with fewer estimators, higher learning rate, and fewer leaves\\n        # This might be faster for exploration or less prone to overfitting on small folds.\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 200,\\n            'learning_rate': 0.05,\\n            'num_leaves': 24,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.1,\\n            'lambda_l2': 0.1,\\n            'feature_fraction': 0.8,\\n            'bagging_fraction': 0.8,\\n            'bagging_freq': 1\\n        }\\n    }\\n]",
  "new_index": "58",
  "new_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\n# Define a list of quantiles as per competition requirements\\nQUANTILE_COLUMNS = [f'quantile_{q}' for q in [\\n    0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n    0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n]]\\n\\ndef create_features_for_all_data(full_df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates comprehensive features for a combined DataFrame (train + test).\\n    Handles time-based, population, location, horizon, and lagged target features.\\n    Lags for future dates (in test set) will appear as NaN and require imputation later.\\n\\n    Args:\\n        full_df (pd.DataFrame): A DataFrame containing both training and test data,\\n                                including 'Total COVID-19 Admissions' where known, and NaNs where unknown.\\n\\n    Returns:\\n        pd.DataFrame: The DataFrame with engineered features.\\n    \\"\\"\\"\\n    df = full_df.copy() # Operate on a true copy to prevent SettingWithCopyWarning\\n\\n    # Convert target_end_date to datetime objects\\n    df['target_end_date'] = pd.to_datetime(df['target_end_date'])\\n\\n    # Ensure data is sorted by location and date for accurate lag calculations\\n    df = df.sort_values(by=['location', 'target_end_date']).reset_index(drop=True)\\n\\n    # --- Time-based features ---\\n    df['year'] = df['target_end_date'].dt.year\\n    df['month'] = df['target_end_date'].dt.month\\n    df['week_of_year'] = df['target_end_date'].dt.isocalendar().week.astype(int)\\n    df['day_of_year'] = df['target_end_date'].dt.dayofyear\\n    df['day_of_week'] = df['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for periodic patterns (e.g., yearly seasonality)\\n    df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n    df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n    df['dayofyear_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 366) # 366 for leap years\\n    df['dayofyear_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 366)\\n\\n    # --- Population features ---\\n    # Impute missing population values: Use median per location, then global median if still missing\\n    location_median_pop = df.groupby('location')['population'].median()\\n    global_median_pop = df['population'].median() # Global median from the original population column\\n    \\n    df['population_imputed'] = df['population'].copy() # Start with a copy of original population\\n    \\n    # Fill NA values using location-specific median\\n    for loc, median_val in location_median_pop.items():\\n        if pd.notna(median_val):\\n            df.loc[(df['location'] == loc) & (df['population_imputed'].isna()), 'population_imputed'] = median_val\\n            \\n    # Fill any remaining NA values with the global median (for locations with all NA population, or completely missing)\\n    df['population_imputed'] = df['population_imputed'].fillna(global_median_pop)\\n    \\n    df['population_log'] = np.log1p(df['population_imputed'])\\n\\n    # --- Location and Horizon features ---\\n    df['location_id'] = df['location'].astype(int)\\n    # The 'horizon' column exists in test_x and will be NaN for train_x; fill train_x with 0.\\n    df['horizon'] = df['horizon'].fillna(0).astype(int)\\n\\n    # --- Time index feature ---\\n    # Calculated relative to the minimum 'target_end_date' in the current \`full_df\` slice.\\n    df['time_idx'] = (df['target_end_date'] - df['target_end_date'].min()).dt.days // 7\\n\\n    # --- Lagged target features and rolling statistics ---\\n    # These are calculated based on 'Total COVID-19 Admissions'.\\n    # For rows where 'Total COVID-19 Admissions' is NaN (i.e., test data),\\n    # the corresponding lagged features will also be NaN if they fall beyond observed data.\\n    \\n    # Group by location and apply shift/rolling operations\\n    for lag in [1, 2, 3, 4, 5, 6, 7, 8]: # Lags up to 8 weeks prior\\n        df[f'admissions_lag_{lag}'] = df.groupby('location')['Total COVID-19 Admissions'].shift(lag)\\n    \\n    # Rolling features based on past 4 weeks (shifted by 1 to avoid data leakage)\\n    df['admissions_rolling_mean_4w'] = df.groupby('location')['Total COVID-19 Admissions'].transform(\\n        lambda x: x.rolling(window=4, min_periods=1).mean().shift(1)\\n    )\\n    df['admissions_rolling_std_4w'] = df.groupby('location')['Total COVID-19 Admissions'].transform(\\n        lambda x: x.rolling(window=4, min_periods=1).std().shift(1)\\n    )\\n        \\n    return df\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame: # Changed return type hint to pd.DataFrame as per competition spec\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile\\n    and makes predictions on the test set, including time-series specific feature handling.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    quantiles = [float(col.split('_')[-1]) for col in QUANTILE_COLUMNS]\\n\\n    # Store original test_x index for final output alignment\\n    original_test_index = test_x.index\\n    \\n    # 1. Combine train_x, train_y, and test_x for unified feature engineering\\n    # Mark rows to distinguish train from test after feature engineering\\n    train_data_combined = train_x.copy()\\n    train_data_combined['Total COVID-19 Admissions'] = train_y # Add target for lag calculation\\n    train_data_combined['_is_test'] = False # Flag for train rows\\n\\n    test_data_combined = test_x.copy()\\n    test_data_combined['Total COVID-19 Admissions'] = np.nan # Target is unknown for test data\\n    test_data_combined['_is_test'] = True # Flag for test rows\\n\\n    # Concatenate train and test data for consistent feature creation\\n    full_df = pd.concat([train_data_combined, test_data_combined], ignore_index=True)\\n    \\n    # 2. Apply comprehensive feature engineering to the combined dataset\\n    full_features_df = create_features_for_all_data(full_df)\\n\\n    # 3. Split back into training and testing sets based on the '_is_test' flag\\n    # Use original indices to ensure correct alignment\\n    train_features = full_features_df[full_features_df['_is_test'] == False].set_index(train_x.index)\\n    test_features = full_features_df[full_features_df['_is_test'] == True].set_index(original_test_index)\\n\\n    # Drop the temporary '_is_test' column and original target column from features\\n    train_features = train_features.drop(columns=['_is_test', 'Total COVID-19 Admissions'])\\n    test_features = test_features.drop(columns=['_is_test', 'Total COVID-19 Admissions'])\\n\\n    # 4. Define features for the model\\n    # Exclude 'population', 'target_end_date', 'location_name' etc., as engineered versions are used\\n    all_potential_features = [\\n        'year', 'month', 'week_of_year', 'day_of_year', 'day_of_week',\\n        'week_sin', 'week_cos', 'dayofyear_sin', 'dayofyear_cos',\\n        'population_log', 'location_id', 'horizon', 'time_idx'\\n    ]\\n    # Add engineered lag and rolling features\\n    lag_features = [f'admissions_lag_{lag}' for lag in [1, 2, 3, 4, 5, 6, 7, 8]] + \\\\\\n                   ['admissions_rolling_mean_4w', 'admissions_rolling_std_4w']\\n    all_potential_features.extend(lag_features)\\n    \\n    # Filter to only include columns that are actually present in *both* dataframes\\n    final_features = list(set(all_potential_features) & set(train_features.columns) & set(test_features.columns))\\n\\n    X_train = train_features[final_features]\\n    X_test = test_features[final_features]\\n    y_train = train_y.copy() # Ensure y_train is a copy\\n\\n    # 5. Handle any remaining NaNs in features\\n    # Missing lagged features (e.g., at the very start of the time series or for test set predictions beyond known data)\\n    # are imputed with 0. Other missing numerical features are imputed with their median from the training set.\\n    for col in final_features:\\n        # Impute NaNs in training data\\n        if X_train[col].isnull().any():\\n            if col in lag_features:\\n                X_train[col] = X_train[col].fillna(0) # Common for missing lags\\n            else:\\n                X_train[col] = X_train[col].fillna(X_train[col].median())\\n        \\n        # Impute NaNs in test data\\n        if X_test[col].isnull().any():\\n            if col in lag_features:\\n                X_test[col] = X_test[col].fillna(0) # Fill missing lags in test with 0\\n            else:\\n                # Use training data's median for imputation in test data\\n                # Handle case where training column might be all NaNs itself\\n                fill_value = X_train[col].median() if not X_train[col].isnull().all() else 0\\n                X_test[col] = X_test[col].fillna(fill_value)\\n\\n    # Define categorical features for LightGBM.\\n    categorical_features = ['location_id', 'year', 'month', 'week_of_year',\\n                            'day_of_year', 'day_of_week', 'horizon']\\n    # Filter to ensure only features present in the final dataset are used as categorical\\n    categorical_features = [f for f in categorical_features if f in final_features]\\n\\n    # Retrieve LightGBM model hyperparameters from the config dictionary,\\n    # or use default values if not specified.\\n    # Using canonical LightGBM parameter names to avoid warnings.\\n    lgbm_params = config.get('lgbm_params', {\\n        'objective': 'quantile',      # Objective for quantile regression\\n        'metric': 'quantile',         # Evaluation metric for quantile regression\\n        'n_estimators': 400,          # Number of boosting rounds\\n        'learning_rate': 0.02,        # Step size shrinkage\\n        'num_leaves': 48,             # Max number of leaves in one tree\\n        'verbose': -1,                # Suppress verbose output during training\\n        'n_jobs': -1,                 # Use all available CPU cores\\n        'seed': 42,                   # Random seed for reproducibility\\n        'boosting_type': 'gbdt',      # Gradient Boosting Decision Tree\\n        'reg_alpha': 0.05,            # L1 regularization\\n        'reg_lambda': 0.05,           # L2 regularization\\n        'colsample_bytree': 0.7,      # Fraction of features considered at each split\\n        'subsample': 0.7,             # Fraction of data sampled for each tree\\n        'subsample_freq': 1           # Frequency for subsampling\\n    })\\n\\n    # DataFrame to store all quantile predictions for the test set\\n    test_y_hat_quantiles = pd.DataFrame(index=original_test_index, columns=QUANTILE_COLUMNS)\\n\\n    # Train a separate LightGBM model for each quantile\\n    for q in quantiles:\\n        # Update the 'alpha' parameter for the current quantile\\n        current_lgbm_params = lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        # Initialize and train the LightGBM Regressor\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        model.fit(X_train, y_train, categorical_feature=categorical_features)\\n\\n        # Make predictions on the test set\\n        preds = model.predict(X_test)\\n        \\n        # Ensure predictions are non-negative, as admissions cannot be less than zero\\n        preds[preds < 0] = 0\\n        \\n        # Store predictions in the results DataFrame\\n        test_y_hat_quantiles[f'quantile_{q}'] = preds\\n\\n    # Enforce monotonicity for quantile predictions\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation metric.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        # For each row, set the current quantile's prediction to be at least the previous one's.\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    return test_y_hat_quantiles\\n\\n# These configurations will be used by the evaluation harness.\\n# A list of dictionaries, where each dictionary defines a set of parameters\\n# to be passed to the \`fit_and_predict_fn\`. The harness will iterate through\\n# each config, run the rolling window evaluation, and score the results,\\n# ultimately selecting the best performing configuration.\\nconfig_list = [\\n    {\\n        # Default/Optimized configuration with more estimators and adjusted params\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 400,\\n            'learning_rate': 0.02,\\n            'num_leaves': 48,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'reg_alpha': 0.05,        # L1 regularization\\n            'reg_lambda': 0.05,       # L2 regularization\\n            'colsample_bytree': 0.7,  # Fraction of features\\n            'subsample': 0.7,         # Fraction of data\\n            'subsample_freq': 1       # Frequency for subsampling\\n        }\\n    },\\n    {\\n        # Configuration with more estimators and deeper trees, potentially for higher complexity\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 600,\\n            'learning_rate': 0.015,\\n            'num_leaves': 64,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'reg_alpha': 0.01,\\n            'reg_lambda': 0.01,\\n            'colsample_bytree': 0.6,\\n            'subsample': 0.6,\\n            'subsample_freq': 1\\n        }\\n    },\\n    {\\n        # Configuration with fewer estimators, higher learning rate, and fewer leaves\\n        # This might be faster for exploration or less prone to overfitting on small folds.\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 200,\\n            'learning_rate': 0.05,\\n            'num_leaves': 24,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'subsample_freq': 1\\n        }\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "710",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters (can be overridden by config)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile', # or 'rmse' if using standard regression + quantile loss\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 1, # This will be updated by specific config in config_list if provided\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.0,\\n        'lambda': 1.0,\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    # Important: remove 'enable_categorical' from config-provided xgb_params\\n    # as we will manually integer encode categorical features for XGBoost.\\n    xgb_params_config_input = config.get('xgb_params', {})\\n    if 'enable_categorical' in xgb_params_config_input:\\n        del xgb_params_config_input['enable_categorical'] # Ensure manual integer encoding is used\\n\\n    xgb_params = {**default_xgb_params, **xgb_params_config_input}\\n\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before re-transform\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Other categorical features might be added here, e.g. 'horizon'\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill std NaNs (e.g. for single value) with 0\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Dynamically compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_pre_dropna = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model_pre_dropna.columns:\\n        X_train_model_pre_dropna[HORIZON_COL] = 0\\n\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # For any initial NaNs (where no prior values exist to ffill), fill with 0.0.\\n    for col in train_specific_features:\\n        X_train_model_pre_dropna[col] = X_train_model_pre_dropna.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n        X_train_model_pre_dropna[col] = X_train_model_pre_dropna[col].fillna(0.0)\\n\\n    train_combined = X_train_model_pre_dropna.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target is NaN, or if essential BASE_FEATURES/CATEGORICAL_FEATURES/HORIZON_COL are NaN.\\n    # This cleans the data used for training the model.\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL] + BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features: Create consistent categories and integer mappings ---\\n    # Get all unique locations and horizons from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n\\n    # For LightGBM: use pandas CategoricalDtype\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    X_train_lgbm[HORIZON_COL] = X_train_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    categorical_feature_names_lgbm = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_lgbm.columns]\\n\\n    # For XGBoost: use explicit integer encoding\\n    location_mapper = {cat: code for code, cat in enumerate(all_location_categories)}\\n    horizon_mapper = {cat: code for code, cat in enumerate(all_horizon_categories)}\\n    \\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_mapper).fillna(-1).astype(int)\\n    X_train_xgb[HORIZON_COL] = X_train_xgb[HORIZON_COL].map(horizon_mapper).fillna(-1).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction.\\n    # This should be based on \`X_train_lgbm\` as it ensures all columns are captured.\\n    X_train_model_cols = X_train_lgbm.columns.tolist()\\n\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names_lgbm)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # X_train_xgb passed here, which has integer-encoded categorical columns\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's TRANSFORMED target values\\n    # from the CLEANED training set (after dropping NaNs).\\n    location_history_data = (\\n        train_combined.set_index(LOCATION_COL)[TRANSFORMED_TARGET_COL]\\n        .groupby(level=0)\\n        .apply(list)\\n        .to_dict()\\n    )\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to last known value\\n            else:\\n                lag_value = 0.0 # Fallback for empty history (e.g., new location not in train)\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Not enough data for full diff, but at least 2 points\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2] # Use 1-period diff\\n            else:\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Mean of all available\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling standard deviation features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create DataFrames for prediction for LGBM and XGBoost, ensuring columns match training.\\n        # Use .copy() to ensure modifications to one don't affect others\\n        X_test_row_base = pd.DataFrame([current_features_dict]).reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare for LightGBM prediction\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row_lgbm[HORIZON_COL] = X_test_row_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Prepare for XGBoost prediction (integer-encoded categorical features)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_mapper).fillna(-1).astype(int)\\n        X_test_row_xgb[HORIZON_COL] = X_test_row_xgb[HORIZON_COL].map(horizon_mapper).fillna(-1).astype(int)\\n        # Explicitly fill any remaining NaNs after feature engineering for XGBoost\\n        # This is a crucial step to prevent NaNs from propagating into XGBoost predictions\\n        X_test_row_xgb = X_test_row_xgb.fillna(0.0)\\n\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    ensemble_preds_for_q.append(lgbm_model_q.predict(X_test_row_lgbm)[0])\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    # Ensure no NaN or Inf is returned by XGBoost; if it is, consider it 0\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    # If pred is not finite, replace with 0.0 before appending.\\n                    # This is a safety measure to prevent Inf/NaN from propagating from XGBoost.\\n                    if np.isfinite(pred): \\n                        ensemble_preds_for_q.append(pred)\\n                    else:\\n                        # Fallback for non-finite predictions (NaN or Inf)\\n                        # A more sophisticated fallback could be the median of the train_y_model\\n                        # or even the LGBM prediction if available. For now, 0.0 is a safe choice\\n                        # that won't result in Inf.\\n                        ensemble_preds_for_q.append(0.0)\\n            \\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback in case no models were trained for a specific quantile/model type.\\n                # Should not happen if ensemble_model_types has at least one valid type and n_ensemble_members > 0.\\n                # If it does happen, use the mean of the training target (transformed).\\n                row_predictions_transformed[q] = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n\\n        # Ensure admissions per million predictions are non-negative after inverse transformation.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle missing, zero, or non-positive population explicitly\\n        if pd.isna(population_val) or population_val <= 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform the median prediction to admissions per million to apply non-negativity.\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million) # ensure non-negative\\n\\n        # Re-transform this median value back to the *feature* scale for use in generating future lag/rolling features.\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        # Ensure the value added to history is finite before appending\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            # If the median prediction is non-finite, append a reasonable fallback, e.g., 0.0\\n            location_history_data.setdefault(current_loc, []).append(0.0)\\n\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # Convert to float array for robust sorting, then convert back to integer.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    # Convert back to DataFrame with original columns and index.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: LGBM-only (previous best)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # Empty params as XGBoost is not used with this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], # Only LGBM\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, # Explicitly no XGB members\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, with increased XGB stability\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 5,\\n            'min_child_weight': 5, # Increased for stability, previously 1\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.0,\\n            'lambda': 1.0,\\n            # 'alpha' is overwritten by quantile 'q' inside the function\\n            # 'enable_categorical' is handled by the function by removing it and using manual integer encoding\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], # Ensemble both\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 3, # Increased for robustness, previously 1\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost parameters, increased XGB stability\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 6,\\n            'min_child_weight': 5, # Increased for stability, previously 1\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.1,\\n            'lambda': 0.15,\\n            # 'alpha' is overwritten by quantile 'q' inside the function\\n            # 'enable_categorical' is handled by the function by removing it and using manual integer encoding\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 3, # Increased for robustness, previously 1\\n    }\\n]",
  "new_index": "869",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}.copy() # Ensure copy to prevent modification issues\\n\\n    # Default XGBoost parameters (can be overridden by config)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile', # or 'rmse' if using standard regression + quantile loss\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 1, # This will be updated by specific config in config_list if provided\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.0,\\n        'lambda': 1.0,\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    # Important: remove 'enable_categorical' from config-provided xgb_params\\n    # as we will manually integer encode categorical features for XGBoost.\\n    xgb_params_config_input = config.get('xgb_params', {}).copy() # Ensure copy\\n    if 'enable_categorical' in xgb_params_config_input:\\n        del xgb_params_config_input['enable_categorical'] # Ensure manual integer encoding is used\\n\\n    xgb_params = {**default_xgb_params, **xgb_params_config_input}\\n\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before re-transform\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Note: For transformed_target = 1.0 (admissions=0), inverse should be 0.\\n            # np.power(x, 4) - 1.0 handles this correctly if x is 1.0.\\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Other categorical features might be added here, e.g. 'horizon'\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill std NaNs (e.g. for single value) with 0\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Dynamically compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_pre_dropna = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model_pre_dropna.columns:\\n        X_train_model_pre_dropna[HORIZON_COL] = 0\\n\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # For any initial NaNs (where no prior values exist to ffill), fill with 0.0.\\n    for col in train_specific_features:\\n        X_train_model_pre_dropna[col] = X_train_model_pre_dropna.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n        X_train_model_pre_dropna[col] = X_train_model_pre_dropna[col].fillna(0.0)\\n\\n    train_combined = X_train_model_pre_dropna.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target is NaN, or if essential BASE_FEATURES/CATEGORICAL_FEATURES/HORIZON_COL are NaN.\\n    # This cleans the data used for training the model.\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL] + BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Cache min/max for clipping transformed predictions later\\n    transformed_train_min_val = y_train_model.min() if not y_train_model.empty else 0.0\\n    transformed_train_max_val = y_train_model.max() if not y_train_model.empty else 1.0\\n\\n\\n    # --- Handle categorical features: Create consistent categories and integer mappings ---\\n    # Get all unique locations and horizons from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n\\n    # For LightGBM: use pandas CategoricalDtype\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    X_train_lgbm[HORIZON_COL] = X_train_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    categorical_feature_names_lgbm = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_lgbm.columns]\\n\\n    # For XGBoost: use explicit integer encoding\\n    location_mapper = {cat: code for code, cat in enumerate(all_location_categories)}\\n    horizon_mapper = {cat: code for code, cat in enumerate(all_horizon_categories)}\\n    \\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_mapper).fillna(-1).astype(int)\\n    X_train_xgb[HORIZON_COL] = X_train_xgb[HORIZON_COL].map(horizon_mapper).fillna(-1).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction.\\n    # This should be based on \`X_train_lgbm\` as it ensures all columns are captured.\\n    X_train_model_cols = X_train_lgbm.columns.tolist()\\n\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names_lgbm)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # X_train_xgb passed here, which has integer-encoded categorical columns\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's TRANSFORMED target values\\n    # from the CLEANED training set (after dropping NaNs).\\n    location_history_data = (\\n        train_combined.set_index(LOCATION_COL)[TRANSFORMED_TARGET_COL]\\n        .groupby(level=0)\\n        .apply(list)\\n        .to_dict()\\n    )\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to last known value\\n            else:\\n                lag_value = 0.0 # Fallback for empty history (e.g., new location not in train)\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Not enough data for full diff, but at least 2 points\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2] # Use 1-period diff\\n            else:\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Mean of all available\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling standard deviation features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create DataFrames for prediction for LGBM and XGBoost, ensuring columns match training.\\n        # Use .copy() to ensure modifications to one don't affect others\\n        X_test_row_base = pd.DataFrame([current_features_dict]).reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare for LightGBM prediction\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row_lgbm[HORIZON_COL] = X_test_row_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Prepare for XGBoost prediction (integer-encoded categorical features)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_mapper).fillna(-1).astype(int)\\n        X_test_row_xgb[HORIZON_COL] = X_test_row_xgb[HORIZON_COL].map(horizon_mapper).fillna(-1).astype(int)\\n        # Explicitly fill any remaining NaNs after feature engineering for XGBoost\\n        # This is a crucial step to prevent NaNs from propagating into XGBoost predictions\\n        X_test_row_xgb = X_test_row_xgb.fillna(0.0)\\n\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    ensemble_preds_for_q.append(lgbm_model_q.predict(X_test_row_lgbm)[0])\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    # If pred is not finite, replace with 0.0 before appending.\\n                    # This is a safety measure to prevent Inf/NaN from propagating from XGBoost.\\n                    if np.isfinite(pred): \\n                        ensemble_preds_for_q.append(pred)\\n                    else:\\n                        # Fallback for non-finite predictions\\n                        ensemble_preds_for_q.append(0.0) # Using 0.0 for safety\\n\\n            if ensemble_preds_for_q:\\n                mean_pred_transformed = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback in case no models were trained or all predictions were non-finite.\\n                mean_pred_transformed = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n            # Apply clipping to transformed prediction for stability\\n            # Clip between min observed transformed value and 1.5x max observed transformed value\\n            # This prevents extremely large or small predictions after inverse transform.\\n            clipped_pred = np.clip(mean_pred_transformed, transformed_train_min_val, transformed_train_max_val * 1.5)\\n            \\n            row_predictions_transformed[q] = clipped_pred\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n\\n        # Ensure admissions per million predictions are non-negative after inverse transformation.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle missing, zero, or non-positive population explicitly\\n        if pd.isna(population_val) or population_val <= 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform the median prediction to admissions per million to apply non-negativity.\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million) # ensure non-negative\\n\\n        # Re-transform this median value back to the *feature* scale for use in generating future lag/rolling features.\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        # Ensure the value added to history is finite before appending\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            # If the median prediction is non-finite, append a reasonable fallback:\\n            #  - The last valid value in history, if available\\n            #  - Or the transformed training mean, if history is empty\\n            #  - Or 0.0 as a last resort\\n            fallback_val = current_loc_hist[-1] if current_loc_hist else (transformed_train_min_val if not y_train_model.empty else 0.0)\\n            location_history_data.setdefault(current_loc, []).append(fallback_val)\\n\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # Convert to float array for robust sorting, then convert back to integer.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    # Convert back to DataFrame with original columns and index.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: LGBM-only (previous best)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # Empty params as XGBoost is not used with this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], # Only LGBM\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, # Explicitly no XGB members\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, with increased XGB stability + Clipping\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 5,\\n            'min_child_weight': 10, # Increased for stability from 5 in previous config, and default 1\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.0,\\n            'lambda': 1.0,\\n            # 'alpha' is overwritten by quantile 'q' inside the function\\n            # 'enable_categorical' is handled by the function by removing it and using manual integer encoding\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], # Ensemble both\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 3, # Increased for robustness\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost parameters, increased XGB stability + Clipping\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 6,\\n            'min_child_weight': 10, # Increased for stability from 5 in previous config, and default 1\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.1,\\n            'lambda': 0.15,\\n            # 'alpha' is overwritten by quantile 'q' inside the function\\n            # 'enable_categorical' is handled by the function by removing it and using manual integer encoding\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 3, # Increased for robustness\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

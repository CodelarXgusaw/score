<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "265",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x using LightGBM for quantile regression.\\n\\n    This function builds a robust forecasting model by leveraging a comprehensive set\\n    of time-series features, including lagged values and rolling statistics of the\\n    population-normalized and transformed target variable. Location and time-based\\n    features are also incorporated. The model is designed for multi-step-ahead\\n    probabilistic forecasting, directly predicting quantiles for various horizons.\\n\\n    Model Explanation and Feature Importance:\\n\\n    The model employs LightGBM, a gradient-boosting framework known for its speed,\\n    efficiency, and ability to handle tabular data, including categorical features\\n    and quantile regression. For each desired quantile (e.g., 0.01, 0.5, 0.99),\\n    a separate LightGBM model is trained using the 'quantile' objective.\\n\\n    Key Features and Their Importance:\\n\\n    1.  **Target Transformation (\`transformed_admissions_per_million\`):**\\n        *   **Purpose:** To normalize the target variable across different state populations\\n            and stabilize its variance.\\n        *   **Mechanism:** The raw \`Total COVID-19 Admissions\` is first normalized by\\n            \`population\` to \`admissions_per_million\`. Then, a \`log1p\` (log(1+x)) transformation\\n            is applied (default). This makes the highly skewed count data more amenable\\n            to modeling by compressing its range and making the distribution more\\n            symmetrical, which helps LightGBM learn more effectively.\\n\\n    2.  **Lagged Target Features (\`lag_X_wk\`):**\\n        *   **Importance:** These are typically the most critical features in time-series\\n            forecasting. Past values of hospital admissions are the strongest\\n            predictors of future admissions due to the autoregressive nature of epidemics.\\n        *   **Mechanism:** Lagged values (e.g., 1, 2, 4, 8, 26, 52 weeks prior) of the\\n            transformed target are included. Short lags capture immediate trends,\\n            while longer lags (e.g., \`lag_52_wk\`) capture strong annual seasonality,\\n            as respiratory illnesses often follow yearly cycles.\\n        *   **For Multi-step Forecasting:** For future horizons in \`test_x\`, these\\n            lags represent the admissions at the *last observed historical point*\\n            (the end of \`train_y\`). The model then learns how these \\"seed\\" values\\n            propagate over different forecast horizons.\\n\\n    3.  **Rolling Mean Features (\`rolling_mean_Y_wk\`):**\\n        *   **Importance:** These features provide a smoothed representation of recent trends,\\n            reducing noise present in single-week lagged values. They help the model\\n            understand the underlying trajectory of the pandemic.\\n        *   **Mechanism:** Rolling means over various windows (e.g., 4, 8, 16 weeks)\\n            of the transformed target are calculated. These averages capture\\n            short-to-medium term smoothed trends and are shifted by one week to\\n            prevent data leakage (only using past information).\\n\\n    4.  **Lagged Difference Features (\`lag_diff_Xwk_Ywk\` - if configured):**\\n        *   **Importance:** While not found to be optimal in the previous trial for this\\n            specific setup, these features are designed to capture the *rate of change*\\n            or acceleration/deceleration of the pandemic. For example, \`lag_diff_1wk_2wk\`\\n            indicates if the current trend is rising or falling week-over-week.\\n        *   **Mechanism:** Differences between various lagged values are computed.\\n\\n    5.  **Date-based Features (\`year\`, \`month\`, \`week_of_year\`, \`sin_week_of_year\`, \`cos_week_of_year\`, \`weeks_since_start\`):**\\n        *   **Importance:** These capture both long-term trends and cyclical seasonality.\\n        *   **Mechanism:**\\n            *   \`year\`, \`weeks_since_start\`: Capture overall progression of the pandemic,\\n                reflecting changes in immunity, vaccine uptake, variant emergence, etc.\\n            *   \`month\`, \`week_of_year\`: Capture monthly/weekly seasonality.\\n            *   \`sin_week_of_year\`, \`cos_week_of_year\`: Cyclical transformations of\\n                \`week_of_year\` ensure that the model understands the continuous nature of\\n                weeks (e.g., week 52 is close to week 1). This is crucial for capturing\\n                seasonal patterns like winter peaks in a smooth manner.\\n\\n    6.  **Location Identifier (\`location\`):**\\n        *   **Importance:** Highly important for capturing state-specific dynamics,\\n            baseline admission rates, public health measures, and population behavior.\\n            Each state may have a unique epidemiological curve.\\n        *   **Mechanism:** Treated as a categorical feature by LightGBM for efficient encoding.\\n\\n    7.  **Horizon Feature (\`horizon\`):**\\n        *   **Importance:** Essential for multi-step forecasting. This feature explicitly\\n            tells the model how many weeks into the future the prediction is for.\\n            The model learns different relationships between features and the target\\n            for different horizons (e.g., current trends are more predictive for\\n            \`horizon=0\` than for \`horizon=3\`).\\n        *   **Mechanism:** Present in \`test_x\` (values -1, 0, 1, 2, 3) and set to \`0\`\\n            for \`train_x\` (as historical data represents the current week). Treated as\\n            a categorical feature.\\n\\n    Overall, the model's strength comes from combining population-adjusted and\\n    transformed autoregressive signals with rich temporal and geographical features,\\n    trained specifically for quantile prediction. The ensemble nature of LightGBM\\n    allows it to capture complex, non-linear relationships within these features.\\n    Predictions are carefully inverse-transformed and then sorted to ensure\\n    monotonicity across quantiles, which is a requirement for Weighted Interval Score.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters, optimized based on previous trials (Config 1 was best).\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    # Lag difference pairs (lag_current, lag_previous) to compute (e.g., lag_1_wk - lag_2_wk)\\n    LAG_DIFF_PAIRS = config.get('lag_diff_pairs', []) # Default to no lag differences based on previous trials\\n\\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p based on previous trials\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Base value is admissions per million. Handle potential division by zero for population.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    # Add lagged difference features for training data\\n    for l1, l2 in LAG_DIFF_PAIRS:\\n        col_name = f'lag_diff_{l1}wk_{l2}wk'\\n        # Ensure the base lag columns exist before trying to compute the difference\\n        if f'lag_{l1}_wk' in train_features_df.columns and f'lag_{l2}_wk' in train_features_df.columns:\\n            train_features_df[col_name] = train_features_df[f'lag_{l1}_wk'] - train_features_df[f'lag_{l2}_wk']\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'lag_diff_{l1}wk_{l2}wk' for l1, l2 in LAG_DIFF_PAIRS] \\n    \\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (primarily lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values, assuming a baseline of zero admissions for missing history.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Get all feature columns that will be used for training, including the horizon\\n    final_feature_cols_for_model = X_train_model.columns.tolist() \\n\\n    # Combine X and y for training and drop rows where target is NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Drop rows if target is NaN\\n    \\n    X_train_model = train_combined[final_feature_cols_for_model]\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Cast 'location' and 'horizon' to category type for LightGBM for efficient handling\\n    categorical_features_lgbm = [LOCATION_COL] \\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n    if HORIZON_COL in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype('category')\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_specific_features_df = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_specific_features_df[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    # This step assumes a \\"direct\\" multi-step forecasting approach, where all future horizons\\n    # use historical features relative to the forecast origin (max_train_date).\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            historical_transformed_target_values = loc_hist_data[TRANSFORMED_TARGET_COL].values \\n            \\n            # Populate simple lag features\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(historical_transformed_target_values) >= lag:\\n                    # Get value from 'lag' weeks ago relative to the last historical point\\n                    latest_lag_value = historical_transformed_target_values[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = historical_transformed_target_values[-1] if historical_transformed_target_values.size > 0 else 0.0\\n                test_specific_features_df.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Take the last 'window' values from historical data for rolling mean calculation\\n                rolling_data = pd.Series(historical_transformed_target_values).tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = 0.0\\n            \\n            # Populate lagged difference features *after* simple lags are populated for the current location\\n            for l1, l2 in LAG_DIFF_PAIRS:\\n                col_name = f'lag_diff_{l1}wk_{l2}wk'\\n                # Access the just-assigned values for the current location's test indices\\n                lag1_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l1}_wk']\\n                lag2_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l2}_wk']\\n                test_specific_features_df.loc[loc_test_indices, col_name] = lag1_val - lag2_val\\n    \\n    # Merge the computed lag/rolling/diff features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_specific_features_df], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # Ensure test set has all the features and in the same order as training set.\\n    for col in final_feature_cols_for_model:\\n        if col not in X_test_model.columns:\\n            X_test_model[col] = 0.0 # Add missing columns and initialize\\n    X_test_model = X_test_model[final_feature_cols_for_model]\\n\\n    # Re-cast 'location' and 'horizon' in X_test_model to category type with categories from train.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    if HORIZON_COL in X_train_model.columns:\\n        train_horizon_categories = X_train_model[HORIZON_COL].cat.categories\\n        X_test_model[HORIZON_COL] = pd.Categorical(X_test_model[HORIZON_COL], categories=train_horizon_categories)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\nconfig_list = [\\n    { # Default configuration: uses 'log1p' transformation, a standard set of\\n      # lags/rolling windows, and no explicit lag difference features.\\n      # This configuration performed best in the previous trial.\\n    }\\n]",
  "new_index": "271",
  "new_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x using LightGBM for quantile regression.\\n\\n    This function builds a robust forecasting model by leveraging a comprehensive set\\n    of time-series features, including lagged values and rolling statistics of the\\n    population-normalized and transformed target variable. Location and time-based\\n    features are also incorporated. The model is designed for multi-step-ahead\\n    probabilistic forecasting, directly predicting quantiles for various horizons.\\n\\n    Model Explanation and Feature Importance:\\n\\n    The model employs LightGBM, a gradient-boosting framework known for its speed,\\n    efficiency, and ability to handle tabular data, including categorical features\\n    and quantile regression. For each desired quantile (e.g., 0.01, 0.5, 0.99),\\n    a separate LightGBM model is trained using the 'quantile' objective.\\n\\n    Key Features and Their Importance:\\n\\n    1.  **Target Transformation (\`transformed_admissions_per_million\`):**\\n        *   **Purpose:** To normalize the target variable across different state populations\\n            and stabilize its variance. Also serves as the primary mechanism to handle\\n            outliers by compressing their scale.\\n        *   **Mechanism:** The raw \`Total COVID-19 Admissions\` is first normalized by\\n            \`population\` to \`admissions_per_million\`. Then, a \`log1p\` (log(1+x)) transformation\\n            is applied (default). This makes the highly skewed count data more amenable\\n            to modeling by compressing its range and making the distribution more\\n            symmetrical, which helps LightGBM learn more effectively. This transformation\\n            effectively mitigates the impact of extremely high values (outliers)\\n            by bringing them closer to the mean in the transformed space.\\n\\n    2.  **Lagged Target Features (\`lag_X_wk\`):**\\n        *   **Importance:** These are typically the most critical features in time-series\\n            forecasting. Past values of hospital admissions are the strongest\\n            predictors of future admissions due to the autoregressive nature of epidemics.\\n        *   **Mechanism:** Lagged values (e.g., 1, 2, 4, 8, 26, 52 weeks prior) of the\\n            transformed target are included. Short lags capture immediate trends,\\n            while longer lags (e.g., \`lag_52_wk\`) capture strong annual seasonality,\\n            as respiratory illnesses often follow yearly cycles.\\n        *   **For Multi-step Forecasting:** For future horizons in \`test_x\`, these\\n            lags represent the admissions at the *last observed historical point*\\n            (the end of \`train_y\`). The model then learns how these \\"seed\\" values\\n            propagate over different forecast horizons.\\n\\n    3.  **Rolling Mean Features (\`rolling_mean_Y_wk\`):**\\n        *   **Importance:** These features provide a smoothed representation of recent trends,\\n            reducing noise present in single-week lagged values. They help the model\\n            understand the underlying trajectory of the pandemic.\\n        *   **Mechanism:** Rolling means over various windows (e.g., 4, 8, 16 weeks)\\n            of the transformed target are calculated. These averages capture\\n            short-to-medium term smoothed trends and are shifted by one week to\\n            prevent data leakage (only using past information).\\n\\n    4.  **Lagged Difference Features (\`lag_diff_Xwk_Ywk\` - if configured):**\\n        *   **Importance:** While not found to be optimal in the previous trial for this\\n            specific setup, these features are designed to capture the *rate of change*\\n            or acceleration/deceleration of the pandemic. For example, \`lag_diff_1wk_2wk\`\\n            indicates if the current trend is rising or falling week-over-week.\\n        *   **Mechanism:** Differences between various lagged values are computed.\\n\\n    5.  **Date-based Features (\`year\`, \`month\`, \`week_of_year\`, \`sin_week_of_year\`, \`cos_week_of_year\`, \`weeks_since_start\`):**\\n        *   **Importance:** These capture both long-term trends and cyclical seasonality.\\n        *   **Mechanism:**\\n            *   \`year\`, \`weeks_since_start\`: Capture overall progression of the pandemic,\\n                reflecting changes in immunity, vaccine uptake, variant emergence, etc.\\n            *   \`month\`, \`week_of_year\`: Capture monthly/weekly seasonality.\\n            *   \`sin_week_of_year\`, \`cos_week_of_year\`: Cyclical transformations of\\n                \`week_of_year\` ensure that the model understands the continuous nature of\\n                weeks (e.g., week 52 is close to week 1). This is crucial for capturing\\n                seasonal patterns like winter peaks in a smooth manner.\\n\\n    6.  **Location Identifier (\`location\`):**\\n        *   **Importance:** Highly important for capturing state-specific dynamics,\\n            baseline admission rates, public health measures, and population behavior.\\n            Each state may have a unique epidemiological curve.\\n        *   **Mechanism:** Treated as a categorical feature by LightGBM for efficient encoding.\\n\\n    7.  **Horizon Feature (\`horizon\`):**\\n        *   **Importance:** Essential for multi-step forecasting. This feature explicitly\\n            tells the model how many weeks into the future the prediction is for.\\n            The model learns different relationships between features and the target\\n            for different horizons (e.g., current trends are more predictive for\\n            \`horizon=0\` than for \`horizon=3\`).\\n        *   **Mechanism:** Present in \`test_x\` (values -1, 0, 1, 2, 3) and set to \`0\`\\n            for \`train_x\` (as historical data represents the current week). Treated as\\n            a categorical feature.\\n\\n    **Outlier and Anomaly Handling (Explicitly Addressed):**\\n    *   **Target Transformation (\`log1p\`):** The primary defense against extreme values in \`Total COVID-19 Admissions\`. It effectively compresses the range of the target, making the modeling task more stable.\\n    *   **Non-Negative Constraints:** All intermediate calculations (admissions per million) and final predictions are explicitly capped at zero. This handles physically impossible negative predictions.\\n    *   **Population Division Handling:** Division by zero for population is prevented, and resulting NaNs are filled, preventing errors from malformed data.\\n    *   **Missing Historical Features:** Lagged and rolling features that cannot be computed due to insufficient historical data are filled with a default of \`0.0\`. This is a common and reasonable imputation strategy for time-series features where \\"no prior data\\" can be interpreted as \\"no observed activity\\".\\n    *   **Quantile Regression Objective:** LightGBM's quantile objective is inherently more robust to outliers in the target variable compared to a mean-squared error objective, as it focuses on predicting specific quantiles of the distribution rather than the mean, which can be heavily pulled by outliers.\\n    *   **Monotonicity Enforcement:** The final quantile predictions are explicitly sorted for each row, ensuring they are monotonically increasing. This is critical for the Weighted Interval Score and results in valid probabilistic forecasts.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters, optimized based on previous trials (Config 1 was best).\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    # Lag difference pairs (lag_current, lag_previous) to compute (e.g., lag_1_wk - lag_2_wk)\\n    LAG_DIFF_PAIRS = config.get('lag_diff_pairs', []) # Default to no lag differences based on previous trials\\n\\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p based on previous trials\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy() # Explicit copy to avoid SettingWithCopyWarning\\n    df_train_full.loc[:, TARGET_COL] = train_y # Use .loc for explicit assignment\\n    df_train_full.loc[:, DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Base value is admissions per million. Handle potential division by zero for population.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full.loc[:, TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full.loc[:, TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full.loc[:, TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full.loc[:, TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy() # Always work on a copy to prevent SettingWithCopyWarning\\n        df.loc[:, DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df.loc[:, 'year'] = df[DATE_COL].dt.year\\n        df.loc[:, 'month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df.loc[:, 'week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df.loc[:, 'sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df.loc[:, 'cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df.loc[:, 'weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df.loc[:, f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df.loc[:, f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    # Add lagged difference features for training data\\n    for l1, l2 in LAG_DIFF_PAIRS:\\n        col_name = f'lag_diff_{l1}wk_{l2}wk'\\n        # Ensure the base lag columns exist before trying to compute the difference\\n        if f'lag_{l1}_wk' in train_features_df.columns and f'lag_{l2}_wk' in train_features_df.columns:\\n            train_features_df.loc[:, col_name] = train_features_df[f'lag_{l1}_wk'] - train_features_df[f'lag_{l2}_wk']\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'lag_diff_{l1}wk_{l2}wk' for l1, l2 in LAG_DIFF_PAIRS if f'lag_diff_{l1}wk_{l2}wk' in train_features_df.columns]\\n    \\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model.loc[:, HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (primarily lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values, assuming a baseline of zero admissions for missing history.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model.loc[:, col] = X_train_model[col].fillna(0.0)\\n\\n    # Get all feature columns that will be used for training, including the horizon\\n    final_feature_cols_for_model = X_train_model.columns.tolist() \\n\\n    # Combine X and y for training and drop rows where target is NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined.loc[:, TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Drop rows if target is NaN\\n    \\n    X_train_model = train_combined[final_feature_cols_for_model]\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Cast 'location' and 'horizon' to category type for LightGBM for efficient handling\\n    categorical_features_lgbm = [LOCATION_COL] \\n    X_train_model.loc[:, LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n    if HORIZON_COL in X_train_model.columns:\\n        X_train_model.loc[:, HORIZON_COL] = X_train_model[HORIZON_COL].astype('category')\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_specific_features_df = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_specific_features_df.loc[:, col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    # This step assumes a \\"direct\\" multi-step forecasting approach, where all future horizons\\n    # use historical features relative to the forecast origin (max_train_date).\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            historical_transformed_target_values = loc_hist_data[TRANSFORMED_TARGET_COL].values \\n            \\n            # Populate simple lag features\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(historical_transformed_target_values) >= lag:\\n                    # Get value from 'lag' weeks ago relative to the last historical point\\n                    latest_lag_value = historical_transformed_target_values[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = historical_transformed_target_values[-1] if historical_transformed_target_values.size > 0 else 0.0\\n                test_specific_features_df.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Take the last 'window' values from historical data for rolling mean calculation\\n                rolling_data = pd.Series(historical_transformed_target_values).tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = 0.0\\n            \\n            # Populate lagged difference features *after* simple lags are populated for the current location\\n            for l1, l2 in LAG_DIFF_PAIRS:\\n                col_name = f'lag_diff_{l1}wk_{l2}wk'\\n                # Ensure the column exists in the temporary dataframe before trying to assign\\n                if col_name in test_specific_features_df.columns:\\n                    # Access the just-assigned values for the current location's test indices\\n                    lag1_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l1}_wk']\\n                    lag2_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l2}_wk']\\n                    test_specific_features_df.loc[loc_test_indices, col_name] = lag1_val - lag2_val\\n    \\n    # Merge the computed lag/rolling/diff features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_specific_features_df], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model.loc[:, col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # Ensure test set has all the features and in the same order as training set.\\n    for col in final_feature_cols_for_model:\\n        if col not in X_test_model.columns:\\n            X_test_model.loc[:, col] = 0.0 # Add missing columns and initialize\\n    X_test_model = X_test_model[final_feature_cols_for_model]\\n\\n    # Re-cast 'location' and 'horizon' in X_test_model to category type with categories from train.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model.loc[:, LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    if HORIZON_COL in X_train_model.columns:\\n        train_horizon_categories = X_train_model[HORIZON_COL].cat.categories\\n        X_test_model.loc[:, HORIZON_COL] = pd.Categorical(X_test_model[HORIZON_COL], categories=train_horizon_categories)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\nconfig_list = [\\n    { # Default configuration: uses 'log1p' transformation, a standard set of\\n      # lags/rolling windows, and no explicit lag difference features.\\n      # This configuration performed best in the previous trial.\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

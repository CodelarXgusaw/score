<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "408",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy\\n    for the test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version explicitly aims to handle seasonality and trends through comprehensive\\n    feature engineering including cyclical date features, \\"weeks since start\\" for trend,\\n    and various short-to-long term lags and rolling means. It also incorporates\\n    various regularization parameters to prevent overfitting.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    \\n    target_transform_type = config.get('target_transform', 'log1p')\\n\\n    # A small epsilon to ensure non-negative values before transformations like sqrt/power,\\n    # and to handle values that might become zero after division by population.\\n    epsilon = 1e-6 \\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Use np.where to prevent division by zero for population.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0 # If population is 0, admissions per million is 0.\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation. Admissions counts cannot be negative.\\n    admissions_per_million[admissions_per_million < 0] = 0 \\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() \\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This is crucial for capturing yearly seasonality patterns.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        # This helps model long-term trends and overall temporal progression.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group.\\n    # Lags capture auto-correlation and short-to-long term dependencies, implicitly handling trends and seasonality.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target.\\n    # Rolling means capture local trends and smoothed patterns.\\n    for window in ROLLING_WINDOWS:\\n        # Use \`closed='left'\` to ensure window includes data *before* the current date.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in test_x and output, and can influence predictions for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs (where ffill couldn't reach) within each location group.\\n    # This handles missing values for lagged/rolling features at the beginning of each location's time series.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Use \`groupby().ffill()\` for forward fill within each location's time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining NaNs (e.g., at the very start of a group's series where no ffill is possible) with 0.0\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n    \\n    # Drop rows from training data where the target is NaN (should not be many)\\n    # or where features (even after fillna) might still be NaN if there's absolutely no history.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows if any selected feature or target is NaN\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    # This prevents errors if a location appears only in the test data.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    \\n    categorical_features_lgbm = [LOCATION_COL] \\n    \\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    # Horizons represent different forecast steps (short-term future), which might have distinct patterns.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile. This is a common and effective approach for quantile regression.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history is critical for generating lagged and rolling features for future predictions.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions, crucial for matching output format.\\n    original_test_x_index = test_x.index \\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    # Sorting ensures we process dates chronologically for each location, which is vital for time series.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        # If history is too short for a specific lag/window, fall back to the most recent value or 0.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag] \\n            elif current_loc_hist: \\n                lag_value = current_loc_hist[-1] # Use the most recent if not enough for the specific lag\\n            else: \\n                lag_value = 0.0 # No history at all for this location\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: \\n                rolling_mean_val = np.mean(current_loc_hist) # Use what's available\\n            else: \\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        # This is critical for LightGBM to correctly align features and prevent errors.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n        \\n        # Re-cast categorical features with appropriate types for prediction.\\n        # This ensures LightGBM treats them as categorical variables during prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n        \\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            # [0] to extract the single prediction value from the array.\\n            pred_transformed = model.predict(X_test_row)[0] \\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        transformed_preds_array = np.array(list(row_predictions_transformed.values()))\\n        \\n        # Inverse transformation logic. Subtract epsilon if it was added during forward transformation.\\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            inv_preds_admissions_per_million = np.power(transformed_preds_array, 2) - epsilon \\n        elif target_transform_type == 'fourth_root':\\n            inv_preds_admissions_per_million = np.power(transformed_preds_array, 4) - epsilon \\n        else:\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle zero population explicitly: if population is zero, admissions must be zero.\\n        if population_val == 0: \\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000 \\n        \\n        # Ensure all predictions are non-negative. This is a common post-processing step for count data.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int) \\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        # This value will be used for calculating lags/rolling means for subsequent dates for the same location,\\n        # making the forecasting iterative and self-correcting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform the median prediction to admissions per million before re-transforming for history.\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(median_pred_transformed_raw, 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(median_pred_transformed_raw, 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n        \\n        # Ensure non-negative before re-transforming for history and add epsilon if needed.\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        # This step is crucial for maintaining consistency in the feature space for iterative predictions.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n        \\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # This guarantees that higher quantiles always predict values greater than or equal to lower quantiles.\\n    predictions_array = predictions_df.values.astype(float) # Ensure float type for robust sorting\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    # Convert back to DataFrame with original columns and index.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1 (Best from previous trial, 'fourth_root'): Robust baseline.\\n        'lgbm_params': {\\n            'n_estimators': 220,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], \\n        'rolling_windows': [8, 16, 26]      \\n    },\\n    { # Config 2 (Second best from previous trial, 'log1p'): Alternative robust baseline.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 4, 8, 12, 26, 52], \\n        'rolling_windows': [2, 4, 8, 16]         \\n    },\\n    { # Config 3 (New - More Regularized 'fourth_root'): Aim for better generalization.\\n        'lgbm_params': {\\n            'n_estimators': 200,      # Slightly less or similar to best\\n            'learning_rate': 0.03,\\n            'num_leaves': 22,         # Reduced complexity\\n            'max_depth': 4,           # Reduced complexity\\n            'min_child_samples': 30,  # Increased regularization\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly reduced feature sampling\\n            'subsample': 0.75,        # Slightly reduced data sampling\\n            'reg_alpha': 0.2,         # Increased L1 regularization\\n            'reg_lambda': 0.2         # Increased L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    },\\n    { # Config 4 (New - More Regularized 'log1p'): Aim for better generalization with log1p.\\n        'lgbm_params': {\\n            'n_estimators': 180,      # Reduced for more regularization\\n            'learning_rate': 0.03,\\n            'num_leaves': 20,         # Reduced complexity further\\n            'max_depth': 4,           # Reduced complexity\\n            'min_child_samples': 35,  # Increased regularization\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,  # Further reduced feature sampling\\n            'subsample': 0.7,         # Further reduced data sampling\\n            'reg_alpha': 0.3,         # Increased L1 regularization\\n            'reg_lambda': 0.3         # Increased L2 regularization\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 4, 8, 12, 26], # Slightly simpler lag set\\n        'rolling_windows': [2, 4, 8, 12]   # Slightly simpler rolling set\\n    }\\n]",
  "new_index": "457",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any, List, Dict\\n\\n# Define constants for column names and common parameters for better readability and maintainability.\\nQUANTILES = [\\n    0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n    0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n]\\nTARGET_COL = 'Total COVID-19 Admissions'\\nDATE_COL = 'target_end_date'\\nLOCATION_COL = 'location'\\nPOPULATION_COL = 'population'\\nHORIZON_COL = 'horizon'\\nTRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\nEPSILON = 1e-6 # A small epsilon to ensure numerical stability for transformations (e.g., sqrt of 0)\\n\\ndef _add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Adds common date-based and trend features to a DataFrame.\\n    These features capture seasonality (year, month, week of year, cyclical week features)\\n    and overall trend (weeks since the earliest date in the dataset).\\n    \\"\\"\\"\\n    df = df_input.copy()\\n    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n    \\n    df['year'] = df[DATE_COL].dt.year\\n    df['month'] = df[DATE_COL].dt.month\\n    # Use .isocalendar().week for ISO week number (1-53), consistent across years.\\n    df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n    \\n    # Add cyclical features for week of year to capture seasonality smoothly.\\n    # These help the model understand the periodic nature of the data.\\n    df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52.0)\\n    df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52.0)\\n\\n    # Weeks since the start of the entire dataset, to capture long-term trends.\\n    df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n    \\n    return df\\n\\ndef _transform_target(series: pd.Series, pop_series: pd.Series, transform_type: str) -> pd.Series:\\n    \\"\\"\\"\\n    Applies a specified transformation to the target variable (Total COVID-19 Admissions).\\n    First, it normalizes admissions by population (per million people), then applies a chosen\\n    transformation (log1p, sqrt, or fourth_root) to stabilize variance and make the distribution\\n    more Gaussian-like, which often helps models perform better.\\n    Handles potential division by zero for population and ensures non-negativity.\\n    \\"\\"\\"\\n    admissions_per_million = np.where(\\n        pop_series != 0,\\n        series / pop_series * 1_000_000,\\n        0.0 # If population is 0, admissions per million is 0.\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=series.index)\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transformation\\n\\n    if transform_type == 'log1p':\\n        return np.log1p(admissions_per_million)\\n    elif transform_type == 'sqrt':\\n        # Add epsilon to handle zero or near-zero values robustly for sqrt.\\n        return np.sqrt(admissions_per_million + EPSILON) \\n    elif transform_type == 'fourth_root':\\n        # Add epsilon for robustness with fourth_root.\\n        return np.power(admissions_per_million + EPSILON, 0.25) \\n    else: # Fallback to raw (per million) if transform type is unknown or invalid.\\n        return admissions_per_million\\n\\ndef _inverse_transform_target(transformed_preds: np.ndarray, pop_value: float, transform_type: str) -> np.ndarray:\\n    \\"\\"\\"\\n    Applies the inverse transformation to the predicted values, converting them\\n    back from the model's prediction scale to total COVID-19 admissions.\\n    It reverses the population scaling and the chosen transformation, then ensures\\n    predictions are non-negative and rounded to integers as hospital admissions are counts.\\n    \\"\\"\\"\\n    if transform_type == 'log1p':\\n        inv_preds_admissions_per_million = np.expm1(transformed_preds)\\n    elif transform_type == 'sqrt':\\n        # Subtract epsilon to reverse the addition during forward transformation.\\n        inv_preds_admissions_per_million = np.power(transformed_preds, 2) - EPSILON \\n    elif transform_type == 'fourth_root':\\n        # Subtract epsilon to reverse the addition during forward transformation.\\n        inv_preds_admissions_per_million = np.power(transformed_preds, 4) - EPSILON \\n    else:\\n        inv_preds_admissions_per_million = transformed_preds\\n\\n    # Convert from admissions per million back to total admissions using the population value.\\n    if pop_value == 0:\\n        final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million, dtype=float)\\n    else:\\n        final_preds_total_admissions = inv_preds_admissions_per_million * pop_value / 1_000_000\\n    \\n    # Ensure all predictions are non-negative.\\n    final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n    # Round to the nearest integer as admissions are discrete counts.\\n    return np.round(final_preds_total_admissions).astype(int)\\n\\ndef _get_value_to_add_to_history(median_pred_transformed_raw: float, transform_type: str) -> float:\\n    \\"\\"\\"\\n    Calculates the median prediction, transforms it back to admissions per million,\\n    and then re-transforms it to the model's feature scale. This value is then\\n    added to the historical data for iterative feature generation in subsequent predictions.\\n    This ensures consistency in the feature space during sequential forecasting.\\n    \\"\\"\\"\\n    if transform_type == 'log1p':\\n        median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n    elif transform_type == 'sqrt':\\n        median_pred_admissions_per_million = np.power(median_pred_transformed_raw, 2) - EPSILON\\n    elif transform_type == 'fourth_root':\\n        median_pred_admissions_per_million = np.power(median_pred_transformed_raw, 4) - EPSILON\\n    else:\\n        median_pred_admissions_per_million = median_pred_transformed_raw\\n    \\n    # Ensure non-negative before re-transforming for history.\\n    median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n    # Re-transform this median value back to the feature scale for use in future lags.\\n    if transform_type == 'log1p':\\n        return np.log1p(median_pred_admissions_per_million)\\n    elif transform_type == 'sqrt':\\n        return np.sqrt(median_pred_admissions_per_million + EPSILON)\\n    elif transform_type == 'fourth_root':\\n        return np.power(median_pred_admissions_per_million + EPSILON, 0.25)\\n    else:\\n        return median_pred_admissions_per_million\\n\\ndef _fit_and_predict_single_model_config(\\n    df_train_full: pd.DataFrame,\\n    test_x_base_features: pd.DataFrame,\\n    model_component_config: dict[str, Any],\\n    min_date_global: pd.Timestamp\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Trains a set of LightGBM models (one for each quantile) based on a single\\n    configuration (lgbm_params, target_transform, lag_weeks, rolling_windows)\\n    and generates iterative predictions for the test set. This function encapsulates\\n    the core logic for a single model within the ensemble.\\n    \\"\\"\\"\\n    \\n    # Extract configuration for this specific model component\\n    lgbm_params = model_component_config.get('lgbm_params', {})\\n    lag_weeks = model_component_config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    rolling_windows = model_component_config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = model_component_config.get('target_transform', 'log1p')\\n\\n    # Create a copy of the full training data to apply component-specific transformations\\n    df_train_full_component = df_train_full.copy()\\n    df_train_full_component[TRANSFORMED_TARGET_COL] = _transform_target(\\n        df_train_full_component[TARGET_COL],\\n        df_train_full_component[POPULATION_COL],\\n        target_transform_type\\n    )\\n\\n    # --- Feature Engineering for Training Data ---\\n    train_features_df = df_train_full_component.copy()\\n    \\n    # Define base features that don't depend on historical target values\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # Generate lagged features for training data. These are crucial for time-series modeling.\\n    for lag in lag_weeks:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features for training data. These capture smoothed trends.\\n    for window in rolling_windows:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all target-derived features\\n    train_specific_features = [f'lag_{lag}_wk' for lag in lag_weeks] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in rolling_windows] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add 'horizon' feature to training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle missing values for lagged/rolling features (e.g., at the beginning of each location's series)\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Forward fill within each location group\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining NaNs (at the very start of a group where ffill couldn't reach) with 0.0\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n    \\n    # Combine features and target to drop rows with any NaNs (where no features could be generated)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) \\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Categorical Feature Handling for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories for \`CategoricalDtype\`.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_base_features[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    \\n    categorical_features_lgbm = [LOCATION_COL] \\n    \\n    # Process 'horizon' as categorical feature, ensuring all horizons in test data are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_base_features[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n    # --- Model Training ---\\n    models = {}\\n    default_lgbm_params = { # Baseline LGBM parameters\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42, # Default random state, can be overridden by config\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override default parameters with component-specific parameters provided in the config.\\n    current_lgbm_params = {**default_lgbm_params, **lgbm_params}\\n\\n    # Train a separate LightGBM model for each quantile.\\n    for q in QUANTILES:\\n        model_params = current_lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history is essential for generating lagged and rolling features for future predictions.\\n    location_history_data = df_train_full_component.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x_base_features.index \\n\\n    # Prepare test data for sequential processing, preserving original index and sorting.\\n    test_x_processed_for_iter = test_x_base_features.copy()\\n    test_x_processed_for_iter['original_index'] = test_x_processed_for_iter.index\\n    test_x_processed_for_iter[DATE_COL] = pd.to_datetime(test_x_processed_for_iter[DATE_COL])\\n    test_x_processed_for_iter = test_x_processed_for_iter.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize DataFrame to store predictions for this specific model component.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test data to predict sequentially.\\n    for idx, row in test_x_processed_for_iter.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Dynamically generate lag and rolling features using \`current_loc_hist\`.\\n        # If history is too short for a specific lag/window, fall back to the most recent value or 0.\\n        for lag in lag_weeks:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # If history exists but is shorter than lag, use the most recent available value.\\n                lag_value = current_loc_hist[-1]\\n            else: # No history at all for this location (e.g., new location in test set).\\n                lag_value = 0.0 \\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for window in rolling_windows:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # If history exists but is shorter than window, use mean of all available.\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert the feature dictionary to a DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n        # Ensure column order and presence matches training data (critical for LightGBM).\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n        \\n        # Re-cast categorical features with appropriate types for consistent prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n        \\n        # Make predictions for all quantiles for this single test row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract the single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions back to original scale (total admissions).\\n        transformed_preds_array = np.array(list(row_predictions_transformed.values()))\\n        \\n        final_preds_total_admissions = _inverse_transform_target(\\n            transformed_preds_array,\\n            row[POPULATION_COL],\\n            target_transform_type\\n        )\\n        \\n        # Store predictions in the DataFrame using the original test_x index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction.\\n        # This value (transformed to the feature scale) will be used for calculating\\n        # lags/rolling means for subsequent dates for the same location.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        value_to_add_to_history = _get_value_to_add_to_history(median_pred_transformed_raw, target_transform_type)\\n        \\n        # Append the newly predicted median value to the history list.\\n        location_history_data[current_loc].append(value_to_add_to_history)\\n        \\n    return predictions_df\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Main function to generate probabilistic forecasts for COVID-19 hospital admissions.\\n    It implements an ensemble approach by combining predictions from multiple LightGBM\\n    models. Each individual model in the ensemble is defined by a configuration dictionary\\n    within the 'ensemble_components' list in the main \`config\` dictionary.\\n    \\"\\"\\"\\n    \\n    # 1. Initial Data Preparation (common for all ensemble components)\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort the training data by location and date for correct lagged/rolling feature calculation.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Determine the global minimum date from the training set. This anchors the 'weeks_since_start' feature.\\n    min_date_global = df_train_full[DATE_COL].min() \\n\\n    # Add base features (date components, trend) to both train and test dataframes once.\\n    df_train_full = _add_base_features(df_train_full, min_date_global)\\n    test_x_base_features = _add_base_features(test_x.copy(), min_date_global)\\n\\n    # 2. Process Ensemble Components\\n    # The 'ensemble_components' list in the config defines each individual model to be trained.\\n    ensemble_components = config.get('ensemble_components', [])\\n    \\n    # Fallback to a default single model configuration if no ensemble components are specified.\\n    if not ensemble_components:\\n        print(\\"Warning: No 'ensemble_components' specified in config. Falling back to default single model configuration.\\")\\n        ensemble_components = [{\\n            'name': 'Default Single Model (fourth_root)',\\n            'lgbm_params': {}, # Use default lgbm params from _fit_and_predict_single_model_config\\n            'target_transform': 'fourth_root',\\n            'lag_weeks': [1, 4, 8, 16, 26, 52], \\n            'rolling_windows': [8, 16, 26]      \\n        }]\\n\\n    all_predictions_dfs = []\\n    # Iterate through each model configuration in the ensemble.\\n    for i, component_config in enumerate(ensemble_components):\\n        print(f\\"Processing ensemble component {i+1}: {component_config.get('name', 'Unnamed Component')}\\")\\n        # Train and predict with the current model component.\\n        preds_df_component = _fit_and_predict_single_model_config(\\n            df_train_full,\\n            test_x_base_features,\\n            component_config,\\n            min_date_global\\n        )\\n        all_predictions_dfs.append(preds_df_component)\\n    \\n    # 3. Combine Predictions from all ensemble components\\n    if not all_predictions_dfs:\\n        # Return an empty DataFrame if no predictions were generated (e.g., empty components list).\\n        return pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES], dtype=int)\\n\\n    # For a simple ensemble, average the predictions across all components.\\n    # This involves stacking the DataFrames vertically and then grouping by the original index to take the mean.\\n    combined_predictions_df = pd.concat(all_predictions_dfs).groupby(level=0).mean()\\n\\n    # 4. Final Post-processing: Ensure non-negativity and integer counts\\n    # After averaging, predictions are floats. Round them and ensure they are non-negative integers.\\n    for col in combined_predictions_df.columns:\\n        combined_predictions_df[col] = combined_predictions_df[col].apply(lambda x: max(0, int(round(x))))\\n\\n    # 5. Ensure Monotonicity of Quantiles\\n    # This is a crucial step for valid quantile forecasts. It ensures that higher quantiles\\n    # always predict values greater than or equal to lower quantiles for each row.\\n    predictions_array = combined_predictions_df.values.astype(float) # Convert to float for robust sorting\\n    predictions_array.sort(axis=1) # Sorts each row in-place, ensuring monotonicity\\n    \\n    # Convert the sorted array back to a DataFrame with original columns and index, and enforce integer type.\\n    final_predictions_df = pd.DataFrame(predictions_array, columns=combined_predictions_df.columns, index=combined_predictions_df.index).astype(int)\\n\\n    return final_predictions_df\\n\\n# These \`config_list\` entries will be scored by the evaluation harness.\\n# Each dictionary represents a different ensemble strategy or set of hyperparameters.\\n# The 'ensemble_components' key within each config defines the individual LightGBM\\n# models that will be trained and averaged internally by \`fit_and_predict_fn\`.\\nconfig_list = [\\n    {   # Ensemble Config 1: A robust ensemble combining a 'fourth_root' model (similar to previous best)\\n        # and a 'log1p' model, with extended feature sets for potentially better capture of complex patterns.\\n        'ensemble_components': [\\n            {\\n                'name': 'Component_1_FourthRoot_ExtendedFeatures',\\n                'lgbm_params': {\\n                    'n_estimators': 220, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5, \\n                    'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n                },\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 2, 3, 4, 8, 12, 16, 26, 52], # Comprehensive lag set\\n                'rolling_windows': [2, 4, 8, 16, 26]      # Comprehensive rolling windows\\n            },\\n            {\\n                'name': 'Component_2_Log1p_StandardFeatures',\\n                'lgbm_params': {\\n                    'n_estimators': 200, 'learning_rate': 0.03, 'num_leaves': 25, 'max_depth': 5, \\n                    'min_child_samples': 20, 'random_state': 43, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n                },\\n                'target_transform': 'log1p',\\n                'lag_weeks': [1, 2, 4, 8, 12, 26, 52], \\n                'rolling_windows': [2, 4, 8, 16]         \\n            }\\n        ]\\n    },\\n    {   # Ensemble Config 2: A more regularized ensemble with components that use slightly\\n        # simpler feature sets and stricter LGBM regularization to potentially improve generalization.\\n        'ensemble_components': [\\n            {\\n                'name': 'Component_3_FourthRoot_Regularized',\\n                'lgbm_params': {\\n                    'n_estimators': 180, 'learning_rate': 0.03, 'num_leaves': 22, 'max_depth': 4, \\n                    'min_child_samples': 30, 'random_state': 44, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.75, 'subsample': 0.75, 'reg_alpha': 0.2, 'reg_lambda': 0.2\\n                },\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 4, 8, 16, 26], # Fewer lags\\n                'rolling_windows': [8, 16]      # Fewer rolling windows\\n            },\\n            {\\n                'name': 'Component_4_Log1p_Regularized',\\n                'lgbm_params': {\\n                    'n_estimators': 160, 'learning_rate': 0.03, 'num_leaves': 20, 'max_depth': 4, \\n                    'min_child_samples': 35, 'random_state': 45, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.7, 'subsample': 0.7, 'reg_alpha': 0.3, 'reg_lambda': 0.3\\n                },\\n                'target_transform': 'log1p',\\n                'lag_weeks': [1, 2, 4, 8, 12], \\n                'rolling_windows': [2, 4, 8]         \\n            }\\n        ]\\n    },\\n    {   # Ensemble Config 3: Focuses on different 'fourth_root' configurations to explore\\n        # the robustness of this transformation with slightly varied LGBM parameters and feature sets.\\n        'ensemble_components': [\\n            {\\n                'name': 'Component_5_FourthRoot_BestPreviousParams',\\n                'lgbm_params': {\\n                    'n_estimators': 220, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5, \\n                    'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n                },\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 4, 8, 16, 26, 52], \\n                'rolling_windows': [8, 16, 26]      \\n            },\\n            {\\n                'name': 'Component_6_FourthRoot_SlightlyTuned',\\n                'lgbm_params': {\\n                    'n_estimators': 200, 'learning_rate': 0.035, 'num_leaves': 28, 'max_depth': 6, \\n                    'min_child_samples': 18, 'random_state': 46, 'n_jobs': -1, 'verbose': -1, \\n                    'colsample_bytree': 0.85, 'subsample': 0.85, 'reg_alpha': 0.05, 'reg_lambda': 0.05\\n                },\\n                'target_transform': 'fourth_root',\\n                'lag_weeks': [1, 2, 4, 8, 16, 26, 52], \\n                'rolling_windows': [4, 8, 16, 26]         \\n            }\\n        ]\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

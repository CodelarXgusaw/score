<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "770",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    To address outliers and anomalies, this version explicitly adds rolling median features,\\n    which are more robust to extreme values than rolling means.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n    # Added for robustness to outliers: Rolling Median Windows\\n    ROLLING_MEDIAN_WINDOWS = config.get('rolling_median_windows', ROLLING_WINDOWS) # Default to same as rolling mean\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of LightGBM models per quantile (for internal ensemble/robustness)\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default in case of empty training data or other issues.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    \\n    # Robustly handle date column: convert to datetime and drop rows where date is invalid/missing.\\n    # 'coerce' will turn unparseable dates into NaT.\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL], errors='coerce')\\n    df_train_full.dropna(subset=[DATE_COL], inplace=True)\\n\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Add horizon to training data (historical data implies horizon 0 relative to itself)\\n    if HORIZON_COL not in df_train_full.columns:\\n        df_train_full[HORIZON_COL] = 0\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    # Ensure population is not zero or NaN to prevent division by zero/NaNs.\\n    # A small positive epsilon ensures numerical stability for 'per million' calculations.\\n    POPULATION_EPSILON = 1.0 # Using 1.0 as the baseline for 'per million' scale.\\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON)) # Ensure minimum positive value\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative target\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before log1p\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        # Ensure DATE_COL is datetime type and handle potential NaTs from previous steps\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\\n        \\n        # Temporarily fill NaT dates with a placeholder to allow feature extraction,\\n        # then fill the derived NaN features. This handles cases where original DATE_COL might be NaT.\\n        df_temp_date = df[DATE_COL].fillna(min_date) # Use a valid date for feature extraction\\n        \\n        df['year'] = df_temp_date.dt.year\\n        df['month'] = df_temp_date.dt.month\\n        df['week_of_year'] = df_temp_date.dt.isocalendar().week.astype(int) # isocalendar handles week number correctly\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df_temp_date - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n        \\n        # Impute any NaNs in newly created numerical date-features (e.g., if original date was NaT)\\n        for col in ['year', 'month', 'week_of_year', 'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start', 'weeks_since_start_sq']:\\n            if col in df.columns and df[col].isnull().any():\\n                df[col] = df[col].fillna(0) # Filling with 0 as a simple placeholder for missing date features\\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    \\n    # Process test_x dates similarly\\n    test_x_processed = test_x.copy()\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL], errors='coerce')\\n    test_x_processed.dropna(subset=[DATE_COL], inplace=True) # Drop test rows with invalid/missing dates\\n    \\n    # If test_x_processed became empty after dropping NaNs for invalid dates, return zero predictions.\\n    if test_x_processed.empty:\\n        print(\\"WARNING: Test data became empty after date processing. Returning default predictions (zeros).\\")\\n        return predictions_df\\n    \\n    test_x_processed = add_base_features(test_x_processed, min_date_global)\\n\\n\\n    # Base features include population, temporal components, and horizon.\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL] \\n    \\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=2, closed='left').std() # min_periods=2 for std\\n            ).fillna(0) # Fill NaNs (e.g. from min_periods=2) with 0\\n\\n    # New: Generate rolling median features, shifted by 1 to avoid data leakage. (Addressing outliers)\\n    for window in ROLLING_MEDIAN_WINDOWS:\\n        train_features_df[f'rolling_median_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').median()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS] + \\\\\\n                              [f'rolling_median_{window}_wk' for window in ROLLING_MEDIAN_WINDOWS] # Added rolling median\\n\\n    # Combine all features for the model\\n    X_train_model_cols_final = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_final].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target or essential (lagged) features are NaN *after* filling.\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n    if train_specific_features: \\n        train_combined.dropna(subset=[f for f in train_specific_features if f in train_combined.columns], inplace=True)\\n\\n    # Check if training data is empty after dropping NaNs.\\n    if train_combined.empty:\\n        print(\\"WARNING: Training data became empty after feature engineering and NaN dropping. Returning default predictions (zeros).\\")\\n        return predictions_df \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed.loc[:, LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q \\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i \\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            \\n            if not X_train_model.empty:\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be updated with median predictions for future steps.\\n    # Group by location and convert to lists for efficient appending\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL] \\n        original_idx = row.loc['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES if col in row.index} \\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                lag_value = 0.0\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1:\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # New: Generate dynamic rolling median features (Addressing outliers)\\n        for window in ROLLING_MEDIAN_WINDOWS:\\n            rolling_col_name = f'rolling_median_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_median_val = np.median(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_median_val = np.median(current_loc_hist)\\n            else:\\n                rolling_median_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_median_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using LGBM models.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES: \\n            lgbm_preds_for_q = []\\n            if models[q]:\\n                for lgbm_model_q in models[q]:\\n                    lgbm_preds_for_q.append(lgbm_model_q.predict(X_test_row)[0])\\n                row_predictions_transformed[q] = np.mean(lgbm_preds_for_q)\\n            else:\\n                row_predictions_transformed[q] = 0.0 \\n\\n        # Extract predictions as an array to apply transformations\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row.loc[POPULATION_COL] \\n        safe_population_test = POPULATION_EPSILON \\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.maximum(0.0, median_pred_admissions_per_million)\\n\\n        # Convert the median prediction back to the 'forward_transform' scale before adding to history.\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity and integer type\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline from previous best, good balance. (Now with rolling median default windows)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8, 52],\\n        'rolling_windows': [8, 16, 26], # These will also be used for rolling median by default\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1, \\n    },\\n    { # Config 2: Stronger regularization - shallower trees, more samples per leaf, higher L1/L2. (Now with rolling median default windows)\\n        'lgbm_params': {\\n            'n_estimators': 300, \\n            'learning_rate': 0.025, \\n            'num_leaves': 22, \\n            'max_depth': 4, \\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, \\n            'subsample': 0.75, \\n            'reg_alpha': 0.2, \\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 26, 52], \\n        'lag_diff_periods': [1, 4, 8, 52],\\n        'rolling_windows': [8, 26], # These will also be used for rolling median by default\\n        'rolling_std_windows': [8], \\n        'n_lgbm_ensemble_members': 1,\\n    },\\n    { # Config 3: More aggressive subsampling and L1/L2 regularization. (Now with rolling median default windows)\\n        'lgbm_params': {\\n            'n_estimators': 280, \\n            'learning_rate': 0.028, \\n            'num_leaves': 24, \\n            'max_depth': 5,\\n            'min_child_samples': 25, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7, \\n            'subsample': 0.7, \\n            'reg_alpha': 0.25, \\n            'reg_lambda': 0.25\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 26, 52], \\n        'lag_diff_periods': [1, 2, 4, 8, 52],\\n        'rolling_windows': [4, 8, 16], # These will also be used for rolling median by default\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n    },\\n    { # Config 4: A slightly different set of rolling windows for median specifically to see impact.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8, 52],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'rolling_median_windows': [4, 8, 12, 16], # Different set of windows for median\\n        'n_lgbm_ensemble_members': 1, \\n    }\\n]",
  "new_index": "801",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    To address outliers and anomalies, this version explicitly adds rolling median features,\\n    which are more robust to extreme values than rolling means.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n    ROLLING_MEDIAN_WINDOWS = config.get('rolling_median_windows', ROLLING_WINDOWS)\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of LightGBM models per quantile (for internal ensemble/robustness)\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default in case of empty training data or other issues.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    \\n    # Robustly handle date column: convert to datetime and drop rows where date is invalid/missing.\\n    # 'coerce' will turn unparseable dates into NaT.\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL], errors='coerce')\\n    df_train_full.dropna(subset=[DATE_COL], inplace=True)\\n\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Add horizon to training data (historical data implies horizon 0 relative to itself)\\n    if HORIZON_COL not in df_train_full.columns:\\n        df_train_full[HORIZON_COL] = 0\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    # Ensure population is not zero or NaN to prevent division by zero/NaNs.\\n    # A small positive epsilon ensures numerical stability for 'per million' calculations.\\n    POPULATION_EPSILON = 1.0 # Using 1.0 as the baseline for 'per million' scale.\\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON)) # Ensure minimum positive value\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative target\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before log1p\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        # Ensure DATE_COL is datetime type and handle potential NaTs from previous steps\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\\n        \\n        # Temporarily fill NaT dates with a placeholder to allow feature extraction,\\n        # then fill the derived NaN features. This handles cases where original DATE_COL might be NaT.\\n        df_temp_date = df[DATE_COL].fillna(min_date) # Use a valid date for feature extraction\\n        \\n        df['year'] = df_temp_date.dt.year\\n        df['month'] = df_temp_date.dt.month\\n        df['week_of_year'] = df_temp_date.dt.isocalendar().week.astype(int) # isocalendar handles week number correctly\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df_temp_date - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n        \\n        # Impute any NaNs in newly created numerical date-features (e.g., if original date was NaT)\\n        for col in ['year', 'month', 'week_of_year', 'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start', 'weeks_since_start_sq']:\\n            if col in df.columns and df[col].isnull().any():\\n                df[col] = df[col].fillna(0) # Filling with 0 as a simple placeholder for missing date features\\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    \\n    # Process test_x dates similarly\\n    test_x_processed = test_x.copy()\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL], errors='coerce')\\n    test_x_processed.dropna(subset=[DATE_COL], inplace=True) # Drop test rows with invalid/missing dates\\n    \\n    # If test_x_processed became empty after dropping NaNs for invalid dates, return zero predictions.\\n    if test_x_processed.empty:\\n        print(\\"WARNING: Test data became empty after date processing. Returning default predictions (zeros).\\")\\n        return predictions_df\\n    \\n    test_x_processed = add_base_features(test_x_processed, min_date_global)\\n\\n\\n    # Base features include population, temporal components, and horizon.\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL] \\n    \\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=2, closed='left').std() # min_periods=2 for std\\n            ).fillna(0) # Fill NaNs (e.g. from min_periods=2) with 0\\n\\n    # New: Generate rolling median features, shifted by 1 to avoid data leakage. (Addressing outliers)\\n    for window in ROLLING_MEDIAN_WINDOWS:\\n        train_features_df[f'rolling_median_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').median()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS] + \\\\\\n                              [f'rolling_median_{window}_wk' for window in ROLLING_MEDIAN_WINDOWS] # Added rolling median\\n\\n    # Combine all features for the model\\n    X_train_model_cols_final = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_final].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    for col in train_specific_features:\\n        if col in X_train_model.columns and X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target or essential (lagged) features are NaN *after* filling.\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n    if train_specific_features: \\n        # Only drop if the feature actually exists in the dataframe\\n        features_to_check = [f for f in train_specific_features if f in train_combined.columns]\\n        if features_to_check: # Only apply if there are actual specific features to check\\n            train_combined.dropna(subset=features_to_check, inplace=True)\\n\\n    # Check if training data is empty after dropping NaNs.\\n    if train_combined.empty:\\n        print(\\"WARNING: Training data became empty after feature engineering and NaN dropping. Returning default predictions (zeros).\\")\\n        return predictions_df \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed.loc[:, LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q \\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i \\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            \\n            if not X_train_model.empty:\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q].append(lgbm_model)\\n            else:\\n                print(f\\"WARNING: X_train_model is empty for quantile {q}. No model trained.\\")\\n\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be updated with median predictions for future steps.\\n    # Group by location and convert to lists for efficient appending\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL] # Use label access for clarity\\n        original_idx = row['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES if col in row.index} \\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # If history is shorter than lag but not empty, use the most recent\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                lag_value = 0.0 # Default if no history\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # If history is too short for diff_period but has at least two points\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # std needs at least 2 points\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # New: Generate dynamic rolling median features (Addressing outliers)\\n        for window in ROLLING_MEDIAN_WINDOWS:\\n            rolling_col_name = f'rolling_median_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_median_val = np.median(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_median_val = np.median(current_loc_hist)\\n            else:\\n                rolling_median_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_median_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using LGBM models.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES: \\n            lgbm_preds_for_q = []\\n            if models[q]: # Check if a model was trained for this quantile\\n                for lgbm_model_q in models[q]:\\n                    lgbm_preds_for_q.append(lgbm_model_q.predict(X_test_row)[0])\\n                row_predictions_transformed[q] = np.mean(lgbm_preds_for_q)\\n            else:\\n                # Fallback to a default (e.g., median of training target) if no model was trained for this quantile\\n                row_predictions_transformed[q] = y_train_model.median() if not y_train_model.empty else 0.0 \\n\\n        # Extract predictions as an array to apply transformations\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL] \\n        safe_population_test = POPULATION_EPSILON \\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # We need to ensure that the median prediction used to update history is compatible with the \`forward_transform\` function.\\n        # It should be in the 'admissions_per_million' scale first, then re-transformed for history.\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.maximum(0.0, median_pred_admissions_per_million)\\n\\n        # Convert the median prediction back to the 'forward_transform' scale before adding to history.\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity and integer type\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Previous best parameters with ensemble of 3 models\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.025,\\n            'num_leaves': 22,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 26, 52],\\n        'lag_diff_periods': [1, 4, 8, 52],\\n        'rolling_windows': [8, 26],\\n        'rolling_std_windows': [8],\\n        'n_lgbm_ensemble_members': 3, # Increased ensemble size\\n    },\\n    { # Config 2: Slightly more estimators, larger leaves, and slightly less regularization, ensemble of 3\\n        'lgbm_params': {\\n            'n_estimators': 350,\\n            'learning_rate': 0.02,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.05, # Less regularization\\n            'reg_lambda': 0.05\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # More lags\\n        'lag_diff_periods': [1, 2, 4, 8, 16, 52], # More diff periods\\n        'rolling_windows': [4, 8, 16, 26], # More rolling windows\\n        'rolling_std_windows': [4, 8, 16],\\n        'rolling_median_windows': [4, 8, 16, 26], # Matching rolling windows for median\\n        'n_lgbm_ensemble_members': 3, # Increased ensemble size\\n    },\\n    { # Config 3: Stronger regularization, smaller leaves, higher min_child_samples, ensemble of 2\\n        'lgbm_params': {\\n            'n_estimators': 280,\\n            'learning_rate': 0.03,\\n            'num_leaves': 20,\\n            'max_depth': 4,\\n            'min_child_samples': 40,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.3, # Stronger regularization\\n            'reg_lambda': 0.3\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 26, 52],\\n        'lag_diff_periods': [1, 4, 8, 52],\\n        'rolling_windows': [8, 16],\\n        'rolling_std_windows': [8],\\n        'rolling_median_windows': [8, 16],\\n        'n_lgbm_ensemble_members': 2, # Smaller ensemble size\\n    },\\n    { # Config 4: A slightly different set of rolling windows for median specifically to see impact, ensemble of 1 for comparison\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8, 52],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'rolling_median_windows': [4, 8, 12, 16], # Different set of windows for median\\n        'n_lgbm_ensemble_members': 1, # Back to single model for this config\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

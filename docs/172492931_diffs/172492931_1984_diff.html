<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1935",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y using LightGBM.\\n\\n    This version uses a single LightGBM model for quantile regression, simplifying the approach\\n    compared to an ensemble. It incorporates robust time-series features (lagged target values,\\n    rolling statistics), a population-normalized and transformed target variable, and location\\n    information. The prediction for the test set is performed iteratively, where median predictions\\n    from previous steps are fed back into the feature generation for future steps, simulating\\n    observed data.\\n\\n    Key features for robustness and performance:\\n    - **Target Transformation:** Applies a 'fourth_root' transformation (configurable) to normalize\\n      the target variable and handle its skewed distribution, making it more amenable to modeling.\\n    - **Population Normalization:** Normalizes admissions by population to create a 'per million'\\n      rate, reducing variance across locations with different population sizes.\\n    - **Time-Series Features:** Generates various lagged values and rolling statistics (mean, std)\\n      of the transformed target, crucial for capturing temporal dependencies and trends.\\n    - **Iterative Prediction:** For multi-step-ahead forecasting, predictions for earlier horizons\\n      are used as \\"observed\\" values to generate features for later horizons.\\n    - **Quantile Regression with LightGBM:** Utilizes LightGBM's native support for quantile\\n      regression by training a separate model for each required quantile.\\n    - **Robust Pre/Post-processing:**\\n        - Handles missing population data and zero populations gracefully.\\n        - Clips transformed and inverse-transformed target values to prevent extreme predictions.\\n        - Employs forward/backward fill and fallback constants for missing time-series features.\\n        - Ensures predicted quantiles are non-negative, monotonically increasing, and maintain\\n          a minimum spread to satisfy submission requirements and improve interpretability.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features (e.g., 'target_end_date', 'location_name', 'location', 'population').\\n        train_y (pd.Series): Training target values ('Total COVID-19 Admissions').\\n        test_x (pd.DataFrame): Test features for future time periods, similar to train_x.\\n        config (dict[str, Any]): Configuration parameters for the model, including LightGBM\\n                                 hyperparameters, feature engineering parameters, and transformation type.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    # Max admissions per million cap to prevent extremely large or unstable target values.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1)) # Must be at least 1 for integer counts\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Determine median population from train data for robust imputation across both train and test.\\n    median_population_train_initial = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train_initial) or median_population_train_initial == 0:\\n        median_population_train_initial = 1.0 # Absolute fallback for median if it's NaN/Inf/0\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        def inverse_transform(x):\\n            # Ensure input to expm1 is non-negative and output is non-negative\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure input to log1p is non-negative\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to sqrt is non-negative\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n\\n    # --- 2. Function to add common date-based features and handle population ---\\n    min_date_global = df_train_full[DATE_COL].min() # Global min date for 'weeks_since_start' consistency\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp, median_pop_fallback: float) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add horizon feature directly, as it's specific to test_x but can be used in train_x (as 0)\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data if not explicitly provided\\n        \\n        # Ensure population is float for all base features and handle potential NaNs/zeros\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_pop_fallback).astype(float)\\n        # Replace any zeros with 1.0 to prevent division by zero in admissions_per_million calculation\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, 1.0, df[POPULATION_COL])\\n        \\n        return df\\n\\n    # Apply base feature engineering to training data\\n    df_train_full = add_base_features(df_train_full, min_date_global, median_population_train_initial)\\n\\n    # Calculate admissions per million and transform for training data\\n    admissions_per_million_train = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000\\n    admissions_per_million_train = np.clip(admissions_per_million_train, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million_train)\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Prepare test_x\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global, median_population_train_initial)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Pre-calculate fallback for missing history. This will be median of the *transformed* target.\\n    median_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(median_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        median_transformed_train_y_fallback = forward_transform(1.0) # Corresponds to 1 admission per million after inverse transform\\n    median_transformed_train_y_fallback = np.clip(median_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Use .diff() then .shift(1) to make it a lag of the difference\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        # closed='left' means the current row is not included in the rolling window calculation, which is correct for forecasting\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    # Fill NaNs from lags/rolling with forward-fill, then back-fill, and finally a constant fallback.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # First, try ffill/bfill within groups\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # If still NaNs (e.g., entire group was NaN, or first elements of a group were NaN),\\n            # fill with specific fallbacks\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else median_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    # This also effectively aligns X_train_model and y_train_model by dropping rows where features are not complete.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Ensures we train only on rows with valid target\\n    # Also drop NaNs from features here just in case, although previous steps should handle most.\\n    train_combined.dropna(subset=X_train_model.columns.tolist(), inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n    # Handle categorical features for LightGBM.\\n    # Collect all unique locations from both train and test to ensure consistent categorical encoding.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: None for q in QUANTILES} # Will store one LGBM model per quantile\\n\\n    for q in QUANTILES:\\n        lgbm_model_params_i = lgbm_params.copy()\\n        lgbm_model_params_i['alpha'] = q # Set quantile for this model\\n        # Use a consistent random state or vary slightly if you want a tiny ensemble\\n        # lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + int(q*100) # Could vary for slight diversity\\n\\n        lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n        try:\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q] = lgbm_model\\n        except Exception as e:\\n            print(f\\"LGBM training failed for Q={q}: {e}\\")\\n            # Model will remain None if training failed\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with training data's transformed target values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Populate current features dictionary, population is already cleaned by add_base_features\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n\\n        # Generate time-series features for the current test row using available history\\n        if not current_loc_hist:\\n            # If no history, use fallbacks for all time-series features\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = median_transformed_train_y_fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = median_transformed_train_y_fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0\\n        else:\\n            # Generate features based on available history\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[len(current_loc_hist) - lag] # Use explicit indexing for robustness\\n                else:\\n                    lag_value = median_transformed_train_y_fallback\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                # Need at least diff_period + 1 values for a diff. E.g., for diff_period=1, need 2 values (last, and last-1)\\n                if len(current_loc_hist) >= diff_period + 1:\\n                    diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (diff_period + 1)]\\n                elif len(current_loc_hist) >= 2: # Fallback to 1-period diff if longer diff not possible\\n                    diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - 2]\\n                else:\\n                    diff_value = 0.0\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_mean_val = median_transformed_train_y_fallback\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1: # Need at least 2 points for std dev\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # Fill new columns with 0.0, though they should be covered by X_train_model_cols list.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            lgbm_model_q = models.get(q)\\n            if lgbm_model_q:\\n                try:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                        row_predictions_transformed[q] = pred_clipped\\n                    else:\\n                        row_predictions_transformed[q] = median_transformed_train_y_fallback\\n                except Exception as e:\\n                    # print(f\\"LGBM prediction failed for Q={q}, loc={current_loc}: {e}\\") # For debugging\\n                    row_predictions_transformed[q] = median_transformed_train_y_fallback\\n            else:\\n                # Fallback if model was not trained successfully\\n                row_predictions_transformed[q] = median_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        # Ensure transformed predictions are within valid bounds before inverse transform\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Ensure inverse transformed predictions are also clipped before converting to total admissions\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL]\\n        # Calculate final total admissions, ensuring no division by zero (population already handled)\\n        final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round and convert to integer, ensuring non-negativity\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the current location with the median prediction (transformed)\\n        # This simulates observing the median prediction as if it were actual data for future lags.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        value_to_add_to_history = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Only add finite values to history to prevent propagation of NaN/Inf\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            # If for some reason the predicted median is not finite, use a safe fallback for history\\n            location_history_data.setdefault(current_loc, []).append(median_transformed_train_y_fallback)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    # Convert to float array for numerical operations\\n    predictions_array = predictions_df.values.astype(float)\\n    \\n    # Ensure all predictions are non-negative\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # Apply monotonicity and minimum spread for each row (set of quantiles for one prediction)\\n    for i in range(predictions_array.shape[0]):\\n        # Sort predictions to ensure they are monotonically increasing\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n        \\n        # Ensure minimum spread between adjacent quantiles\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n            \\n    # Convert back to DataFrame and ensure integer type\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Simple LightGBM only with fourth_root transform.\\n      # This configuration represents a robust baseline with good performance characteristics.\\n        'lgbm_params': {\\n            'n_estimators': 250, \\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Comprehensive lag features\\n        'lag_diff_periods': [1, 2, 4, 8], # Lagged differences for trend\\n        'rolling_windows': [8, 16, 26], # Rolling means for seasonality/long-term trend\\n        'rolling_std_windows': [4, 8], # Rolling std for volatility\\n        'max_admissions_per_million': 5000.0, # Cap predictions to a reasonable range\\n        'min_pred_spread': 1 # Ensure quantiles are distinct integer counts\\n    }\\n]",
  "new_index": "1984",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y using LightGBM.\\n\\n    This version uses a single LightGBM model for quantile regression, simplifying the approach\\n    compared to an ensemble. It incorporates robust time-series features (lagged target values,\\n    rolling statistics), a population-normalized and transformed target variable, and location\\n    information. The prediction for the test set is performed iteratively, where median predictions\\n    from previous steps are fed back into the feature generation for future steps, simulating\\n    observed data.\\n\\n    Key features for robustness and performance:\\n    - **Target Transformation:** Applies a 'fourth_root' transformation (configurable) to normalize\\n      the target variable and handle its skewed distribution, making it more amenable to modeling.\\n    - **Population Normalization:** Normalizes admissions by population to create a 'per million'\\n      rate, reducing variance across locations with different population sizes.\\n    - **Time-Series Features:** Generates various lagged values and rolling statistics (mean, std)\\n      of the transformed target, crucial for capturing temporal dependencies and trends.\\n    - **Iterative Prediction:** For multi-step-ahead forecasting, predictions for earlier horizons\\n      are used as \\"observed\\" values to generate features for later horizons. This is a crucial\\n      assumption for multi-step forecasting: that the predicted median can stand in for actuals.\\n    - **Quantile Regression with LightGBM:** Utilizes LightGBM's native support for quantile\\n      regression by training a separate model for each required quantile.\\n    - **Robust Pre/Post-processing:**\\n        - Handles missing population data and zero populations gracefully.\\n        - Clips transformed and inverse-transformed target values to prevent extreme predictions.\\n        - Employs forward/backward fill and fallback constants for missing time-series features.\\n        - Ensures predicted quantiles are non-negative, monotonically increasing, and maintain\\n          a minimum spread to satisfy submission requirements and improve interpretability.\\n        - Assumes historical patterns (seasonality, trends) captured by features will continue.\\n          The model's ability to generalize depends on the stability of these patterns.\\n    - **Limitations:** The model does not incorporate external factors (e.g., vaccination rates,\\n      variant prevalence) which could be crucial drivers of COVID-19 dynamics. It also assumes\\n      a fixed annual seasonality, which might not always hold for epidemic waves.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features (e.g., 'target_end_date', 'location_name', 'location', 'population').\\n        train_y (pd.Series): Training target values ('Total COVID-19 Admissions').\\n        test_x (pd.DataFrame): Test features for future time periods, similar to train_x.\\n        config (dict[str, Any]): Configuration parameters for the model, including LightGBM\\n                                 hyperparameters, feature engineering parameters, and transformation type.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    # Max admissions per million cap to prevent extremely large or unstable target values.\\n    # This assumes that admissions per million will not exceed this cap, which might be a\\n    # strong assumption in severe epidemic waves. It protects against outliers.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals\\n    # Must be at least 1 for integer counts, ensuring distinct integer predictions.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1))\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Determine median population from train data for robust imputation across both train and test.\\n    # Assumes that the training data's population median is representative.\\n    median_population_train_initial = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train_initial) or median_population_train_initial == 0:\\n        median_population_train_initial = 1.0 # Absolute fallback for median if it's NaN/Inf/0\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        def inverse_transform(x):\\n            # Ensure input to expm1 is non-negative and output is non-negative\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure input to log1p is non-negative\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to sqrt is non-negative\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n\\n    # --- 2. Function to add common date-based features and handle population ---\\n    min_date_global = df_train_full[DATE_COL].min() # Global min date for 'weeks_since_start' consistency\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp, median_pop_fallback: float) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add horizon feature directly, as it's specific to test_x but can be used in train_x (as 0)\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data if not explicitly provided\\n        \\n        # Ensure population is float for all base features and handle potential NaNs/zeros\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_pop_fallback).astype(float)\\n        # Replace any zeros with 1.0 to prevent division by zero in admissions_per_million calculation\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, 1.0, df[POPULATION_COL])\\n        \\n        return df\\n\\n    # Apply base feature engineering to training data\\n    df_train_full = add_base_features(df_train_full, min_date_global, median_population_train_initial)\\n\\n    # Calculate admissions per million and transform for training data\\n    admissions_per_million_train = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000\\n    admissions_per_million_train = np.clip(admissions_per_million_train, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million_train)\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Prepare test_x\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global, median_population_train_initial)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Pre-calculate fallback for missing history. This will be median of the *transformed* target.\\n    # This fallback is used when there's insufficient historical data for lags/rolling means.\\n    median_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(median_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        median_transformed_train_y_fallback = forward_transform(1.0) # Corresponds to 1 admission per million after inverse transform\\n    median_transformed_train_y_fallback = np.clip(median_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Use .diff() then .shift(1) to make it a lag of the difference\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        # closed='left' means the current row is not included in the rolling window calculation, which is correct for forecasting\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    # Fill NaNs from lags/rolling with forward-fill, then back-fill, and finally a constant fallback.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # First, try ffill/bfill within groups\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # If still NaNs (e.g., entire group was NaN, or first elements of a group were NaN),\\n            # fill with specific fallbacks.\\n            # Using 0.0 for diff/std is a common strategy assuming no change/variance if history is missing.\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else median_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    # This also effectively aligns X_train_model and y_train_model by dropping rows where features are not complete.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Ensures we train only on rows with valid target\\n    # Also drop NaNs from features here just in case, although previous steps should handle most.\\n    train_combined.dropna(subset=X_train_model.columns.tolist(), inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n    # Handle categorical features for LightGBM.\\n    # Collect all unique locations from both train and test to ensure consistent categorical encoding.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: None for q in QUANTILES} # Will store one LGBM model per quantile\\n\\n    for q in QUANTILES:\\n        lgbm_model_params_i = lgbm_params.copy()\\n        lgbm_model_params_i['alpha'] = q # Set quantile for this model\\n\\n        lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n        try:\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q] = lgbm_model\\n        except Exception as e:\\n            print(f\\"LGBM training failed for Q={q}: {e}\\")\\n            # If training fails, the model remains None, and fallback predictions will be used.\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with training data's transformed target values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    # This is crucial for correctly feeding back predictions into future features.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Populate current features dictionary, population is already cleaned by add_base_features\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n\\n        # Generate time-series features for the current test row using available history\\n        if not current_loc_hist:\\n            # If no history for a location (e.g., new location in test set or very early in its history),\\n            # use fallbacks for all time-series features. This is a strong assumption of no prior trends.\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = median_transformed_train_y_fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = median_transformed_train_y_fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0\\n        else:\\n            # Generate features based on available history\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n                else:\\n                    # Fallback if specific lag not available\\n                    lag_value = median_transformed_train_y_fallback\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                # Need at least diff_period + 1 values for a diff (e.g., for diff_period=1, need 2 values: last, and last-1)\\n                if len(current_loc_hist) >= diff_period + 1:\\n                    diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (diff_period + 1)]\\n                else:\\n                    # If not enough history for the requested diff_period, default to 0.0 (no change).\\n                    diff_value = 0.0\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_mean_val = median_transformed_train_y_fallback\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1: # Need at least 2 points for std dev\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0 # Std dev is 0 if only one point or no points\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # Fill new columns with 0.0, though they should be covered by X_train_model_cols list.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            lgbm_model_q = models.get(q)\\n            if lgbm_model_q:\\n                try:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                        row_predictions_transformed[q] = pred_clipped\\n                    else:\\n                        row_predictions_transformed[q] = median_transformed_train_y_fallback\\n                except Exception as e:\\n                    # print(f\\"LGBM prediction failed for Q={q}, loc={current_loc}: {e}\\") # For debugging\\n                    row_predictions_transformed[q] = median_transformed_train_y_fallback\\n            else:\\n                # Fallback if model was not trained successfully for this quantile\\n                row_predictions_transformed[q] = median_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        # Ensure transformed predictions are within valid bounds before inverse transform\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Ensure inverse transformed predictions are also clipped before converting to total admissions\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL]\\n        # Calculate final total admissions, ensuring no division by zero (population already handled)\\n        final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round and convert to integer, ensuring non-negativity\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the current location with the median prediction (transformed)\\n        # This simulates observing the median prediction as if it were actual data for future lags.\\n        # This is a key assumption for multi-step time series forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        value_to_add_to_history = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Only add finite values to history to prevent propagation of NaN/Inf\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            # If for some reason the predicted median is not finite, use a safe fallback for history\\n            location_history_data.setdefault(current_loc, []).append(median_transformed_train_y_fallback)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    # Convert to float array for numerical operations\\n    predictions_array = predictions_df.values.astype(float)\\n    \\n    # Ensure all predictions are non-negative\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # Apply monotonicity and minimum spread for each row (set of quantiles for one prediction)\\n    for i in range(predictions_array.shape[0]):\\n        # Sort predictions to ensure they are monotonically increasing.\\n        # This is a hard constraint from the competition.\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n        \\n        # Ensure minimum spread between adjacent quantiles.\\n        # This prevents zero-width intervals, which might be penalized or cause issues.\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n            \\n    # Convert back to DataFrame and ensure integer type\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Simple LightGBM only with fourth_root transform.\\n      # This configuration represents a robust baseline with good performance characteristics.\\n        'lgbm_params': {\\n            'n_estimators': 250, \\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Comprehensive lag features\\n        'lag_diff_periods': [1, 2, 4, 8], # Lagged differences for trend\\n        'rolling_windows': [8, 16, 26], # Rolling means for seasonality/long-term trend\\n        'rolling_std_windows': [4, 8], # Rolling std for volatility\\n        'max_admissions_per_million': 5000.0, # Cap predictions to a reasonable range\\n        'min_pred_spread': 1 # Ensure quantiles are distinct integer counts\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "266",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom xgboost import XGBRegressor # Importing XGBoost for ensembling\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y, using an ensemble of LGBM and XGBoost.\\n\\n    The model incorporates time-series features, a population-normalized and transformed target variable,\\n    and location information. Lagged target variables and rolling means are explicitly created and utilized.\\n    Predictions from multiple models (LGBMRegressor and XGBRegressor) are averaged.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM, XGBoost, and Feature Engineering ---\\n    # Default LGBM parameters based on common robust settings or best from prior trials.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    # Allow overriding default LGBM parameters with values from the 'config' dictionary\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters, also tuned for quantile regression.\\n    default_xgb_params = {\\n        'objective': 'reg:quantileerror', # XGBoost's objective for quantile regression\\n        'eval_metric': 'rmse',            # Common evaluation metric for regression\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 1,            # Similar to LightGBM's min_child_samples\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbosity': 0,                   # Suppress verbose output\\n        'reg_alpha': 0.1,                 # L1 regularization\\n        'reg_lambda': 0.1,                # L2 regularization\\n        'enable_categorical': True        # Explicitly enable categorical feature handling for XGBoost\\n    }\\n    # Allow overriding default XGBoost parameters with values from the 'config' dictionary\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable via 'config'\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    \\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p (best from Trial 1)\\n\\n    # Ensemble weighting - can be adjusted via config.\\n    ensemble_weights = config.get('ensemble_weights', {'lgbm': 0.5, 'xgb': 0.5})\\n    # Normalize weights to sum to 1 to ensure consistent scaling.\\n    total_weight = sum(ensemble_weights.values())\\n    if total_weight > 0:\\n        ensemble_weights = {k: v / total_weight for k, v in ensemble_weights.items()}\\n    else: # Fallback if weights are somehow zero or invalid\\n        ensemble_weights = {'lgbm': 0.5, 'xgb': 0.5}\\n\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Handle potential division by zero for population by replacing 0 with NaN, then filling resulting NaNs with 0.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing population\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transformation\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (will primarily be in lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM/XGBoost for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_lag_rolling_features = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_lag_rolling_features[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical transformed data\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                # Use .iloc[-lag] for positional indexing. Handle cases where not enough history.\\n                if len(loc_hist_data) >= lag:\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-1] if not loc_hist_data[TRANSFORMED_TARGET_COL].empty else 0.0\\n                test_lag_rolling_features.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features from the end of the historical transformed data\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target\\n                rolling_data = loc_hist_data[TRANSFORMED_TARGET_COL].tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = 0.0\\n        # If loc_hist_data is empty, test_lag_rolling_features for this location remains initialized to 0.0\\n\\n    # Merge the computed lag/rolling features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_lag_rolling_features], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # Ensure test_x_model has all columns present in X_train_model. Add missing cols as 0.0.\\n    missing_cols_in_test = set(final_feature_cols) - set(X_test_model.columns)\\n    for col in missing_cols_in_test:\\n        X_test_model[col] = 0.0 # Fill numerical missing features with 0.0\\n    \\n    X_test_model = X_test_model[final_feature_cols] # Ensure order is same\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This handles potential unseen categories in test or ensures consistent encoding.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    # Identify categorical features for LightGBM/XGBoost based on the final aligned columns\\n    categorical_features_lgbm = [LOCATION_COL] \\n\\n    # Handle 'horizon' column as categorical. Explicitly define all possible categories.\\n    if HORIZON_COL in X_test_model.columns: \\n        all_horizon_categories = sorted(list(set([-1, 0, 1, 2, 3]))) \\n        \\n        # Ensure 'horizon' column in train_x is present and set to category type with all categories\\n        if HORIZON_COL not in X_train_model.columns:\\n            X_train_model[HORIZON_COL] = 0 \\n        X_train_model[HORIZON_COL] = pd.Categorical(X_train_model[HORIZON_COL], categories=all_horizon_categories)\\n        \\n        # Ensure 'horizon' column in test_x is set to category type with all categories\\n        X_test_model[HORIZON_COL] = pd.Categorical(X_test_model[HORIZON_COL], categories=all_horizon_categories)\\n        \\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        # --- LGBM Model Training and Prediction ---\\n        preds_lgbm_transformed = np.zeros(len(X_test_model))\\n        if ensemble_weights.get('lgbm', 0) > 0:\\n            lgbm_model_params = lgbm_params.copy()\\n            lgbm_model_params['alpha'] = q # Set the quantile for this specific model\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params)\\n            lgbm_model.fit(X_train_model, y_train_model,\\n                           categorical_feature=categorical_features_lgbm)\\n            preds_lgbm_transformed = lgbm_model.predict(X_test_model)\\n            preds_lgbm_transformed[preds_lgbm_transformed < 0] = 0 # Ensure non-negative predictions\\n\\n        # --- XGBoost Model Training and Prediction ---\\n        preds_xgb_transformed = np.zeros(len(X_test_model))\\n        if ensemble_weights.get('xgb', 0) > 0:\\n            xgb_model_params = xgb_params.copy()\\n            xgb_model_params['alpha'] = q # Set the quantile for this specific model\\n\\n            xgb_model = XGBRegressor(**xgb_model_params)\\n            # XGBoost handles categorical features automatically if columns are of 'category' dtype and enable_categorical=True\\n            xgb_model.fit(X_train_model, y_train_model) \\n            preds_xgb_transformed = xgb_model.predict(X_test_model)\\n            preds_xgb_transformed[preds_xgb_transformed < 0] = 0 # Ensure non-negative predictions\\n\\n        # --- Ensemble Predictions ---\\n        # Simple weighted average of transformed predictions\\n        ensemble_preds_transformed = (\\n            ensemble_weights.get('lgbm', 0) * preds_lgbm_transformed + \\n            ensemble_weights.get('xgb', 0) * preds_xgb_transformed\\n        )\\n        \\n        predictions[f'quantile_{q}'] = ensemble_preds_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    # Multiply by population and divide by 1 million.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: Ensemble with default LGBM and XGBoost parameters, log1p transform.\\n      # This is the primary proposed ensemble model, averaging predictions.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 5,\\n            'min_child_weight': 1,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 2: Ensemble with slightly different LGBM parameters and default XGBoost.\\n        'lgbm_params': {\\n            'n_estimators': 150,      # Fewer estimators\\n            'learning_rate': 0.05,    # Faster learning rate\\n            'num_leaves': 20,         \\n            'max_depth': 4,           \\n            'min_child_samples': 30,  # Higher min_child_samples for more robust splits\\n            'random_state': 42,       \\n            'colsample_bytree': 0.7,  \\n            'subsample': 0.7,         \\n            'reg_alpha': 0.2,         \\n            'reg_lambda': 0.2         \\n        },\\n        'xgb_params': { # Default XGB parameters\\n            'n_estimators': 200, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 1,\\n            'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'reg_alpha': 0.1, 'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 3: Ensemble with default LGBM and slightly different XGBoost parameters.\\n        'lgbm_params': { # Default LGBM parameters\\n            'n_estimators': 200, 'learning_rate': 0.03, 'num_leaves': 25, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'colsample_bytree': 0.8, 'subsample': 0.8,\\n            'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150,      # Fewer estimators\\n            'learning_rate': 0.05,    # Faster learning rate\\n            'max_depth': 4,\\n            'min_child_weight': 3,    # Higher min_child_weight\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 4: Only LGBM (to compare ensemble performance against the previous best single model)\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'xgb_params': {}, # Empty dict, effectively disables XGBoost if weight is 0\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 1.0, 'xgb': 0.0} # Only LGBM contributes\\n    },\\n    { # Config 5: Only XGBoost (to see its standalone performance)\\n        'lgbm_params': {}, # Empty dict, effectively disables LGBM if weight is 0\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 5,\\n            'min_child_weight': 1,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.0, 'xgb': 1.0} # Only XGBoost contributes\\n    }\\n]",
  "new_index": "287",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom xgboost import XGBRegressor # Importing XGBoost for ensembling\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y, using an ensemble of LGBM and XGBoost.\\n\\n    The model incorporates time-series features, a population-normalized and transformed target variable,\\n    and location information. Lagged target variables and rolling means are explicitly created and utilized.\\n    Predictions from multiple models (LGBMRegressor and XGBRegressor) are averaged.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM, XGBoost, and Feature Engineering ---\\n    # Default LGBM parameters based on common robust settings or best from prior trials.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    # Allow overriding default LGBM parameters with values from the 'config' dictionary\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters, also tuned for quantile regression.\\n    default_xgb_params = {\\n        'objective': 'reg:quantileerror', # XGBoost's objective for quantile regression\\n        'eval_metric': 'rmse',            # Common evaluation metric for regression (not directly used for quantile objective, but often required)\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 1,            # Similar to LightGBM's min_child_samples\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbosity': 0,                   # Suppress verbose output\\n        'reg_alpha': 0.1,                 # L1 regularization\\n        'reg_lambda': 0.1,                # L2 regularization\\n        'enable_categorical': True        # Explicitly enable categorical feature handling for XGBoost\\n    }\\n    # Allow overriding default XGBoost parameters with values from the 'config' dictionary\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable via 'config'\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    \\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p (best from Trial 1)\\n\\n    # Ensemble weighting - can be adjusted via config.\\n    ensemble_weights = config.get('ensemble_weights', {'lgbm': 0.5, 'xgb': 0.5})\\n    # Normalize weights to sum to 1 to ensure consistent scaling.\\n    total_weight = sum(ensemble_weights.values())\\n    if total_weight > 0:\\n        ensemble_weights = {k: v / total_weight for k, v in ensemble_weights.items()}\\n    else: # Fallback if weights are somehow zero or invalid\\n        ensemble_weights = {'lgbm': 0.5, 'xgb': 0.5}\\n\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Handle potential division by zero for population by replacing 0 with a small non-zero number.\\n    safe_population_train = df_train_full[POPULATION_COL].replace(0, 1e-6).astype(float)\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transformation\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (will primarily be in lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM/XGBoost for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_lag_rolling_features = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_lag_rolling_features[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical transformed data\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                # Use .iloc[-lag] for positional indexing. Handle cases where not enough history.\\n                if len(loc_hist_data) >= lag:\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-1] if not loc_hist_data[TRANSFORMED_TARGET_COL].empty else 0.0\\n                \\n                # Ensure the value is not NaN before assigning\\n                if pd.isna(latest_lag_value):\\n                    latest_lag_value = 0.0 # Fallback for edge cases where even last valid value is NaN\\n\\n                test_lag_rolling_features.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features from the end of the historical transformed data\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target\\n                rolling_data = loc_hist_data[TRANSFORMED_TARGET_COL].tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = 0.0\\n        # If loc_hist_data is empty, test_lag_rolling_features for this location remains initialized to 0.0\\n\\n    # Merge the computed lag/rolling features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_lag_rolling_features], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # Ensure test_x_model has all columns present in X_train_model. Add missing cols as 0.0.\\n    missing_cols_in_test = set(final_feature_cols) - set(X_test_model.columns)\\n    for col in missing_cols_in_test:\\n        X_test_model[col] = 0.0 # Fill numerical missing features with 0.0\\n    \\n    X_test_model = X_test_model[final_feature_cols] # Ensure order is same\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This handles potential unseen categories in test or ensures consistent encoding.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    # Identify categorical features for LightGBM/XGBoost based on the final aligned columns\\n    categorical_features_lgbm = [LOCATION_COL] \\n\\n    # Handle 'horizon' column as categorical. Explicitly define all possible categories.\\n    if HORIZON_COL in X_test_model.columns: \\n        all_horizon_categories = sorted(list(set([-1, 0, 1, 2, 3]))) \\n        \\n        # Ensure 'horizon' column in train_x is present and set to category type with all categories\\n        if HORIZON_COL not in X_train_model.columns:\\n            X_train_model[HORIZON_COL] = 0 \\n        X_train_model[HORIZON_COL] = pd.Categorical(X_train_model[HORIZON_COL], categories=all_horizon_categories)\\n        \\n        # Ensure 'horizon' column in test_x is set to category type with all categories\\n        X_test_model[HORIZON_COL] = pd.Categorical(X_test_model[HORIZON_COL], categories=all_horizon_categories)\\n        \\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        # --- LGBM Model Training and Prediction ---\\n        preds_lgbm_transformed = np.zeros(len(X_test_model))\\n        if ensemble_weights.get('lgbm', 0) > 0:\\n            lgbm_model_params = lgbm_params.copy()\\n            lgbm_model_params['alpha'] = q # Set the quantile for this specific model\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params)\\n            lgbm_model.fit(X_train_model, y_train_model,\\n                           categorical_feature=categorical_features_lgbm)\\n            preds_lgbm_transformed = lgbm_model.predict(X_test_model)\\n            preds_lgbm_transformed[preds_lgbm_transformed < 0] = 0 # Ensure non-negative predictions\\n            # If LGBM produces NaNs (highly unlikely), fill with 0\\n            if np.any(np.isnan(preds_lgbm_transformed)):\\n                preds_lgbm_transformed[np.isnan(preds_lgbm_transformed)] = 0.0\\n\\n        # --- XGBoost Model Training and Prediction ---\\n        preds_xgb_transformed = np.zeros(len(X_test_model))\\n        if ensemble_weights.get('xgb', 0) > 0:\\n            xgb_model_params = xgb_params.copy()\\n            xgb_model_params['alpha'] = q # Set the quantile for this specific model\\n\\n            xgb_model = XGBRegressor(**xgb_model_params)\\n            # XGBoost handles categorical features automatically if columns are of 'category' dtype and enable_categorical=True\\n            xgb_model.fit(X_train_model, y_train_model) \\n            preds_xgb_transformed = xgb_model.predict(X_test_model)\\n            preds_xgb_transformed[preds_xgb_transformed < 0] = 0 # Ensure non-negative predictions\\n            \\n            # CRITICAL FIX: If XGBoost produces NaNs or Infs, effectively nullify its contribution for this quantile\\n            if np.any(np.isnan(preds_xgb_transformed)) or np.any(np.isinf(preds_xgb_transformed)):\\n                print(f\\"WARNING: XGBoost produced NaNs/Infs for quantile {q}. Setting its predictions to 0 for this quantile. \\"\\n                      \\"This means its contribution to the ensemble for this quantile will be effectively zero.\\")\\n                preds_xgb_transformed = np.zeros(len(X_test_model)) # Effectively remove its contribution\\n            \\n\\n        # --- Ensemble Predictions ---\\n        # Simple weighted average of transformed predictions\\n        ensemble_preds_transformed = (\\n            ensemble_weights.get('lgbm', 0) * preds_lgbm_transformed + \\n            ensemble_weights.get('xgb', 0) * preds_xgb_transformed\\n        )\\n        \\n        predictions[f'quantile_{q}'] = ensemble_preds_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, 1e-6).astype(float) to avoid division by zero if population somehow becomes 0.\\n    # Multiply by population and divide by 1 million.\\n    # It is important to use test_x[POPULATION_COL] which is the population for the target_end_date of the test set.\\n    safe_test_population = test_x[POPULATION_COL].replace(0, 1e-6).astype(float)\\n    predictions_df = predictions_df.multiply(safe_test_population, axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\nconfig_list = [\\n    { # Config 1: Ensemble with default LGBM and robust XGBoost parameters, log1p transform.\\n      # This is the primary proposed ensemble model, averaging predictions.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 4,           # Slightly reduced max_depth for stability\\n            'min_child_weight': 5,    # Increased min_child_weight for stability\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 2: Ensemble with slightly different LGBM parameters and default/robust XGBoost.\\n        'lgbm_params': {\\n            'n_estimators': 150,      # Fewer estimators\\n            'learning_rate': 0.05,    # Faster learning rate\\n            'num_leaves': 20,         \\n            'max_depth': 4,           \\n            'min_child_samples': 30,  # Higher min_child_samples for more robust splits\\n            'random_state': 42,       \\n            'colsample_bytree': 0.7,  \\n            'subsample': 0.7,         \\n            'reg_alpha': 0.2,         \\n            'reg_lambda': 0.2         \\n        },\\n        'xgb_params': { # Robust XGB parameters\\n            'n_estimators': 200, 'learning_rate': 0.03, 'max_depth': 4, 'min_child_weight': 5,\\n            'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'reg_alpha': 0.1, 'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 3: Ensemble with default LGBM and slightly different XGBoost parameters.\\n        'lgbm_params': { # Default LGBM parameters\\n            'n_estimators': 200, 'learning_rate': 0.03, 'num_leaves': 25, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'colsample_bytree': 0.8, 'subsample': 0.8,\\n            'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150,      # Fewer estimators\\n            'learning_rate': 0.05,    # Faster learning rate\\n            'max_depth': 3,           # Even lower max_depth\\n            'min_child_weight': 10,   # Even higher min_child_weight\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.5, 'xgb': 0.5}\\n    },\\n    { # Config 4: Only LGBM (this was the best performing in Trial 1, so keep it for comparison)\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'xgb_params': {}, # Empty dict, effectively disables XGBoost if weight is 0\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 1.0, 'xgb': 0.0} # Only LGBM contributes\\n    },\\n    { # Config 5: Only XGBoost with robust params (to see its standalone performance now)\\n        'lgbm_params': {}, # Empty dict, effectively disables LGBM if weight is 0\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 4,\\n            'min_child_weight': 5,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'enable_categorical': True\\n        },\\n        'target_transform': 'log1p',\\n        'ensemble_weights': {'lgbm': 0.0, 'xgb': 1.0} # Only XGBoost contributes\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "235",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach aims for robustness and generalization\\n    by leveraging common time-series patterns and normalizing the target by population.\\n    Lagged target variables and rolling means are explicitly created and utilized.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters based on the best performing config from Trial 1 output.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, based on best from Trial 1.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    \\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Base value is admissions per million. Handle potential division by zero for population.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (will primarily be in lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_lag_rolling_features = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_lag_rolling_features[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical transformed data\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(loc_hist_data) >= lag:\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    # or 0 if no data\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-1] if not loc_hist_data[TRANSFORMED_TARGET_COL].empty else 0.0\\n                test_lag_rolling_features.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features from the end of the historical transformed data\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target\\n                rolling_data = loc_hist_data[TRANSFORMED_TARGET_COL].tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_lag_rolling_features.loc[loc_test_indices, rolling_col_name] = 0.0\\n    \\n    # Merge the computed lag/rolling features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_lag_rolling_features], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # Ensure both DataFrames have the exact same columns in the exact same order\\n    # It is crucial to reindex X_test_model with the columns from X_train_model\\n    X_test_model = X_test_model[final_feature_cols]\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This handles potential unseen categories in test or ensures consistent encoding.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    # Identify categorical features for LightGBM based on the final aligned columns\\n    categorical_features_lgbm = [LOCATION_COL] \\n    # Treat 'horizon' as categorical if it's present, as its values are discrete and limited.\\n    if HORIZON_COL in final_feature_cols:\\n        X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype('category')\\n        X_test_model[HORIZON_COL] = X_test_model[HORIZON_COL].astype('category')\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model for LightGBM's quantile objective\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: Best performing from Trial 1, using 'log1p' and optimized LGBM params.\\n      # This is set as the default within the function for better out-of-box performance.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'log1p'\\n    },\\n    { # Config 2: A slightly different set of LGBM parameters, aiming for more regularization.\\n        'lgbm_params': {\\n            'n_estimators': 150,      # Fewer estimators\\n            'learning_rate': 0.05,    # Faster learning rate\\n            'num_leaves': 20,         \\n            'max_depth': 4,           \\n            'min_child_samples': 30,  # Higher min_child_samples for more robust splits\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.7,  \\n            'subsample': 0.7,         \\n            'reg_alpha': 0.2,         \\n            'reg_lambda': 0.2         \\n        },\\n        'target_transform': 'log1p'\\n    },\\n    { # Config 3: Explore a different set of lag and rolling window features with the default LGBM params.\\n      # This explores feature engineering robustness.\\n        'lag_weeks': [1, 2, 3, 4, 12, 26, 52], # Added 3-week lag and 12-week lag\\n        'rolling_windows': [2, 4, 8, 12],     # Added 2-week and 12-week rolling means\\n        'target_transform': 'log1p'\\n    }\\n]",
  "new_index": "279",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\nimport warnings\\n\\n# Suppress LightGBM categorical feature warning, as we handle it correctly.\\n# This warning can be noisy but usually doesn't indicate a critical issue if categories are handled.\\nwarnings.filterwarnings(\\"ignore\\", category=UserWarning, module=\\"lightgbm\\")\\n# Suppress specific FutureWarning from Pandas, often related to older indexing patterns\\nwarnings.filterwarnings(\\"ignore\\", category=FutureWarning, module=\\"pandas\\")\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach aims for robustness and generalization\\n    by leveraging common time-series patterns and normalizing the target by population.\\n    Lagged target variables and rolling means are explicitly created and utilized.\\n    Regularization is primarily handled through LightGBM parameters and by\\n    optionally restricting the training data to a recent window.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters, optimized from previous trials.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    \\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') \\n    \\n    # New configuration for training data windowing (regularization).\\n    # If None, use all available training data. Otherwise, use data from the last N weeks.\\n    min_train_date_offset_weeks = config.get('min_train_date_offset_weeks', None) \\n\\n    # --- Initial Data Preparation (full historical data for feature derivation) ---\\n    # This DataFrame (\`df_full_history\`) contains all historical data provided by the harness.\\n    # It will be used to derive features for both training and test sets, ensuring that\\n    # lags/rolling means for test data are based on the full available history up to the current point.\\n    df_full_history = train_x.copy()\\n    df_full_history[TARGET_COL] = train_y\\n    df_full_history[DATE_COL] = pd.to_datetime(df_full_history[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_full_history = df_full_history.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target on the full historical data\\n    # Handle potential division by zero for population.\\n    admissions_per_million_full_hist = df_full_history[TARGET_COL] / df_full_history[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million_full_hist = admissions_per_million_full_hist.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million_full_hist[admissions_per_million_full_hist < 0] = 0 # Ensure non-negative before transform\\n\\n    if target_transform_type == 'log1p':\\n        df_full_history[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_full_hist)\\n    elif target_transform_type == 'sqrt':\\n        df_full_history[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_full_hist)\\n    elif target_transform_type == 'fourth_root':\\n        df_full_history[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_full_hist, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_full_history[TRANSFORMED_TARGET_COL] = admissions_per_million_full_hist\\n\\n    # Determine the global minimum date from the *original* full training set\\n    # for \`weeks_since_start\` consistency across all folds/runs.\\n    min_date_global = df_full_history[DATE_COL].min()\\n    # Also get the max date of the full training data provided by the harness.\\n    original_max_train_date = df_full_history[DATE_COL].max()\\n\\n    # Function to add common date-based features\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Apply base feature extraction to full historical data and test dataframes\\n    df_full_history = add_base_features(df_full_history, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # Helper function to generate time-series features (lags and rolling means)\\n    def generate_ts_features(df_target: pd.DataFrame, target_col_name: str, lags: list, rolling_windows: list) -> pd.DataFrame:\\n        df = df_target.copy()\\n        for loc_id in df[LOCATION_COL].unique():\\n            loc_mask = (df[LOCATION_COL] == loc_id)\\n            loc_df_slice = df.loc[loc_mask].copy() # Work on a slice to avoid SettingWithCopyWarning\\n            \\n            # Generate lagged transformed target features\\n            for lag in lags:\\n                df.loc[loc_mask, f'lag_{lag}_wk'] = loc_df_slice[target_col_name].shift(lag)\\n\\n            # Generate rolling mean features, shifted by 1 to avoid data leakage\\n            for window in rolling_windows:\\n                df.loc[loc_mask, f'rolling_mean_{window}_wk'] = \\\\\\n                    loc_df_slice[target_col_name].rolling(window=window, min_periods=1).mean().shift(1)\\n        return df\\n\\n    # --- Prepare Training Data for the Model ---\\n    df_train_for_model = df_full_history.copy()\\n    # Apply training data windowing (regularization step)\\n    if min_train_date_offset_weeks is not None:\\n        filter_date_threshold = original_max_train_date - pd.Timedelta(weeks=min_train_date_offset_weeks)\\n        df_train_for_model = df_train_for_model[df_train_for_model[DATE_COL] >= filter_date_threshold].copy()\\n        df_train_for_model = df_train_for_model.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Generate time-series dependent features specifically for the training data window\\n    train_features_df_with_ts = generate_ts_features(df_train_for_model, TRANSFORMED_TARGET_COL, LAG_WEEKS, ROLLING_WINDOWS)\\n\\n    y_train_model = train_features_df_with_ts[TRANSFORMED_TARGET_COL]\\n    \\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df_with_ts[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (primarily lag/rolling features at series start/due to windowing).\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        # Fill NaNs per location using forward fill then backfill for time series consistency\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(\\n            lambda group: group.fillna(method='ffill').fillna(method='bfill')\\n        )\\n        # Final fallback to 0.0 if still NaN (e.g., location entirely new in the filtered window or no history)\\n        X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # --- Prepare Test Data for Prediction ---\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    test_lag_rolling_features_df = pd.DataFrame(index=X_test_model.index)\\n    \\n    # Calculate and apply the latest available lagged/rolling features for the test data.\\n    # These features must be based ONLY on the *full transformed historical data*\\n    # (\`df_full_history\`) available up to \`original_max_train_date\`.\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        loc_hist_data = df_full_history[\\n            (df_full_history[LOCATION_COL] == loc_id) & \\n            (df_full_history[DATE_COL] <= original_max_train_date) # Use original max date from train_x\\n        ].sort_values(DATE_COL)\\n        \\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical transformed data\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(loc_hist_data) >= lag:\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the earliest available or 0.\\n                    latest_lag_value = loc_hist_data[TRANSFORMED_TARGET_COL].iloc[0] if not loc_hist_data[TRANSFORMED_TARGET_COL].empty else 0.0\\n                test_lag_rolling_features_df.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features from the end of the historical transformed data\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target\\n                rolling_data = loc_hist_data[TRANSFORMED_TARGET_COL].tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_lag_rolling_features_df.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_lag_rolling_features_df.loc[loc_test_indices, rolling_col_name] = 0.0\\n        else: # If no historical data for this location in df_full_history, fill all with 0\\n            for col in train_specific_features:\\n                test_lag_rolling_features_df.loc[loc_test_indices, col] = 0.0\\n    \\n    # Merge the computed lag/rolling features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_lag_rolling_features_df], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute:\\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0) # Fallback to 0.0\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # Ensure both DataFrames have the exact same columns in the exact same order\\n    X_test_model = X_test_model[final_feature_cols]\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    categorical_features_lgbm = [LOCATION_COL] \\n    # Treat 'horizon' as categorical if it's present, as its values are discrete and limited.\\n    if HORIZON_COL in final_feature_cols:\\n        # Get unique horizon values from training data to define categories\\n        train_horizon_categories = X_train_model[HORIZON_COL].unique()\\n        X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=train_horizon_categories))\\n        X_test_model[HORIZON_COL] = X_test_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=train_horizon_categories))\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model for LightGBM's quantile objective\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1 (Baseline): Best performing from Trial 1, full history for training.\\n      # Serves as a reference to compare against regularization effects.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'log1p',\\n        'min_train_date_offset_weeks': None # Use all available training data\\n    },\\n    { # Config 2 (Windowed Training - 1 Year): Trains model on data from the last 52 weeks.\\n      # This acts as a strong regularization by focusing on recent trends and discarding older, potentially\\n      # less relevant data (e.g., from early pandemic stages).\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'log1p',\\n        'min_train_date_offset_weeks': 52 # Train on data from last 52 weeks\\n    },\\n    { # Config 3 (Windowed Training - 2 Years + Stronger LGBM Reg.): \\n      # Combines a longer training window (2 years) with more aggressive LGBM regularization parameters.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 20,         # Slightly reduced complexity\\n            'max_depth': 4,           # Slightly reduced complexity\\n            'min_child_samples': 30,  # Increased min_child_samples for more robust splits\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.7,  # More feature subsampling\\n            'subsample': 0.7,         # More data subsampling\\n            'reg_alpha': 0.3,         # Stronger L1 regularization\\n            'reg_lambda': 0.3         # Stronger L2 regularization\\n        },\\n        'target_transform': 'log1p',\\n        'min_train_date_offset_weeks': 104 # Train on data from last 104 weeks (2 years)\\n    },\\n    { # Config 4 (Aggressive LGBM Reg. - Full History): Uses all available training data,\\n      # but with even stronger LGBM regularization parameters.\\n        'lgbm_params': {\\n            'n_estimators': 150,      # Slightly fewer estimators (to pair with higher LR and reg)\\n            'learning_rate': 0.05,    # Slightly higher learning rate\\n            'num_leaves': 20,         \\n            'max_depth': 4,           \\n            'min_child_samples': 30,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.6,  # More aggressive feature subsampling\\n            'subsample': 0.6,         # More aggressive data subsampling\\n            'reg_alpha': 0.5,         # Even stronger L1 regularization\\n            'reg_lambda': 0.5         # Even stronger L2 regularization\\n        },\\n        'target_transform': 'log1p',\\n        'min_train_date_offset_weeks': None # Use all available training data\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

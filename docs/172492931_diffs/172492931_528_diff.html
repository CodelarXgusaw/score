<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "499",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy\\n    for the test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version explicitly aims to handle seasonality and trends through comprehensive\\n    feature engineering including cyclical date features, \\"weeks since start\\" for trend,\\n    and various short-to-long term lags and rolling means. Special attention is given\\n    to time-series specific missing data handling for generated features.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = config.get('target_transform', 'log1p')\\n    \\n    # A small epsilon for numerical stability in transformations (e.g., sqrt/log1p of zero)\\n    epsilon = 1e-6 \\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people\\n    # Handle potential zero population to prevent division errors.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation (admissions cannot be negative)\\n    admissions_per_million[admissions_per_million < 0] = 0 \\n\\n    # Apply chosen target transformation (log1p, sqrt, fourth_root, or raw)\\n    # Adding epsilon to prevent issues with log(0) or sqrt(0) for very small values\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else: \\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() \\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use ISO week number for consistency, which can range 1-53\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This helps model yearly patterns like flu seasons or holiday impacts.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since the start of the entire dataset, to capture overall trends.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define lists of features used by the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group.\\n    # Lags capture auto-correlation and temporal dependencies.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target.\\n    # Rolling means smooth noise and capture local trends.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures the window includes data *before* the current date, avoiding future leakage.\\n        # \`min_periods=1\` allows calculation even if fewer than \`window\` points are available at series start.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This feature exists in test_x and can capture different dynamics for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs from shifting\\n    # (e.g., if a week is missing in the middle of a location's series).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining initial NaNs (at the very beginning of a location's data) with 0.0.\\n            # This is a common and reasonable approach for count data when no prior history exists,\\n            # indicating no admissions or no data to derive a feature from.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Drop rows from training data if any selected feature or target is still NaN after fillna.\\n    # This ensures no NaNs are passed to the LGBM model during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) \\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories in the model.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    \\n    categorical_features_lgbm = [LOCATION_COL] \\n    \\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be extended with predictions during the iterative forecasting process.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions to the correct output format.\\n    original_test_x_index = test_x.index \\n\\n    # Prepare test data for sequential processing: Keep original index and sort by location and date.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize prediction DataFrame with the original test_x index and required quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        # This is a key time-series specific imputation strategy for future steps:\\n        # use historical data, or recent values/zeros if history is insufficient.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag] \\n            elif current_loc_hist: \\n                # If not enough history for the specific lag, use the most recent available value.\\n                lag_value = current_loc_hist[-1] \\n            else: \\n                # If no history at all for this location (e.g., completely new location), default to 0.0.\\n                lag_value = 0.0 \\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: \\n                # Use all available history if less than window size but some data exists.\\n                rolling_mean_val = np.mean(current_loc_hist) \\n            else: \\n                # No history for this location.\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling any new/missing (static)\\n        # feature columns that might arise in test_x (though unlikely for this dataset) with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n        \\n        # Re-cast categorical features with appropriate types for prediction to match training.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n        \\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        transformed_preds_array = np.array(list(row_predictions_transformed.values()))\\n        \\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            # Clamp negative predictions to 0 before inverse transform to avoid NaNs (sqrt of negative)\\n            # or complex numbers, ensuring valid real-valued outputs.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2) - epsilon \\n        elif target_transform_type == 'fourth_root':\\n            # Clamp negative predictions to 0 before inverse transform.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4) - epsilon \\n        else: # If no specific transformation was applied\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0: \\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000 \\n        \\n        # Ensure all predictions are non-negative and round to integer counts.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int) \\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed for consistency).\\n        # This is critical for generating lagged/rolling features for subsequent future dates for the same location.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform median prediction to admissions per million, clamping to non-negative\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n        \\n        # Ensure non-negative before re-transforming for history\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n        \\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    predictions_array = predictions_df.values.astype(float) \\n    predictions_array.sort(axis=1) # Sorts each row to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# Configuration options to be evaluated by the scoring harness.\\n# These configs explore different target transformations, LightGBM hyperparameters,\\n# and choices for lagged/rolling features, based on refining the previous trial's best performance.\\nconfig_list = [\\n    { # Config A: Best performing from previous trials with 'fourth_root' transformation.\\n      # Serves as a strong baseline to beat.\\n        'lgbm_params': {\\n            'n_estimators': 220,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], \\n        'rolling_windows': [8, 16, 26]      \\n    },\\n    { # Config B: Slightly more aggressive 'fourth_root' model.\\n      # Increased estimators and complexity (num_leaves, max_depth) with slightly\\n      # reduced regularization, aiming for a bit more learning capacity.\\n        'lgbm_params': {\\n            'n_estimators': 250,      # Increased\\n            'learning_rate': 0.025,   # Slightly lower\\n            'num_leaves': 31,         # Increased\\n            'max_depth': 6,           # Increased\\n            'min_child_samples': 18,  # Slightly lower (less regularization)\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.85, # Increased\\n            'subsample': 0.85,        # Increased\\n            'reg_alpha': 0.08,        # Slightly lower\\n            'reg_lambda': 0.08        # Slightly lower\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # Added back 2-week lag\\n        'rolling_windows': [4, 8, 16, 26]      # Added back 4-week window\\n    },\\n    { # Config C: Refined 'log1p' model with adjusted features.\\n      # Re-tuned LGBM parameters for the 'log1p' transformation,\\n      # exploring a slightly different set of lags and rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 230,      # Slightly more estimators\\n            'learning_rate': 0.028,   # Fine-tuned\\n            'num_leaves': 28,         # Slightly more leaves\\n            'max_depth': 5,           \\n            'min_child_samples': 22,  # Slightly more regularization\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.75, # Slightly less feature sampling\\n            'subsample': 0.75,        # Slightly less data sampling\\n            'reg_alpha': 0.15,        # Slightly more regularization\\n            'reg_lambda': 0.15        # Slightly more regularization\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # Added 3-week lag\\n        'rolling_windows': [2, 4, 8, 16, 26]      # Added 26-week window\\n    },\\n    { # Config D: More regularized 'fourth_root' model.\\n      # Reduced model complexity (num_leaves, max_depth) and increased regularization\\n      # to potentially combat overfitting, with a different set of rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 200,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 20,         # Reduced\\n            'max_depth': 4,           # Reduced\\n            'min_child_samples': 25,  # Increased\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.7,  # Reduced\\n            'subsample': 0.7,         # Reduced\\n            'reg_alpha': 0.2,         # Increased\\n            'reg_lambda': 0.2         # Increased\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 52], # Removed 26-week lag\\n        'rolling_windows': [12, 24]      # Different window sizes\\n    }\\n]",
  "new_index": "528",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame: # Changed return type hint to pd.DataFrame as per competition spec\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates comprehensive time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy\\n    for the test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version explicitly aims to handle seasonality and trends through extensive\\n    feature engineering, including cyclical date features, \\"weeks since start\\" for overall trend,\\n    and various short-to-long term lags and rolling means. Special attention is given\\n    to time-series specific missing data handling for generated features and ensuring\\n    monotonicity of quantile predictions.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = config.get('target_transform', 'log1p')\\n    \\n    # A small epsilon for numerical stability in transformations (e.g., sqrt/log1p of zero)\\n    epsilon = 1e-6 \\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people\\n    # Handle potential zero population to prevent division errors.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation (admissions cannot be negative)\\n    admissions_per_million[admissions_per_million < 0] = 0 \\n\\n    # Apply chosen target transformation (log1p, sqrt, fourth_root, or raw)\\n    # Adding epsilon to prevent issues with log(0) or sqrt(0) for very small values\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else: \\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    # This date will change per fold in a rolling window evaluation.\\n    min_date_global = df_train_full[DATE_COL].min() \\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use ISO week number for consistency, which can range 1-53\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This helps model yearly patterns like flu seasons or holiday impacts.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since the start of the entire dataset, to capture overall trends.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define lists of features used by the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group.\\n    # Lags capture auto-correlation and temporal dependencies.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target.\\n    # Rolling means smooth noise and capture local trends.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures the window includes data *before* the current date, avoiding future leakage.\\n        # \`min_periods=1\` allows calculation even if fewer than \`window\` points are available at series start.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n    \\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This feature exists in test_x and can capture different dynamics for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs from shifting\\n    # (e.g., if a week is missing in the middle of a location's series).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining initial NaNs (at the very beginning of a location's data) with 0.0.\\n            # This is a common and reasonable approach for count data when no prior history exists,\\n            # indicating no admissions or no data to derive a feature from.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Drop rows from training data if any selected feature or target is still NaN after fillna.\\n    # This ensures no NaNs are passed to the LGBM model during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) \\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories in the model.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    \\n    categorical_features_lgbm = [LOCATION_COL] \\n    \\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be extended with predictions during the iterative forecasting process.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions to the correct output format.\\n    original_test_x_index = test_x.index \\n\\n    # Prepare test data for sequential processing: Keep original index and sort by location and date.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n    \\n    # Initialize prediction DataFrame with the original test_x index and required quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        # This is a key time-series specific imputation strategy for future steps:\\n        # use historical data, or recent values/zeros if history is insufficient.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag] \\n            elif current_loc_hist: \\n                # If not enough history for the specific lag, use the most recent available value.\\n                lag_value = current_loc_hist[-1] \\n            else: \\n                # If no history at all for this location (e.g., completely new location), default to 0.0.\\n                lag_value = 0.0 \\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: \\n                # Use all available history if less than window size but some data exists.\\n                rolling_mean_val = np.mean(current_loc_hist) \\n            else: \\n                # No history for this location.\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling any new/missing (static)\\n        # feature columns that might arise in test_x (though unlikely for this dataset) with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n        \\n        # Re-cast categorical features with appropriate types for prediction to match training.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n        \\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        transformed_preds_array = np.array(list(row_predictions_transformed.values()))\\n        \\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            # Clamp negative predictions to 0 before inverse transform to avoid NaNs (sqrt of negative)\\n            # or complex numbers, ensuring valid real-valued outputs.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2) - epsilon \\n        elif target_transform_type == 'fourth_root':\\n            # Clamp negative predictions to 0 before inverse transform.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4) - epsilon \\n        else: # If no specific transformation was applied\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0: \\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000 \\n        \\n        # Ensure all predictions are non-negative and round to integer counts.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int) \\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed for consistency).\\n        # This is critical for generating lagged/rolling features for subsequent future dates for the same location.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform median prediction to admissions per million, clamping to non-negative\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n        \\n        # Ensure non-negative before re-transforming for history\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n        \\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    predictions_array = predictions_df.values.astype(float) \\n    predictions_array.sort(axis=1) # Sorts each row to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# Configuration options to be evaluated by the scoring harness.\\n# These configs explore different target transformations, LightGBM hyperparameters,\\n# and choices for lagged/rolling features, based on refining the previous trial's best performance.\\nconfig_list = [\\n    { # Config 1: Re-confirming the best performing configuration from previous trials.\\n      # This serves as a strong baseline.\\n        'lgbm_params': {\\n            'n_estimators': 220,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], \\n        'rolling_windows': [8, 16, 26]      \\n    },\\n    { # Config 2: Adds more short-term lags and rolling windows to Config 1's LGBM parameters.\\n      # This aims to capture more immediate trends and faster-changing dynamics.\\n        'lgbm_params': {\\n            'n_estimators': 220,      \\n            'learning_rate': 0.03,    \\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # Added 2-week lag\\n        'rolling_windows': [4, 8, 16, 26]      # Added 4-week window\\n    },\\n    { # Config 3: Increases \`n_estimators\` and slightly reduces \`learning_rate\` for potentially better generalization.\\n      # This is a common strategy to improve model accuracy if not already overfitting.\\n        'lgbm_params': {\\n            'n_estimators': 300,      # Increased\\n            'learning_rate': 0.02,    # Reduced\\n            'num_leaves': 26,         \\n            'max_depth': 5,           \\n            'min_child_samples': 20,  \\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.8,  \\n            'subsample': 0.8,         \\n            'reg_alpha': 0.1,         \\n            'reg_lambda': 0.1         \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], \\n        'rolling_windows': [8, 16, 26]      \\n    },\\n    { # Config 4: A slightly more complex LGBM model with reduced regularization.\\n      # This config is an attempt to push model capacity further, potentially capturing more intricate patterns.\\n        'lgbm_params': {\\n            'n_estimators': 250,      # Increased\\n            'learning_rate': 0.025,   # Slightly lower\\n            'num_leaves': 30,         # Increased\\n            'max_depth': 6,           # Increased\\n            'min_child_samples': 18,  # Slightly lower (less regularization)\\n            'random_state': 42,       \\n            'n_jobs': -1,             \\n            'verbose': -1,            \\n            'colsample_bytree': 0.85, # Increased\\n            'subsample': 0.85,        # Increased\\n            'reg_alpha': 0.08,        # Slightly lower\\n            'reg_lambda': 0.08        # Slightly lower\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # Comprehensive lags\\n        'rolling_windows': [4, 8, 16, 26]      # Comprehensive windows\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

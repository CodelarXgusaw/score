<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "259",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates comprehensive\\n    time-series features, including lagged values and rolling statistics of the\\n    population-normalized and transformed target variable. Location and time-based\\n    features are also used. The approach aims for robustness and generalization\\n    by leveraging common time-series patterns and normalizing the target by population.\\n    Lagged target variables, rolling means, and lagged differences are explicitly created and utilized.\\n    This version adds an optional clipping step for the target variable to handle outliers.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters, optimized based on previous trials.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    LAG_DIFF_PAIRS = config.get('lag_diff_pairs', [])\\n\\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p\\n\\n    # New: Outlier handling - Clipping upper quantile\\n    clip_upper_quantile = config.get('clip_upper_quantile', None) # Default to no clipping\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate admissions per million people\\n    # Handle potential division by zero for population.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transform\\n\\n    # --- Outlier Handling: Clipping before transformation ---\\n    if clip_upper_quantile is not None and 0 < clip_upper_quantile < 1:\\n        # Calculate the upper bound based on the specified quantile of the *entire* admissions_per_million data\\n        upper_bound = admissions_per_million.quantile(clip_upper_quantile)\\n        admissions_per_million = admissions_per_million.clip(upper=upper_bound)\\n\\n    # Apply chosen transformation\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    # Add lagged difference features for training data\\n    for l1, l2 in LAG_DIFF_PAIRS:\\n        col_name = f'lag_diff_{l1}wk_{l2}wk'\\n        # Ensure the base lag columns exist before trying to compute the difference\\n        if f'lag_{l1}_wk' in train_features_df.columns and f'lag_{l2}_wk' in train_features_df.columns:\\n            train_features_df[col_name] = train_features_df[f'lag_{l1}_wk'] - train_features_df[f'lag_{l2}_wk']\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'lag_diff_{l1}wk_{l2}wk' for l1, l2 in LAG_DIFF_PAIRS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (will primarily be in lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values, assuming a baseline of zero admissions for missing history.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # 4. Generate features for test data\\n    # Initial features for test set, including the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the *transformed* data available up to the last date in train_y.\\n    max_train_date = df_train_full[DATE_COL].max() # The latest date for which target data is available\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_specific_features_df = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_specific_features_df[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            historical_transformed_target_values = loc_hist_data[TRANSFORMED_TARGET_COL].values \\n            \\n            # Populate simple lag features\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(historical_transformed_target_values) >= lag:\\n                    latest_lag_value = historical_transformed_target_values[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = historical_transformed_target_values[-1] if historical_transformed_target_values.size > 0 else 0.0\\n                test_specific_features_df.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target, converted to Series temporarily for rolling\\n                rolling_data = pd.Series(historical_transformed_target_values).tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = 0.0\\n            \\n            # Populate lagged difference features *after* simple lags are populated for the current location\\n            for l1, l2 in LAG_DIFF_PAIRS:\\n                col_name = f'lag_diff_{l1}wk_{l2}wk'\\n                # Access the just-assigned values for the current location's test indices\\n                # These are already correctly broadcast when assigned to the slice\\n                lag1_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l1}_wk']\\n                lag2_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l2}_wk']\\n                test_specific_features_df.loc[loc_test_indices, col_name] = lag1_val - lag2_val\\n        # If no historical data for a location, its specific features remain 0.0 as initialized.\\n    \\n    # Merge the computed lag/rolling/diff features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_specific_features_df], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute: \\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # Ensure both DataFrames have the exact same columns in the exact same order\\n    # It is crucial to reindex X_test_model with the columns from X_train_model\\n    X_test_model = X_test_model[final_feature_cols]\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This handles potential unseen categories in test or ensures consistent encoding.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    # Identify categorical features for LightGBM based on the final aligned columns\\n    categorical_features_lgbm = [LOCATION_COL] \\n    # Treat 'horizon' as categorical if it's present, as its values are discrete and limited.\\n    if HORIZON_COL in final_feature_cols:\\n        X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype('category')\\n        X_test_model[HORIZON_COL] = X_test_model[HORIZON_COL].astype('category')\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model for LightGBM's quantile objective\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: Baseline from previous trial (best performing). log1p transform, no clipping.\\n    },\\n    { # Config 2: Add clipping at 99.9th percentile. Still log1p transform.\\n        'clip_upper_quantile': 0.999 \\n    },\\n    { # Config 3: Add clipping at 99.5th percentile. Still log1p transform.\\n        'clip_upper_quantile': 0.995 \\n    },\\n    { # Config 4: Combine clipping (e.g., 99.9th) with previously explored lag differences.\\n        'lag_diff_pairs': [(1, 2), (2, 4), (1, 52)],\\n        'clip_upper_quantile': 0.999\\n    },\\n    { # Config 5: Explore 'sqrt' transformation with clipping and lag differences.\\n        'target_transform': 'sqrt',\\n        'lag_diff_pairs': [(1, 2), (2, 4), (1, 52)],\\n        'clip_upper_quantile': 0.999\\n    }\\n]",
  "new_index": "310",
  "new_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates comprehensive\\n    time-series features, including lagged values and rolling statistics of the\\n    population-normalized and transformed target variable. Location and time-based\\n    features are also used. The approach aims for robustness and generalization\\n    by leveraging common time-series patterns and normalizing the target by population.\\n    Lagged target variables, rolling means, and lagged differences are explicitly created and utilized.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    \\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LGBM parameters, optimized based on previous trials.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,      \\n        'learning_rate': 0.03,     \\n        'num_leaves': 25,         \\n        'max_depth': 5,           \\n        'min_child_samples': 20,  \\n        'random_state': 42,       \\n        'n_jobs': -1,             \\n        'verbose': -1,            \\n        'colsample_bytree': 0.8,  \\n        'subsample': 0.8,         \\n        'reg_alpha': 0.1,         \\n        'reg_lambda': 0.1         \\n    }\\n    # Allow overriding default parameters via the 'config' dictionary\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lag weeks and rolling windows for feature engineering, configurable.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    LAG_DIFF_PAIRS = config.get('lag_diff_pairs', [])\\n\\n    # Target transformation type - configurable via the 'config' dictionary.\\n    target_transform_type = config.get('target_transform', 'log1p') # Default to log1p\\n\\n    # Outlier handling - Clipping upper quantile. Default to no clipping, as previous trials showed it might not improve performance.\\n    clip_upper_quantile = config.get('clip_upper_quantile', None) \\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y, and prepare for transformations\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct lag/rolling calculations. Essential for time-series features.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate admissions per million people to normalize for population size differences\\n    # Handle potential division by zero for population.\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL].replace(0, np.nan) * 1_000_000\\n    admissions_per_million = admissions_per_million.fillna(0) # Fill NaNs from 0 population or missing\\n    admissions_per_million[admissions_per_million < 0] = 0 # Ensure non-negative before transform\\n\\n    # --- Outlier Handling: Clipping before transformation ---\\n    if clip_upper_quantile is not None and 0 < clip_upper_quantile < 1:\\n        # Calculate the upper bound based on the specified quantile of the *entire* admissions_per_million data\\n        upper_bound = admissions_per_million.quantile(clip_upper_quantile)\\n        admissions_per_million = admissions_per_million.clip(upper=upper_bound)\\n\\n    # Apply chosen transformation to handle skewness common in count data\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # 2. Function to add common date-based features\\n    # \`min_date_global\` is determined from the full training data to ensure consistent \`weeks_since_start\`.\\n    def add_base_features(df_input: pd.DataFrame, min_date_global: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        \\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        \\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date_global).dt.days / 7).astype(int)\\n        \\n        return df\\n\\n    # Determine the global minimum date from the training set for \`weeks_since_start\` consistency\\n    min_date_global = df_train_full[DATE_COL].min()\\n    \\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global) \\n    \\n    # Define base features (features not derived from the target variable)\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n    \\n    # Generate lagged transformed target features for each location group\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n    \\n    # Add lagged difference features for training data\\n    for l1, l2 in LAG_DIFF_PAIRS:\\n        col_name = f'lag_diff_{l1}wk_{l2}wk'\\n        # Ensure the base lag columns exist before trying to compute the difference\\n        if f'lag_{l1}_wk' in train_features_df.columns and f'lag_{l2}_wk' in train_features_df.columns:\\n            train_features_df[col_name] = train_features_df[f'lag_{l1}_wk'] - train_features_df[f'lag_{l2}_wk']\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    \\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'lag_diff_{l1}wk_{l2}wk' for l1, l2 in LAG_DIFF_PAIRS] \\n    \\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This column is present in \`test_x\` but not in \`train_x\`.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0 \\n\\n    # Handle NaNs in numerical features (will primarily be in lag/rolling features at series start).\\n    # Filling with 0.0 for transformed values, assuming a baseline of zero admissions for missing history.\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM for efficient handling\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (due to shifting and lack of historical data for lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True) # Drops rows at the beginning of series with NaN lags/rolling means\\n    \\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # 4. Generate features for test data (Direct forecasting approach)\\n    # The \`horizon\` column in \`test_x\` is crucial for multi-step direct forecasting.\\n    # Lagged and rolling features for \`test_x\` are based on the latest available *actual* data from \`train_y\`.\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n    \\n    # Identify the maximum date available in the training data (last known actual observation)\\n    max_train_date = df_train_full[DATE_COL].max()\\n    \\n    # Create an empty DataFrame to hold the new lag/rolling features for test_x\\n    test_specific_features_df = pd.DataFrame(index=X_test_model.index)\\n    for col in train_specific_features:\\n        test_specific_features_df[col] = 0.0 # Initialize with default zero\\n\\n    # Calculate and apply the latest available lagged/rolling features for each location in test_x.\\n    # These features will be constant for all horizons within a specific location for a given reference_date.\\n    # The 'horizon' feature will differentiate the predictions for future weeks.\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) & \\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n        \\n        # Get the indices in X_test_model that belong to the current location\\n        loc_test_indices = X_test_model.index[X_test_model[LOCATION_COL] == loc_id]\\n\\n        if not loc_hist_data.empty:\\n            historical_transformed_target_values = loc_hist_data[TRANSFORMED_TARGET_COL].values \\n            \\n            # Populate simple lag features (e.g., lag_1_wk is value from max_train_date - 1 week)\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(historical_transformed_target_values) >= lag:\\n                    latest_lag_value = historical_transformed_target_values[-lag]\\n                else:\\n                    # If not enough history for a specific lag, use the most recent available value (if any)\\n                    latest_lag_value = historical_transformed_target_values[-1] if historical_transformed_target_values.size > 0 else 0.0\\n                test_specific_features_df.loc[loc_test_indices, lag_col_name] = latest_lag_value\\n            \\n            # Populate rolling features (e.g., rolling_mean_4_wk is mean of last 4 weeks up to max_train_date)\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Ensure rolling calculation is on the transformed target, converted to Series temporarily for rolling\\n                rolling_data = pd.Series(historical_transformed_target_values).tail(window)\\n                if not rolling_data.empty:\\n                    rolling_mean_val = rolling_data.mean()\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                else:\\n                    test_specific_features_df.loc[loc_test_indices, rolling_col_name] = 0.0\\n            \\n            # Populate lagged difference features *after* simple lags are populated for the current location\\n            for l1, l2 in LAG_DIFF_PAIRS:\\n                col_name = f'lag_diff_{l1}wk_{l2}wk'\\n                # Access the just-assigned values for the current location's test indices\\n                lag1_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l1}_wk']\\n                lag2_val = test_specific_features_df.loc[loc_test_indices, f'lag_{l2}_wk']\\n                test_specific_features_df.loc[loc_test_indices, col_name] = lag1_val - lag2_val\\n        # If no historical data for a location, its specific features remain 0.0 as initialized.\\n    \\n    # Merge the computed lag/rolling/diff features into X_test_model\\n    X_test_model = pd.concat([X_test_model, test_specific_features_df], axis=1)\\n\\n    # Impute any remaining NaNs in test features (e.g., for new locations not in train or specific edge cases)\\n    for col in numerical_cols_to_impute: \\n        if col in X_test_model.columns:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist() \\n\\n    # It is crucial to reindex X_test_model with the columns from X_train_model\\n    X_test_model = X_test_model[final_feature_cols]\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This handles potential unseen categories in test or ensures consistent encoding.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n    \\n    # Identify categorical features for LightGBM based on the final aligned columns\\n    categorical_features_lgbm = [LOCATION_COL] \\n    # Treat 'horizon' as categorical if it's present, as its values are discrete and limited.\\n    if HORIZON_COL in final_feature_cols:\\n        X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype('category')\\n        X_test_model[HORIZON_COL] = X_test_model[HORIZON_COL].astype('category')\\n        categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    # To explain model predictions, we can accumulate feature importances across all quantile models\\n    feature_importances_sum = pd.Series(0, index=X_train_model.columns)\\n\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the quantile for this specific model for LightGBM's quantile objective\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        \\n        # Make predictions for the current quantile on the test set\\n        preds_q_transformed = model.predict(X_test_model)\\n        predictions[f'quantile_{q}'] = preds_q_transformed\\n        \\n        # Accumulate feature importances from each quantile model for overall insight\\n        feature_importances_sum += pd.Series(model.feature_importances_, index=X_train_model.columns)\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Inverse transform predictions based on the chosen transformation\\n    if target_transform_type == 'log1p':\\n        predictions_df = np.expm1(predictions_df)\\n    elif target_transform_type == 'sqrt':\\n        predictions_df = np.power(predictions_df, 2)\\n    elif target_transform_type == 'fourth_root':\\n        predictions_df = np.power(predictions_df, 4)\\n    # If no specific transformation, then predictions_df is already in admissions_per_million scale.\\n\\n    # Convert from admissions per million back to total admissions\\n    # Use .replace(0, np.nan) to avoid division by zero if population somehow becomes 0.\\n    predictions_df = predictions_df.multiply(test_x[POPULATION_COL].replace(0, np.nan), axis=0) / 1_000_000 \\n    \\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # Convert to numpy array for efficient sorting\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place\\n    # Convert back to DataFrame with original columns and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # --- Model Explanation ---\\n    print(\\"\\\\n--- Model Explanation and Feature Importance ---\\")\\n    print(\\"This model uses LightGBM to predict COVID-19 hospital admissions quantiles.\\")\\n    print(\\"It employs a direct multi-step forecasting approach, where all lagged features\\")\\n    print(\\"are derived from the latest available actual data in the training set.\\")\\n    print(\\"The \`horizon\` feature then helps the model differentiate between predictions\\")\\n    print(\\"for different future weeks (e.g., current week vs. 3 weeks ahead).\\")\\n    print(\\"\\\\nKey features and their roles (based on typical importance in such models):\\")\\n    print(\\"1.  **Lagged Admissions & Rolling Means (\`lag_X_wk\`, \`rolling_mean_X_wk\`):** These are by far the most critical features.\\")\\n    print(\\"    Current hospital admissions are highly correlated with recent past admissions. Lagged values capture this direct dependency.\\")\\n    print(\\"    Rolling means provide a smoothed trend of recent activity, making the model more robust to short-term fluctuations and capturing underlying waves.\\")\\n    print(\\"    \`lag_1_wk\` (admissions 1 week ago) and \`rolling_mean_4_wk\` (average over last 4 weeks) are typically paramount.\\")\\n    print(\\"2.  **Horizon (\`horizon\`):** This feature indicates how many weeks into the future the prediction is relative to the reference date.\\")\\n    print(\\"    It allows the model to learn that uncertainty often increases with a longer \`horizon\`, and the patterns or typical values might shift over longer forecast lead times.\\")\\n    print(\\"3.  **Cyclical Time Features (\`week_of_year\`, \`sin_week_of_year\`, \`cos_week_of_year\`):**\\")\\n    print(\\"    COVID-19 admissions exhibit strong seasonality (e.g., winter surges, summer lulls). These features help the model capture weekly and annual patterns smoothly.\\")\\n    print(\\"4.  **Weeks Since Start (\`weeks_since_start\`):** Captures the overall long-term trend of the pandemic, such as a general decline or evolution over years.\\")\\n    print(\\"5.  **Location (\`location\`):** Treated as a categorical feature, it allows the model to learn state-specific patterns and baseline admission rates, accounting for local factors like population density, policies, and demographics.\\")\\n    print(\\"6.  **Population (\`population\`):** Normalizing admissions by population helps the model compare states of different sizes more effectively. It acts as a scaling factor for predictions, ensuring larger states are predicted to have higher absolute admissions proportionally.\\")\\n\\n    print(\\"\\\\nAggregated Feature Importances (summed across all quantile models, top 10 features):\\")\\n    # Sort and print top N important features\\n    top_n_features = 10\\n    print(feature_importances_sum.sort_values(ascending=False).head(top_n_features))\\n    print(\\"\\\\nNote: Lower importance features (not shown here) still contribute but to a lesser extent. The actual importance can vary slightly for each specific quantile model.\\")\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: Baseline (best performing from previous trial). log1p transform, default LGBM params.\\n        # This uses the default lgbm_params (n_estimators=200, learning_rate=0.03, num_leaves=25, max_depth=5)\\n    },\\n    { # Config 2: Slightly more estimators and lower learning rate for potentially better accuracy\\n        'lgbm_params': {\\n            'n_estimators': 300,      \\n            'learning_rate': 0.02,     \\n            'num_leaves': 25,         \\n            'max_depth': 5,           \\n        }\\n    },\\n    { # Config 3: More estimators, lower learning rate, slightly increased complexity (num_leaves, max_depth)\\n        'lgbm_params': {\\n            'n_estimators': 350,      \\n            'learning_rate': 0.02,     \\n            'num_leaves': 31,         \\n            'max_depth': 6,           \\n        }\\n    },\\n    { # Config 4: Explore 'sqrt' transformation with default LGBM params\\n        'target_transform': 'sqrt'\\n    },\\n    { # Config 5: Baseline with some specific lag differences added back, re-testing their utility\\n        'lag_diff_pairs': [(1, 2), (2, 4), (1, 52)]\\n    },\\n    { # Config 6: Combine improved LGBM params with specific lag differences\\n        'lgbm_params': {\\n            'n_estimators': 350,      \\n            'learning_rate': 0.02,     \\n            'num_leaves': 31,         \\n            'max_depth': 6,           \\n        },\\n        'lag_diff_pairs': [(1, 2), (2, 4), (1, 52)]\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

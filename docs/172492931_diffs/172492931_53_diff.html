<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "17",
  "old_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\ndef create_features(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates time-based and categorical features from the input DataFrame.\\n    This function processes both training and testing data for consistent feature engineering.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    # Convert target_end_date to datetime objects\\n    df_copy['target_end_date'] = pd.to_datetime(df_copy['target_end_date'])\\n\\n    # Extract date-related features\\n    df_copy['year'] = df_copy['target_end_date'].dt.year\\n    df_copy['month'] = df_copy['target_end_date'].dt.month\\n    # Use .isocalendar().week which gives the ISO week number (1-53)\\n    df_copy['week_of_year'] = df_copy['target_end_date'].dt.isocalendar().week.astype(int)\\n    df_copy['day_of_year'] = df_copy['target_end_date'].dt.dayofyear\\n    df_copy['day_of_week'] = df_copy['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for periodic patterns (e.g., yearly seasonality)\\n    # Using 52.0 for weeks (standard weeks in a year) ensures float division.\\n    df_copy['week_sin'] = np.sin(2 * np.pi * df_copy['week_of_year'] / 52.0)\\n    df_copy['week_cos'] = np.cos(2 * np.pi * df_copy['week_of_year'] / 52.0)\\n    # Using 366.0 for day of year to account for potential leap years, ensures float division.\\n    df_copy['dayofyear_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 366.0)\\n    df_copy['dayofyear_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 366.0)\\n\\n    # Log transform population, which is often skewed. np.log1p(x) computes log(1+x).\\n    df_copy['population_log'] = np.log1p(df_copy['population'])\\n\\n    # Treat 'location' as a categorical feature. Converting to category codes is robust.\\n    df_copy['location_id'] = df_copy['location'].astype('category').cat.codes\\n\\n    # The 'horizon' feature is present in test_x and is crucial for multi-step ahead forecasting.\\n    # For training data (train_x), 'horizon' is not typically available. We set it to 0\\n    # to indicate it's the observed historical data for consistent feature sets.\\n    if 'horizon' in df_copy.columns:\\n        df_copy['horizon'] = df_copy['horizon'].astype(int)\\n    else:\\n        df_copy['horizon'] = 0 # Default for training data\\n\\n    # A simple time index can capture overall trends over time.\\n    # Calculated relative to the minimum 'target_end_date' in the current dataframe slice.\\n    df_copy['time_idx'] = (df_copy['target_end_date'] - df_copy['target_end_date'].min()).dt.days // 7\\n\\n    return df_copy\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile\\n    and makes predictions on the test set.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    # Define the list of quantiles to predict as per competition requirements\\n    quantiles = [0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n                 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99]\\n\\n    # Apply feature engineering to both training and test data\\n    train_features = create_features(train_x)\\n    test_features = create_features(test_x)\\n\\n    # Define the features to be used for training the model\\n    features_to_use = [\\n        'year', 'month', 'week_of_year', 'day_of_year', 'day_of_week',\\n        'week_sin', 'week_cos', 'dayofyear_sin', 'dayofyear_cos',\\n        'population_log', 'location_id', 'horizon', 'time_idx'\\n    ]\\n\\n    # Ensure selected features are present in both train_features and test_features\\n    # This prevents errors if a feature is somehow missing in a specific fold/slice.\\n    common_features = list(set(features_to_use) & set(train_features.columns) & set(test_features.columns))\\n    X_train = train_features[common_features]\\n    X_test = test_features[common_features]\\n    y_train = train_y\\n\\n    # Define categorical features for LightGBM.\\n    # Filter to ensure only features actually present in X_train are used as categorical.\\n    categorical_features_candidates = ['location_id', 'year', 'month', 'week_of_year',\\n                                       'day_of_year', 'day_of_week', 'horizon']\\n    categorical_features = [f for f in categorical_features_candidates if f in common_features]\\n\\n    # Retrieve LightGBM model hyperparameters from the config dictionary,\\n    # or use default values if not specified.\\n    lgbm_params_from_config = config.get('lgbm_params', {})\\n\\n    # Default parameters for LightGBM. These will be overridden by values in lgbm_params_from_config.\\n    default_lgbm_params = {\\n        'objective': 'quantile',  # Objective for quantile regression\\n        'metric': 'quantile',     # Evaluation metric for quantile regression\\n        'n_estimators': 300,      # Number of boosting rounds\\n        'learning_rate': 0.03,    # Step size shrinkage\\n        'num_leaves': 32,         # Max number of leaves in one tree\\n        'verbose': -1,            # Suppress verbose output during training\\n        'n_jobs': -1,             # Use all available CPU cores\\n        'seed': 42,               # Random seed for reproducibility\\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\\n        'lambda_l1': 0.1,         # L1 regularization\\n        'lambda_l2': 0.1,         # L2 regularization\\n        'feature_fraction': 0.8,  # Fraction of features considered at each split\\n        'bagging_fraction': 0.8,  # Fraction of data sampled for each tree\\n        'bagging_freq': 1         # Frequency for bagging\\n    }\\n    # Merge default and provided parameters, giving precedence to config values\\n    final_lgbm_params = {**default_lgbm_params, **lgbm_params_from_config}\\n\\n    # DataFrame to store all quantile predictions for the test set\\n    test_y_hat_quantiles = pd.DataFrame(index=test_x.index)\\n\\n    # Train a separate LightGBM model for each quantile\\n    for q in quantiles:\\n        # Update the 'alpha' parameter for the current quantile\\n        current_lgbm_params = final_lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        # Initialize and train the LightGBM Regressor\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        model.fit(X_train, y_train, categorical_feature=categorical_features)\\n\\n        # Make predictions on the test set\\n        preds = model.predict(X_test)\\n\\n        # Ensure predictions are non-negative, as admissions cannot be less than zero\\n        preds[preds < 0] = 0\\n\\n        # Store predictions in the results DataFrame\\n        test_y_hat_quantiles[f'quantile_{q}'] = preds\\n\\n    # Enforce monotonicity for quantile predictions\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation metric.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        # For each row, set the current quantile's prediction to be at least the previous one's.\\n        # This uses the column-wise maximum, which is efficient.\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    return test_y_hat_quantiles\\n\\n# These configurations will be used by the evaluation harness.\\n# A list of dictionaries, where each dictionary defines a set of parameters\\n# to be passed to the \`fit_and_predict_fn\`. The harness will select the best\\n# configuration based on its internal cross-validation or scoring process.\\nconfig_list = [\\n    {\\n        # Config 1: Baseline / Original Trial settings.\\n        # Balanced complexity and learning.\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.03,\\n            'num_leaves': 32,\\n            'lambda_l1': 0.1,\\n            'lambda_l2': 0.1,\\n            'feature_fraction': 0.8,\\n            'bagging_fraction': 0.8\\n        }\\n    },\\n    {\\n        # Config 2: More estimators, lower learning rate, more complex leaves, less regularization.\\n        # Aiming for potentially better fit by allowing more complex trees and more boosting rounds,\\n        # compensated by a slower learning rate to prevent immediate overfitting.\\n        'lgbm_params': {\\n            'n_estimators': 500,\\n            'learning_rate': 0.02,\\n            'num_leaves': 64,\\n            'lambda_l1': 0.05,\\n            'lambda_l2': 0.05,\\n            'feature_fraction': 0.7,\\n            'bagging_fraction': 0.7\\n        }\\n    },\\n    {\\n        # Config 3: Fewer estimators, higher learning rate, simpler leaves, more regularization.\\n        # Aiming for a faster, potentially more generalized model that might be less prone\\n        # to overfitting specific training data patterns due to stronger regularization and simpler trees.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.05,\\n            'num_leaves': 24,\\n            'lambda_l1': 0.2,\\n            'lambda_l2': 0.2,\\n            'feature_fraction': 0.9,\\n            'bagging_fraction': 0.9\\n        }\\n    }\\n]",
  "new_index": "53",
  "new_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\ndef create_base_features(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates time-based and categorical features from the input DataFrame.\\n    This function processes both training and testing data for consistent feature engineering.\\n    It does NOT create lagged features or rolling statistics, as those depend on the target variable\\n    and require specific handling for train vs. test data.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    # Ensure target_end_date is datetime\\n    if not pd.api.types.is_datetime64_any_dtype(df_copy['target_end_date']):\\n        df_copy['target_end_date'] = pd.to_datetime(df_copy['target_end_date'])\\n\\n    # Extract date-related features\\n    df_copy['year'] = df_copy['target_end_date'].dt.year\\n    df_copy['month'] = df_copy['target_end_date'].dt.month\\n    # Use .isocalendar().week which gives the ISO week number (1-53)\\n    df_copy['week_of_year'] = df_copy['target_end_date'].dt.isocalendar().week.astype(int)\\n    df_copy['day_of_year'] = df_copy['target_end_date'].dt.dayofyear\\n    df_copy['day_of_week'] = df_copy['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for periodic patterns (e.g., yearly seasonality)\\n    # Using 52.0 for weeks (standard weeks in a year) ensures float division.\\n    df_copy['week_sin'] = np.sin(2 * np.pi * df_copy['week_of_year'] / 52.0)\\n    df_copy['week_cos'] = np.cos(2 * np.pi * df_copy['week_of_year'] / 52.0)\\n    # Using 366.0 for day of year to account for potential leap years, ensures float division.\\n    df_copy['dayofyear_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 366.0)\\n    df_copy['dayofyear_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 366.0)\\n\\n    # Log transform population, which is often skewed. np.log1p(x) computes log(1+x).\\n    df_copy['population_log'] = np.log1p(df_copy['population'])\\n\\n    # Treat 'location' as a categorical feature. Converting to category codes is robust.\\n    # Note: 'location' column in the input might already be numerical strings ('01', '06').\\n    # Using .astype('category').cat.codes maps these unique string identifiers to integers (0, 1, 2...).\\n    df_copy['location_id'] = df_copy['location'].astype('category').cat.codes\\n\\n    # The 'horizon' feature is present in test_x. For training data (train_x),\\n    # it's implicitly 0, as we are modeling historical observations.\\n    if 'horizon' in df_copy.columns:\\n        df_copy['horizon'] = df_copy['horizon'].astype(int)\\n    else:\\n        df_copy['horizon'] = 0 # Default for training data\\n\\n    # A simple time index can capture overall trends over time.\\n    # Calculated relative to the minimum 'target_end_date' in the current dataframe slice.\\n    df_copy['time_idx'] = (df_copy['target_end_date'] - df_copy['target_end_date'].min()).dt.days // 7\\n\\n    return df_copy\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile\\n    and makes predictions on the test set, incorporating time-series features\\n    like lags and rolling statistics.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    quantiles = [0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n                 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99]\\n\\n    # Combine train_x and train_y to create a full historical dataframe\\n    train_df_full = train_x.copy()\\n    train_df_full['Total COVID-19 Admissions'] = train_y\\n\\n    # Ensure date column is datetime and sort for correct time-series operations\\n    train_df_full['target_end_date'] = pd.to_datetime(train_df_full['target_end_date'])\\n    train_df_full = train_df_full.sort_values(by=['location', 'target_end_date']).reset_index(drop=True)\\n\\n    # --- Feature Engineering: Lags and Rolling Statistics for TRAINING DATA ---\\n    lags = config.get('lags', [1, 2, 3, 4, 8]) # Lags in weeks (e.g., 1-week ago, 2-weeks ago)\\n    rolling_windows = config.get('rolling_windows', [4, 8]) # Rolling mean/std windows in weeks\\n\\n    # Calculate lagged values for 'Total COVID-19 Admissions'\\n    for lag in lags:\\n        train_df_full[f'admissions_lag_{lag}'] = train_df_full.groupby('location')['Total COVID-19 Admissions'].shift(lag)\\n    # Fill any NaNs created by shifting (e.g., for the earliest records in each location's history).\\n    # Using 0 is a common strategy for count data, implying no prior admissions if history is too short.\\n    for col in [f'admissions_lag_{lag}' for lag in lags]:\\n        train_df_full[col] = train_df_full[col].fillna(0) # A more robust strategy might be mean/median or specific imputation.\\n\\n    # Calculate rolling mean and standard deviation\\n    for window in rolling_windows:\\n        train_df_full[f'rolling_mean_{window}'] = train_df_full.groupby('location')['Total COVID-19 Admissions'].transform(\\n            lambda x: x.rolling(window=window, min_periods=1).mean()\\n        )\\n        train_df_full[f'rolling_std_{window}'] = train_df_full.groupby('location')['Total COVID-19 Admissions'].transform(\\n            lambda x: x.rolling(window=window, min_periods=1).std()\\n        ).fillna(0) # Fill NaNs for std (e.g., if window is 1 or fewer records than window size)\\n\\n\\n    # Apply base features (non-lag/rolling) to the enriched training dataframe\\n    train_processed_df = create_base_features(train_df_full)\\n\\n    # Prepare test data with base features first\\n    test_processed_df = create_base_features(test_x.copy())\\n\\n    # --- Populate Lags and Rolling Stats for TEST DATA ---\\n    # For test data, we cannot calculate future lags from actual observed data.\\n    # Instead, we use the *last available actual* values from the training data for each location\\n    # to populate the lagged features for all test rows belonging to that location.\\n    # This is a common simplification when a recursive forecasting approach (predicting step-by-step\\n    # and feeding predictions back as lags) is not feasible or desired within the harness's structure.\\n    last_known_data = train_df_full.groupby('location').last()\\n\\n    # Pre-populate lag and rolling columns in test_processed_df with NaNs or 0s\\n    for lag in lags:\\n        test_processed_df[f'admissions_lag_{lag}'] = np.nan\\n    for window in rolling_windows:\\n        test_processed_df[f'rolling_mean_{window}'] = np.nan\\n        test_processed_df[f'rolling_std_{window}'] = np.nan\\n\\n    # Fill lag and rolling features for each row in test_processed_df\\n    # by looking up the last known values from the training data for its location.\\n    for loc_val, loc_df in test_processed_df.groupby('location'):\\n        if loc_val in last_known_data.index:\\n            # Get the last known values for this location from the training data\\n            last_admissions_val = last_known_data.loc[loc_val, 'Total COVID-19 Admissions']\\n            \\n            # Fill all corresponding test rows for this location\\n            test_processed_df.loc[loc_df.index, f'admissions_lag_{lag}'] = last_admissions_val\\n            for lag in lags:\\n                test_processed_df.loc[loc_df.index, f'admissions_lag_{lag}'] = last_admissions_val\\n\\n            for window in rolling_windows:\\n                test_processed_df.loc[loc_df.index, f'rolling_mean_{window}'] = \\\\\\n                    last_known_data.loc[loc_val, f'rolling_mean_{window}']\\n                test_processed_df.loc[loc_df.index, f'rolling_std_{window}'] = \\\\\\n                    last_known_data.loc[loc_val, f'rolling_std_{window}']\\n        else:\\n            # Handle locations not present in training data (unlikely for states/jurisdictions).\\n            # Fill with 0 or a global average from train_y.\\n            for lag in lags:\\n                test_processed_df.loc[loc_df.index, f'admissions_lag_{lag}'] = 0\\n            for window in rolling_windows:\\n                test_processed_df.loc[loc_df.index, f'rolling_mean_{window}'] = 0\\n                test_processed_df.loc[loc_df.index, f'rolling_std_{window}'] = 0\\n\\n\\n    # Define the complete list of features to be used for training the model\\n    features_to_use = [\\n        'year', 'month', 'week_of_year', 'day_of_year', 'day_of_week',\\n        'week_sin', 'week_cos', 'dayofyear_sin', 'dayofyear_cos',\\n        'population_log', 'location_id', 'horizon', 'time_idx'\\n    ]\\n    features_to_use.extend([f'admissions_lag_{lag}' for lag in lags])\\n    features_to_use.extend([f'rolling_mean_{window}' for window in rolling_windows])\\n    features_to_use.extend([f'rolling_std_{window}' for window in rolling_windows])\\n\\n\\n    # Select only the chosen features for training and testing dataframes\\n    # It's crucial to ensure both X_train and X_test have the same columns in the same order.\\n    # Filter to common columns to be safe, although the feature engineering should ensure consistency.\\n    X_train = train_processed_df[features_to_use].copy()\\n    X_test = test_processed_df[features_to_use].copy()\\n\\n    # Align columns between X_train and X_test in case of any discrepancy (e.g., if a feature\\n    # was all NaN in one set and thus dropped by pandas, but not in the other).\\n    # This ensures consistency for LightGBM.\\n    common_cols = list(set(X_train.columns) & set(X_test.columns))\\n    X_train = X_train[common_cols]\\n    X_test = X_test[common_cols]\\n    # Ensure order is consistent\\n    X_test = X_test[X_train.columns]\\n\\n    # Define categorical features for LightGBM.\\n    # Filter to ensure only features actually present in X_train are used as categorical.\\n    categorical_features_candidates = ['location_id', 'year', 'month', 'week_of_year',\\n                                       'day_of_year', 'day_of_week', 'horizon']\\n    categorical_features = [f for f in categorical_features_candidates if f in X_train.columns]\\n\\n\\n    # Retrieve LightGBM model hyperparameters from the config dictionary,\\n    # or use default values if not specified.\\n    lgbm_params_from_config = config.get('lgbm_params', {})\\n\\n    # Default parameters for LightGBM. These will be overridden by values in lgbm_params_from_config.\\n    default_lgbm_params = {\\n        'objective': 'quantile',  # Objective for quantile regression\\n        'metric': 'quantile',     # Evaluation metric for quantile regression\\n        'n_estimators': 300,      # Number of boosting rounds\\n        'learning_rate': 0.03,    # Step size shrinkage\\n        'num_leaves': 32,         # Max number of leaves in one tree\\n        'verbose': -1,            # Suppress verbose output during training\\n        'n_jobs': -1,             # Use all available CPU cores\\n        'seed': 42,               # Random seed for reproducibility\\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\\n        'lambda_l1': 0.1,         # L1 regularization\\n        'lambda_l2': 0.1,         # L2 regularization\\n        'feature_fraction': 0.8,  # Fraction of features considered at each split\\n        'bagging_fraction': 0.8,  # Fraction of data sampled for each tree\\n        'bagging_freq': 1         # Frequency for bagging\\n    }\\n    # Merge default and provided parameters, giving precedence to config values\\n    final_lgbm_params = {**default_lgbm_params, **lgbm_params_from_config}\\n\\n    # DataFrame to store all quantile predictions for the test set\\n    test_y_hat_quantiles = pd.DataFrame(index=test_x.index)\\n\\n    models = {} # Store trained models (e.g., for feature importance analysis)\\n\\n    # Train a separate LightGBM model for each quantile\\n    for q in quantiles:\\n        # Update the 'alpha' parameter for the current quantile\\n        current_lgbm_params = final_lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        # Initialize and train the LightGBM Regressor\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        model.fit(X_train, train_y, categorical_feature=categorical_features)\\n        models[q] = model # Store model for potential future use (e.g., importance)\\n\\n        # Make predictions on the test set\\n        preds = model.predict(X_test)\\n\\n        # Ensure predictions are non-negative, as admissions cannot be less than zero\\n        preds[preds < 0] = 0\\n\\n        # Store predictions in the results DataFrame\\n        test_y_hat_quantiles[f'quantile_{q}'] = preds\\n\\n    # Enforce monotonicity for quantile predictions\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation metric.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        # For each row, set the current quantile's prediction to be at least the previous one's.\\n        # This uses the column-wise maximum, which is efficient.\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    # --- Feature Importance Analysis (as requested for explanation) ---\\n    # We'll use the model trained for the 0.5 (median) quantile as a representative\\n    # to understand general feature importance.\\n    if config.get('print_feature_importance', False) and 0.5 in models:\\n        median_model = models[0.5]\\n        # Create a DataFrame for better readability\\n        importance_df = pd.DataFrame({\\n            'feature': X_train.columns,\\n            'importance': median_model.feature_importances_\\n        }).sort_values(by='importance', ascending=False)\\n        print(\\"\\\\n--- Feature Importances (from Median Quantile Model) ---\\")\\n        print(importance_df.head(15)) # Print top 15 important features for brevity\\n        print(\\"\\\\nExplanation of Feature Importance:\\")\\n        print(\\"Features with higher importance values were more frequently used by the LightGBM model\\")\\n        print(\\"to make splits in its decision trees, indicating their strong influence on predictions.\\")\\n        print(\\"Typically, for time series like this, lagged values of the target variable (e.g., 'admissions_lag_1', 'rolling_mean_4')\\")\\n        print(\\"are the most important as past observations are strong predictors of future ones.\\")\\n        print(\\"Other key features include 'location_id' (capturing state-specific baselines and trends),\\")\\n        print(\\"'horizon' (critical for multi-step-ahead forecasts), 'population_log' (scaling factor),\\")\\n        print(\\"and cyclical time features (e.g., 'week_sin', 'week_cos') for seasonality.\\")\\n\\n    return test_y_hat_quantiles\\n\\n# These configurations will be used by the evaluation harness.\\n# A list of dictionaries, where each dictionary defines a different configuration\\n# for your \`fit_and_predict_fn\`. The harness will iterate through these configs,\\n# run the rolling window evaluation for each, and select the best one.\\nconfig_list = [\\n    {\\n        # Config 1: A balanced setup with common lags and rolling windows.\\n        # Aiming for a good general performance across different data slices.\\n        'lgbm_params': {\\n            'n_estimators': 350,      # Moderate number of trees\\n            'learning_rate': 0.025,   # Moderate learning rate\\n            'num_leaves': 36,         # Balanced tree complexity\\n            'lambda_l1': 0.1,         # L1 regularization\\n            'lambda_l2': 0.1,         # L2 regularization\\n            'feature_fraction': 0.75, # Feature subsampling\\n            'bagging_fraction': 0.75, # Data subsampling\\n            'bagging_freq': 1\\n        },\\n        'lags': [1, 2, 3, 4, 8, 12], # Lags in weeks: captures recent, monthly, and quarterly patterns\\n        'rolling_windows': [4, 8, 12], # Rolling stats: 1-month, 2-month, 3-month averages for smoothed trends\\n        'print_feature_importance': True # Enable printing for this config for analysis\\n    },\\n    {\\n        # Config 2: More aggressive learning, fewer estimators, focus on shorter-term lags.\\n        # Might perform better on rapidly changing trends if less regularization is beneficial,\\n        # or if the evaluation window is shorter.\\n        'lgbm_params': {\\n            'n_estimators': 250,      # Fewer trees\\n            'learning_rate': 0.04,    # Higher learning rate\\n            'num_leaves': 28,         # Slightly less complex trees\\n            'lambda_l1': 0.05,        # Less regularization\\n            'lambda_l2': 0.05,\\n            'feature_fraction': 0.8,\\n            'bagging_fraction': 0.8,\\n            'bagging_freq': 1\\n        },\\n        'lags': [1, 2, 4], # Focus on very recent and weekly lags\\n        'rolling_windows': [2, 4], # Shorter rolling windows to capture very recent dynamics\\n        'print_feature_importance': False # Disable to reduce output noise for other configs\\n    },\\n    {\\n        # Config 3: More conservative learning, more estimators, more complex model (higher num_leaves).\\n        # More lags and rolling windows to capture broader historical patterns. Potentially more robust\\n        # for stable, long-term trends but might be slower.\\n        'lgbm_params': {\\n            'n_estimators': 500,      # More trees\\n            'learning_rate': 0.015,   # Lower learning rate\\n            'num_leaves': 64,         # Higher tree complexity\\n            'lambda_l1': 0.15,        # More regularization\\n            'lambda_l2': 0.15,\\n            'feature_fraction': 0.7,\\n            'bagging_fraction': 0.7,\\n            'bagging_freq': 1\\n        },\\n        'lags': [1, 2, 4, 8, 12, 24, 36, 52], # Comprehensive lags covering up to a year\\n        'rolling_windows': [4, 8, 12, 26, 52], # Comprehensive rolling stats, including half-year and full-year\\n        'print_feature_importance': False\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "128",
  "old_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression, incorporating time-series\\n    features, population, and location information. It provides an option\\n    to apply a log1p transformation to the target variable.\\n\\n    This function is designed to be called within a rolling window evaluation harness.\\n    It prepares features based on the historical data provided in train_x/train_y\\n    and makes predictions for the future periods specified in test_x.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model,\\n                                  including LGBM hyperparameters and feature engineering options.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'. The index matches test_x's index.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # Column name for forecast horizon\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters for quantile regression\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200, # Default, will be overridden by config\\n        'learning_rate': 0.05, # Default, will be overridden by config\\n        'num_leaves': 31, # Default, will be overridden by config\\n        'max_depth': -1, # Default, will be overridden by config\\n        'min_child_samples': 20, # Default, will be overridden by config\\n        'random_state': 42,\\n        'n_jobs': -1, # Use all available CPU cores\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8, # Subsample columns\\n        'subsample': 0.8, # Subsample rows\\n    }\\n    # Override defaults with parameters provided in the config dictionary\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 12, 26, 52]) # Lags of target variable\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 12]) # Windows for rolling statistics\\n    USE_LOG_TRANSFORM = config.get('use_log_transform', True) # Apply log1p transform to target\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y for historical data processing\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    # Ensure date column is datetime and sort for correct lag/rolling calculations\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data by location and date to ensure correct time-series operations\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # 2. Function to add common date features and population scaling\\n    def add_static_features(df_input):\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # week_of_year from isocalendar handles cases where Jan 1st is in previous year's week\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        # Removed day_of_year, day_of_week, day_of_month for simplicity as weekly data is less granular\\n\\n        # Approximate 'season' based on month for capturing yearly patterns\\n        df['season'] = (df['month'] % 12 + 3) // 3\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly\\n        # Using sin/cos transforms helps the model understand periodicity without abrupt jumps\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Scale population to prevent disproportionate influence due to large numbers\\n        df['population_scaled'] = df[POPULATION_COL] / 1_000_000\\n\\n        return df\\n\\n    # Apply static feature extraction to training and test dataframes\\n    df_train_full = add_static_features(df_train_full)\\n    test_x_processed = add_static_features(test_x.copy())\\n\\n    # Define base features (features not derived from the target variable, common to train/test)\\n    # Simplified BASE_FEATURES by removing granular daily features\\n    BASE_FEATURES = [POPULATION_COL, 'population_scaled', 'year', 'month', 'week_of_year',\\n                     'season', 'sin_week_of_year', 'cos_week_of_year']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Treat location as categorical feature\\n\\n    # 3. Generate time-series dependent features for training data\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged target features for each location group\\n    # These features represent the number of admissions in previous weeks\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean and standard deviation features, shifted to avoid data leakage\\n    # These capture recent trends and volatility in admissions\\n    for window in ROLLING_WINDOWS:\\n        # Shift by 1 to ensure that rolling stats only use data *prior* to the target_end_date\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\\n            )\\n\\n    # Prepare target variable for training\\n    y_train_model = train_features_df[TARGET_COL]\\n\\n    # Apply log transform to target if enabled to handle skewed distribution and large values\\n    if USE_LOG_TRANSFORM:\\n        y_train_model = np.log1p(y_train_model) # log1p(x) = log(1+x) avoids log(0) issues\\n\\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_WINDOWS]\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    # Ensure all columns are present before slicing\\n    for col in X_train_model_cols:\\n        if col not in train_features_df.columns:\\n            train_features_df[col] = np.nan # Add missing columns, will be handled by fillna later\\n\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical training data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Handle NaNs in numerical features (primarily in lag/rolling features at series start)\\n    # Filling with 0.0 is a common strategy for features that don't exist yet (e.g., first few weeks of data)\\n    numerical_cols_to_impute = [col for col in train_specific_features if col in X_train_model.columns]\\n    for col in numerical_cols_to_impute:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Cast 'location' to category type for LightGBM for efficient handling and consistent encoding\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype('category')\\n\\n    # Drop rows where target or features are NaN (e.g., due to shifting or not enough historical data for lags)\\n    # This ensures only complete feature sets are used for training.\\n    valid_train_indices = y_train_model.dropna().index\\n    X_train_model = X_train_model.loc[valid_train_indices]\\n    y_train_model = y_train_model.loc[valid_train_indices]\\n\\n\\n    # 4. Generate features for test data (forecasting future periods)\\n    # Start with base features and the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n\\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the data available up to the last date in train_y,\\n    # as the model cannot see future values.\\n\\n    # Determine the latest date for which target data is available in the training set\\n    max_train_date = df_train_full[DATE_COL].max()\\n\\n    # Create a temporary dataframe to store latest features per location for merging\\n    latest_features_data = []\\n\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date\\n        loc_hist_data = df_train_full[\\n            (df_train_full[LOCATION_COL] == loc_id) &\\n            (df_train_full[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n\\n        loc_feats = {LOCATION_COL: loc_id} # Initialize with location ID\\n\\n        # Initialize lag and rolling features to 0.0\\n        for lag in LAG_WEEKS:\\n            loc_feats[f'lag_{lag}_wk'] = 0.0\\n        for window in ROLLING_WINDOWS:\\n            loc_feats[f'rolling_mean_{window}_wk'] = 0.0\\n            loc_feats[f'rolling_std_{window}_wk'] = 0.0\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical data\\n            for lag in LAG_WEEKS:\\n                if len(loc_hist_data) >= lag:\\n                    loc_feats[f'lag_{lag}_wk'] = loc_hist_data[TARGET_COL].iloc[-lag]\\n                # If not enough history for a specific lag, it retains the initialized 0.0\\n\\n            # Populate rolling features from the end of the historical data\\n            for window in ROLLING_WINDOWS:\\n                # We need data *before* the prediction date for rolling features\\n                rolling_data_for_mean_std = loc_hist_data[TARGET_COL].tail(window)\\n                if not rolling_data_for_mean_std.empty:\\n                    rolling_mean_val = rolling_data_for_mean_std.mean()\\n                    rolling_std_val = rolling_data_for_mean_std.std()\\n\\n                    # Handle potential NaN from std if window=1 (std dev of single value is NaN)\\n                    loc_feats[f'rolling_mean_{window}_wk'] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                    loc_feats[f'rolling_std_{window}_wk'] = rolling_std_val if not pd.isna(rolling_std_val) else 0.0\\n\\n        latest_features_data.append(loc_feats)\\n\\n    # Convert latest features to DataFrame and merge with test_x\\n    latest_features_df = pd.DataFrame(latest_features_data)\\n\\n    # Merge historical lag/rolling features into the test features dataframe\\n    X_test_model = pd.merge(X_test_model, latest_features_df, on=LOCATION_COL, how='left')\\n\\n    # Impute any remaining NaNs in test features.\\n    for col in numerical_cols_to_impute:\\n        if col not in X_test_model.columns:\\n             # This handles cases where a feature might be generated in train but not test initially\\n            X_test_model[col] = 0.0\\n        else:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n\\n    # 5. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model\\n    final_feature_cols = X_train_model.columns.tolist()\\n\\n    # Ensure test data has all features expected by the trained model, filling missing with 0.0\\n    for col in final_feature_cols:\\n        if col not in X_test_model.columns:\\n            X_test_model[col] = 0.0 # Add any missing columns to test set\\n    X_test_model = X_test_model[final_feature_cols] # Align order and select columns\\n\\n    # Re-cast 'location' in X_test_model to category type with categories from train.\\n    # This ensures consistent encoding for LightGBM.\\n    train_location_categories = X_train_model[LOCATION_COL].cat.categories\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=train_location_categories)\\n\\n    # Identify categorical features for LightGBM based on the final aligned columns\\n    categorical_features_lgbm = [col for col in final_feature_cols if col in CATEGORICAL_FEATURES_LIST]\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n\\n        # Make predictions for the current quantile on the test set\\n        preds_q = model.predict(X_test_model)\\n\\n        # Inverse transform if log transform was used\\n        if USE_LOG_TRANSFORM:\\n            preds_q = np.expm1(preds_q) # exp(x) - 1, to revert log1p\\n\\n        predictions[f'quantile_{q}'] = preds_q\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # This sorts the predicted quantiles for each prediction instance in ascending order.\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place by quantile value\\n    # Recreate DataFrame to maintain column names and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: A more aggressive simplification\\n        'lgbm_params': {\\n            'n_estimators': 100,\\n            'learning_rate': 0.1,\\n            'num_leaves': 10,\\n            'max_depth': 3,\\n            'min_child_samples': 40,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 52], # Focus on immediate and yearly seasonality\\n        'rolling_windows': [4], # Single 4-week rolling window\\n        'use_log_transform': False # Consistent with previous best result\\n    },\\n    { # Config 2: Similar to previous best, with simplified base features (dropping granular date features)\\n        'lgbm_params': {\\n            'n_estimators': 150,\\n            'learning_rate': 0.08,\\n            'num_leaves': 15,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 12],\\n        'rolling_windows': [4],\\n        'use_log_transform': False\\n    },\\n    { # Config 3: A slightly more robust, but still relatively simple option (also with simplified base features)\\n        'lgbm_params': {\\n            'n_estimators': 120,\\n            'learning_rate': 0.07,\\n            'num_leaves': 15,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 8, 52], # More lags for robustness\\n        'rolling_windows': [4, 8], # Two rolling window sizes\\n        'use_log_transform': False\\n    }\\n]",
  "new_index": "157",
  "new_code": "import numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression, incorporating time-series\\n    features, population, and location information. It explicitly handles\\n    missing time-series points by creating a dense weekly index for each location\\n    and filling missing values for 'Total COVID-19 Admissions' before generating\\n    lagged and rolling features. This ensures that time-dependent features\\n    accurately reflect calendar weeks.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model,\\n                                  including LGBM hyperparameters and feature engineering options.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'. The index matches test_x's index.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # Column name for forecast horizon\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters for quantile regression\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.05,\\n        'num_leaves': 31,\\n        'max_depth': -1,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 12, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 12])\\n    USE_LOG_TRANSFORM = config.get('use_log_transform', True)\\n\\n    # --- Feature Engineering ---\\n\\n    # 1. Combine train_x and train_y for historical data processing\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n\\n    # Ensure location column is string for consistent handling before categorical conversion\\n    # and to union all possible location IDs from train and test.\\n    df_train_full[LOCATION_COL] = df_train_full[LOCATION_COL].astype(str)\\n    test_x[LOCATION_COL] = test_x[LOCATION_COL].astype(str)\\n\\n    # Get all unique locations that appear in either train or test data.\\n    # This is crucial for ensuring categorical features are consistent and for processing all relevant locations.\\n    all_locations_union = pd.Index(df_train_full[LOCATION_COL].unique()).union(test_x[LOCATION_COL].unique())\\n\\n    # 2. Pre-processing for time-series: Ensure dense weekly index per location and handle missing target values\\n    processed_train_data = []\\n    for loc_id in all_locations_union: # Iterate over all known locations\\n        loc_data = df_train_full[df_train_full[LOCATION_COL] == loc_id].copy()\\n\\n        if loc_data.empty:\\n            # If a location is in test_x but not train_x, it has no historical training data.\\n            # We skip it for training data processing; its test features will be initialized to 0.0 later.\\n            continue\\n\\n        # Sort by date and set as index to allow resampling/reindexing\\n        loc_data = loc_data.set_index(DATE_COL).sort_index()\\n\\n        # Define the full date range for this specific location within the train period\\n        # Using 'W-SAT' to align with typical CDC Epi-week end dates (Saturday)\\n        loc_min_date = loc_data.index.min()\\n        loc_max_date = loc_data.index.max()\\n        full_loc_dates = pd.date_range(start=loc_min_date, end=loc_max_date, freq='W-SAT')\\n\\n        # Reindex to create a dense weekly series. This will introduce NaNs for weeks not originally present.\\n        loc_data_reindexed = loc_data.reindex(full_loc_dates)\\n\\n        # Fill static features (population, location_name, location ID).\\n        # These should generally be constant or slow-changing within a location.\\n        # Ffill then bfill ensures all reindexed rows get a value if available.\\n        loc_data_reindexed[POPULATION_COL] = loc_data_reindexed[POPULATION_COL].ffill().bfill()\\n        loc_data_reindexed[LOCATION_COL] = loc_data_reindexed[LOCATION_COL].ffill().bfill()\\n        loc_data_reindexed['location_name'] = loc_data_reindexed['location_name'].ffill().bfill()\\n\\n        # Handle missing 'Total COVID-19 Admissions' for reindexed weeks.\\n        # Filling with 0.0 assumes that if a weekly report is missing, it implies zero or very low admissions.\\n        # This makes the time series dense and values for lags/rolling means consistent for calendar weeks.\\n        loc_data_reindexed[TARGET_COL] = loc_data_reindexed[TARGET_COL].fillna(0.0)\\n\\n        # Reset index to make target_end_date a column again\\n        loc_data_reindexed = loc_data_reindexed.reset_index().rename(columns={'index': DATE_COL})\\n        processed_train_data.append(loc_data_reindexed)\\n\\n    # Concatenate all processed location dataframes\\n    if processed_train_data:\\n        df_train_full_processed = pd.concat(processed_train_data, ignore_index=True)\\n    else:\\n        # If no training data is available (e.g., train_x is empty), return a scaffold of zeros.\\n        # This case is highly unlikely in a Kaggle setting but ensures robustness.\\n        predictions_df = pd.DataFrame(0, index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        return predictions_df\\n\\n    # Sort again by location and date to ensure correct time-series operations\\n    df_train_full_processed = df_train_full_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # 3. Function to add common date features and population scaling\\n    def add_static_features(df_input):\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['season'] = (df['month'] % 12 + 3) // 3 # Simple season approximation (e.g., Dec-Feb = winter)\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n        df['population_scaled'] = df[POPULATION_COL] / 1_000_000\\n        return df\\n\\n    # Apply static feature extraction to the processed training data and raw test data\\n    df_train_full_processed = add_static_features(df_train_full_processed)\\n    test_x_processed = add_static_features(test_x.copy())\\n\\n    # Define base features (features not derived from the target variable, common to train/test)\\n    BASE_FEATURES = [POPULATION_COL, 'population_scaled', 'year', 'month', 'week_of_year',\\n                     'season', 'sin_week_of_year', 'cos_week_of_year']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # 'location' will be treated as categorical\\n\\n    # 4. Generate time-series dependent features for training data\\n    # These features are now calculated on the dense and filled 'Total COVID-19 Admissions' series.\\n    for lag in LAG_WEEKS:\\n        df_train_full_processed[f'lag_{lag}_wk'] = df_train_full_processed.groupby(LOCATION_COL)[TARGET_COL].shift(lag)\\n\\n    for window in ROLLING_WINDOWS:\\n        df_train_full_processed[f'rolling_mean_{window}_wk'] = \\\\\\n            df_train_full_processed.groupby(LOCATION_COL)[TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\\n            )\\n        df_train_full_processed[f'rolling_std_{window}_wk'] = \\\\\\n            df_train_full_processed.groupby(LOCATION_COL)[TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\\n            )\\n\\n    # Prepare target variable for training\\n    y_train_model = df_train_full_processed[TARGET_COL]\\n\\n    # Apply log transform to target if enabled to handle skewed distribution and large values\\n    if USE_LOG_TRANSFORM:\\n        y_train_model = np.log1p(y_train_model)\\n\\n    # Compile the list of all feature columns for training\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_WINDOWS]\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    # Ensure all columns are present before slicing (e.g., if a feature list is empty)\\n    for col in X_train_model_cols:\\n        if col not in df_train_full_processed.columns:\\n            df_train_full_processed[col] = np.nan # Add missing columns\\n\\n    X_train_model = df_train_full_processed[X_train_model_cols].copy()\\n    X_train_model[HORIZON_COL] = 0 # Add horizon feature for training data\\n\\n    # Fill NaNs in numerical features that result from shifting/rolling (typically at series start).\\n    # Filling with 0.0 is a common strategy for features that don't exist yet (e.g., first few weeks of data).\\n    for col in train_specific_features:\\n        if col in X_train_model.columns:\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Convert 'location' to category type with *all possible* categories from train and test.\\n    # This ensures consistency and proper handling by LightGBM even if a location is only in test set.\\n    X_train_model[LOCATION_COL] = pd.Categorical(X_train_model[LOCATION_COL], categories=all_locations_union)\\n\\n    # Drop rows where target is NaN (e.g., if some initial \`Total COVID-19 Admissions\` values were NaN\\n    # and not filled by \`fillna(0.0)\` for some reason, or if population was still NaN).\\n    # This also implicitly removes rows where lags/rolling stats are NaN if they were not filled.\\n    valid_train_indices = y_train_model.dropna().index\\n    X_train_model = X_train_model.loc[valid_train_indices]\\n    y_train_model = y_train_model.loc[valid_train_indices]\\n\\n    # 5. Generate features for test data (forecasting future periods)\\n    # Start with base features and the inherent 'horizon' from test_x\\n    X_test_model = test_x_processed[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]].copy()\\n\\n    # Convert 'location' to category type for test set, using the same categories as train.\\n    X_test_model[LOCATION_COL] = pd.Categorical(X_test_model[LOCATION_COL], categories=all_locations_union)\\n\\n    # Derive 'latest observed' lag and rolling features for the test data.\\n    # These features must be based ONLY on the data available up to the last date in train_y.\\n    max_train_date = df_train_full_processed[DATE_COL].max()\\n\\n    latest_features_data = []\\n\\n    for loc_id in X_test_model[LOCATION_COL].unique():\\n        # Filter historical data for the current location, up to max_train_date.\\n        # Use the processed (dense and filled) training data for historical values.\\n        loc_hist_data = df_train_full_processed[\\n            (df_train_full_processed[LOCATION_COL] == loc_id) &\\n            (df_train_full_processed[DATE_COL] <= max_train_date)\\n        ].sort_values(DATE_COL)\\n\\n        loc_feats = {LOCATION_COL: loc_id}\\n\\n        # Initialize all lag and rolling features to 0.0 for this location.\\n        # This handles cases where a location is new or has insufficient history.\\n        for lag in LAG_WEEKS:\\n            loc_feats[f'lag_{lag}_wk'] = 0.0\\n        for window in ROLLING_WINDOWS:\\n            loc_feats[f'rolling_mean_{window}_wk'] = 0.0\\n            loc_feats[f'rolling_std_{window}_wk'] = 0.0\\n\\n        if not loc_hist_data.empty:\\n            # Populate lag features from the end of the historical data\\n            for lag in LAG_WEEKS:\\n                if len(loc_hist_data) >= lag:\\n                    loc_feats[f'lag_{lag}_wk'] = loc_hist_data[TARGET_COL].iloc[-lag]\\n                # If not enough history for a specific lag, it retains the initialized 0.0\\n\\n            # Populate rolling features from the end of the historical data\\n            for window in ROLLING_WINDOWS:\\n                rolling_data_for_mean_std = loc_hist_data[TARGET_COL].tail(window)\\n                if not rolling_data_for_mean_std.empty:\\n                    rolling_mean_val = rolling_data_for_mean_std.mean()\\n                    rolling_std_val = rolling_data_for_mean_std.std()\\n                    # Handle potential NaN from std if window=1 (std dev of single value is NaN)\\n                    loc_feats[f'rolling_mean_{window}_wk'] = rolling_mean_val if not pd.isna(rolling_mean_val) else 0.0\\n                    loc_feats[f'rolling_std_{window}_wk'] = rolling_std_val if not pd.isna(rolling_std_val) else 0.0\\n                # If rolling_data_for_mean_std is empty, it retains the initialized 0.0\\n        latest_features_data.append(loc_feats)\\n\\n    # Convert latest features to DataFrame and merge with test_x\\n    latest_features_df = pd.DataFrame(latest_features_data)\\n    X_test_model = pd.merge(X_test_model, latest_features_df, on=LOCATION_COL, how='left')\\n\\n    # Impute any remaining NaNs in test features (should only be from the merge if a feature wasn't in latest_features_df).\\n    for col in train_specific_features:\\n        if col not in X_test_model.columns:\\n             # This handles cases where a feature might be generated in train but not test initially\\n            X_test_model[col] = 0.0\\n        else:\\n            X_test_model[col] = X_test_model[col].fillna(0.0)\\n\\n    # 6. Align columns between train and test datasets\\n    # This step is critical to ensure feature consistency for the model.\\n    # It also handles cases where some features might not have been generated for test_x initially.\\n    final_feature_cols = X_train_model.columns.tolist()\\n\\n    # Ensure test data has all features expected by the trained model, filling missing with 0.0\\n    for col in final_feature_cols:\\n        if col not in X_test_model.columns:\\n            X_test_model[col] = 0.0\\n    X_test_model = X_test_model[final_feature_cols] # Align order and select columns\\n\\n    # Identify categorical features for LightGBM based on the final aligned columns\\n    # LightGBM expects categorical features to be pd.Category dtype or to be specified by column name.\\n    categorical_features_lgbm = [col for col in final_feature_cols if col in CATEGORICAL_FEATURES_LIST]\\n\\n    # --- Model Training and Prediction ---\\n    predictions = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n\\n        # Initialize and train LightGBM Regressor for the current quantile\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n\\n        # Make predictions for the current quantile on the test set\\n        preds_q = model.predict(X_test_model)\\n\\n        # Inverse transform if log transform was used\\n        if USE_LOG_TRANSFORM:\\n            preds_q = np.expm1(preds_q) # exp(x) - 1, to revert log1p\\n\\n        predictions[f'quantile_{q}'] = preds_q\\n\\n    # --- Post-processing ---\\n    # Convert predictions dictionary to a DataFrame, matching the test_x index\\n    predictions_df = pd.DataFrame(predictions, index=test_x.index)\\n\\n    # Ensure all predictions are non-negative, as hospital admissions cannot be negative\\n    predictions_df[predictions_df < 0] = 0\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts)\\n    # This sorts the predicted quantiles for each prediction instance in ascending order.\\n    predictions_array = predictions_df.values\\n    predictions_array.sort(axis=1) # Sorts each row in-place by quantile value\\n    # Recreate DataFrame to maintain column names and index\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# These will get scored by code that I supply. You'll get back a summary\\n# of the performance of each of them.\\nconfig_list = [\\n    { # Config 1: A more aggressive simplification\\n        'lgbm_params': {\\n            'n_estimators': 100,\\n            'learning_rate': 0.1,\\n            'num_leaves': 10,\\n            'max_depth': 3,\\n            'min_child_samples': 40,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 52], # Focus on immediate and yearly seasonality\\n        'rolling_windows': [4], # Single 4-week rolling window\\n        'use_log_transform': False # Consistent with previous best result\\n    },\\n    { # Config 2: Similar to previous best, with simplified base features (dropping granular date features)\\n        'lgbm_params': {\\n            'n_estimators': 150,\\n            'learning_rate': 0.08,\\n            'num_leaves': 15,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 12],\\n        'rolling_windows': [4],\\n        'use_log_transform': False\\n    },\\n    { # Config 3: A slightly more robust, but still relatively simple option (also with simplified base features)\\n        'lgbm_params': {\\n            'n_estimators': 120,\\n            'learning_rate': 0.07,\\n            'num_leaves': 15,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n        },\\n        'lag_weeks': [1, 4, 8, 52], # More lags for robustness\\n        'rolling_windows': [4, 8], # Two rolling window sizes\\n        'use_log_transform': False\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

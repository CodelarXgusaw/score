<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "981",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version simplifies the model to use only LightGBM and focuses on robust\\n    handling of missing data, improved numerical stability by clipping transformed\\n    target and predictions, and new date features like weekday. It also adds a\\n    heuristic 'is_major_holiday_week' feature to capture potential holiday effects.\\n    Key aspects include:\\n    - Robust feature engineering with fallbacks for missing history.\\n    - Rigorous clipping of values at various stages (before/after transformation, before/after inverse transformation).\\n    - Ensures monotonicity of quantile predictions.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1) \\n\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Ensure population is never zero or negative for division\\n    safe_population = np.where(df_train_full[POPULATION_COL] <= 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    \\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result of power - 1.0 is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result of power - 1.0 is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        # Ensure location is always a zero-padded string to handle FIPS codes consistently\\n        df[LOCATION_COL] = df[LOCATION_COL].astype(str).str.zfill(2)\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add a heuristic for major holiday weeks in the US.\\n        # This is a simplification using approximate week numbers, and would be more robust\\n        # with a proper holiday calendar.\\n        major_holiday_weeks_map = {\\n            # Thanksgiving (last Thurs Nov): typically week 47 or 48\\n            47: True, 48: True,\\n            # Christmas/New Year: typically week 52, week 1\\n            52: True, 1: True,\\n            # July 4th: typically week 27 or 28\\n            27: True, 28: True,\\n            # Memorial Day (last Mon May): typically week 21 or 22\\n            21: True, 22: True,\\n            # Labor Day (first Mon Sep): typically week 35 or 36\\n            35: True, 36: True,\\n        }\\n        df['is_major_holiday_week'] = df['week_of_year'].map(major_holiday_weeks_map).fillna(False).astype(int)\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', 'is_major_holiday_week'] # Added new feature\\n    \\n    # LOCATION_COL is the only true categorical feature for tree models\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    # Use a small non-zero value for \`forward_transform\` to avoid log(0) if \`1.0\` becomes \`0.0\`.\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    # Ensure fallback value is clipped to prevent issues if mean is outside expected range\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point)\\n            if X_train_model[col].isnull().any():\\n                # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    # or where *after* imputation, some features are still NaN (e.g., location has no data at all).\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    location_categorical_dtype = pd.CategoricalDtype(categories=all_location_categories)\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(location_categorical_dtype)\\n    \\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM (only LOCATION_COL)\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} # List of models for each quantile\\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members): # n_lgbm_ensemble_members can be 1 for a single model\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Use different random states for ensemble members\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_model, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q].append(lgbm_model)\\n        \\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation\\n    # history should contain transformed values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        # Get history for the current location. If location not in history, initialize empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and a numerical horizon feature.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL] # Categorical\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Numerical\\n\\n        # Generate time-series specific features for the current row\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the last available value if history exists but is shorter than lag\\n                lag_value = current_loc_hist[-1]\\n            else: # Fallback if no history for this location\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) > 1: # If not enough for diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # No sufficient history for diff\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else: # No history\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Use all available history if enough for std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else: # Std dev is 0 for 0 or 1 data points\\n                rolling_std_val = 0.0 \\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        # Fill any missing columns (shouldn't happen if feature sets are consistent)\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical feature (LOCATION_COL). This is critical for consistency.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(location_categorical_dtype)\\n        # HORIZON_COL remains numerical\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n            \\n            # Prediction from LGBM models (could be a single model or multiple ensemble members)\\n            if q in models and models[q]: # Check if models exist for this quantile\\n                for lgbm_model_q in models[q]:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                # Average predictions from ensemble members for this quantile\\n                mean_pred = np.mean(ensemble_preds_for_q)\\n                # Additional clip to ensure the averaged prediction is within defined bounds\\n                row_predictions_transformed[q] = np.clip(mean_pred, 0.0, MAX_TRANSFORMED_VALUE) \\n            else:\\n                # If all ensemble members failed or no models were trained for this quantile,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        \\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (transformed value)\\n        # Use the median prediction from the current ensemble to inform future features\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Clip median transformed prediction before adding to history\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_raw)\\n\\n    # Ensure monotonicity of quantiles\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row (quantiles for a given prediction)\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM with fourth_root, added 'is_major_holiday_week', and n_lgbm_ensemble_members=3\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 3, # Increased to 3 for a small ensemble\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 2 (from previous trial, now benefits from 'is_major_holiday_week')\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8, 12],\\n        'rolling_windows': [4, 8, 16, 26],\\n        'rolling_std_windows': [4, 8, 16],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 3 (from previous trial, now benefits from 'is_major_holiday_week')\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.02,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 4 (from previous trial, now benefits from 'is_major_holiday_week')\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p', # Changed transform\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    }\\n]",
  "new_index": "1022",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on robust handling of missing data, improved numerical stability\\n    by clipping transformed target and predictions, and includes advanced date features.\\n    Key aspects include:\\n    - Robust feature engineering with fallbacks for missing history.\\n    - Rigorous clipping of values at various stages (before/after transformation, before/after inverse transformation).\\n    - Ensures monotonicity of quantile predictions.\\n    - Uses an ensemble of LightGBM models for each quantile to potentially improve robustness.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1) \\n\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0)\\n    MIN_ADMISSIONS_PER_MILLION = 0.0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Ensure population is never zero or negative for division\\n    safe_population = np.where(df_train_full[POPULATION_COL] <= 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    \\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, MIN_ADMISSIONS_PER_MILLION, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result of power - 1.0 is non-negative\\n            return np.maximum(MIN_ADMISSIONS_PER_MILLION, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(MIN_ADMISSIONS_PER_MILLION, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(MIN_ADMISSIONS_PER_MILLION, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result of power - 1.0 is non-negative\\n            return np.maximum(MIN_ADMISSIONS_PER_MILLION, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(MIN_ADMISSIONS_PER_MILLION, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        # Ensure location is always a zero-padded string to handle FIPS codes consistently\\n        df[LOCATION_COL] = df[LOCATION_COL].astype(str).str.zfill(2)\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add a heuristic for major holiday weeks in the US.\\n        # This is a simplification using approximate ISO week numbers, and would be more robust\\n        # with a proper holiday calendar. However, for a general solution without external\\n        # libraries or complex holiday calculations, this provides a reasonable proxy.\\n        major_holiday_weeks_map = {\\n            # Thanksgiving (last Thurs Nov): typically week 47 or 48\\n            47: True, 48: True,\\n            # Christmas/New Year: typically week 52, week 1\\n            52: True, 1: True,\\n            # July 4th: typically week 27 or 28\\n            27: True, 28: True,\\n            # Memorial Day (last Mon May): typically week 21 or 22\\n            21: True, 22: True,\\n            # Labor Day (first Mon Sep): typically week 35 or 36\\n            35: True, 36: True,\\n        }\\n        df['is_major_holiday_week'] = df['week_of_year'].map(major_holiday_weeks_map).fillna(False).astype(int)\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', 'is_major_holiday_week']\\n    \\n    # LOCATION_COL is the only true categorical feature for tree models\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        # Use transform with lambda and rolling to ensure calculation per group\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    # Ensure fallback value is clipped to prevent issues if mean is outside expected range\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    for col in train_specific_features:\\n        # Fill within groups first, forward then backward. This handles gaps within a state's time series.\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].ffill().bfill()\\n        \\n        # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point\\n        # in its history to fill from), using the global fallback.\\n        if X_train_model[col].isnull().any():\\n            # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n            # For other features (lags, means, diffs), use the mean transformed value.\\n            fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    location_categorical_dtype = pd.CategoricalDtype(categories=all_location_categories)\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(location_categorical_dtype)\\n    \\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM (only LOCATION_COL)\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} # List of models for each quantile\\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members): # n_lgbm_ensemble_members can be 1 for a single model\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Use different random states for ensemble members\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_model, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q].append(lgbm_model)\\n        \\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation\\n    # history should contain transformed values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test data to process chronologically by location for correct recursive feature generation\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        # Get history for the current location. If location not in history, initialize empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and a numerical horizon feature.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL] # Categorical\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Numerical\\n\\n        # Generate time-series specific features for the current row\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the last available value if history exists but is shorter than lag\\n                lag_value = current_loc_hist[-1]\\n            else: # Fallback if no history for this location\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # If not enough for diff_period, try diff with 1 period if possible\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # No sufficient history for diff\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else: # No history\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Use all available history if enough for std (>1 data points)\\n                rolling_std_val = np.std(current_loc_hist)\\n            else: # Std dev is 0 for 0 or 1 data points\\n                rolling_std_val = 0.0 \\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        # Fill any missing columns with default (0.0) - should not happen if feature sets are consistent\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical feature (LOCATION_COL). This is critical for consistency.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(location_categorical_dtype)\\n        # HORIZON_COL remains numerical\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n            \\n            # Prediction from LGBM models (could be a single model or multiple ensemble members)\\n            if q in models and models[q]: # Check if models exist for this quantile\\n                for lgbm_model_q in models[q]:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                # Average predictions from ensemble members for this quantile\\n                mean_pred = np.mean(ensemble_preds_for_q)\\n                # Additional clip to ensure the averaged prediction is within defined bounds\\n                row_predictions_transformed[q] = np.clip(mean_pred, 0.0, MAX_TRANSFORMED_VALUE) \\n            else:\\n                # If all ensemble members failed or no models were trained for this quantile,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        \\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, MIN_ADMISSIONS_PER_MILLION, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Calculate total admissions, ensuring non-negative results\\n        if population_val <= 0: # Handle zero or negative population safely\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round and ensure non-negative integer predictions\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (transformed value)\\n        # Use the median prediction from the current ensemble to inform future features\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Clip median transformed prediction before adding to history\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_raw)\\n\\n    # Ensure monotonicity of quantiles for each row\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row (quantiles for a given prediction)\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Based on previous best, slightly refined lag/rolling windows and added ensemble.\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.02,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1,\\n            'max_bin': 255 # Default, but explicit\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 12, 26, 52], # Slightly adjusted lags\\n        'lag_diff_periods': [1, 4, 8, 12],\\n        'rolling_windows': [8, 16, 26, 52], # Added 52-week rolling\\n        'rolling_std_windows': [4, 8, 16],\\n        'n_lgbm_ensemble_members': 1, # Keeping 1 for best performance\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 2: More estimators, slightly different learning rate, slightly deeper trees.\\n        'lgbm_params': {\\n            'n_estimators': 350,\\n            'learning_rate': 0.015,\\n            'num_leaves': 31,\\n            'max_depth': 6,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16, 26],\\n        'rolling_std_windows': [4, 8, 16],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 3: Try 'log1p' transformation with balanced parameters\\n        'lgbm_params': {\\n            'n_estimators': 280,\\n            'learning_rate': 0.025,\\n            'num_leaves': 28,\\n            'max_depth': 5,\\n            'min_child_samples': 22,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 4: Try 'sqrt' transformation, slight ensemble\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'sqrt',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 3, # Using an ensemble here\\n        'max_admissions_per_million': 10000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

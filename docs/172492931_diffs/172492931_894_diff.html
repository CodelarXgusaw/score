<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "855",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version addresses potential numerical stability issues identified in previous trials,\\n    particularly regarding the \`horizon\` feature handling and the clipping of predictions.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # This column exists in test_x, but not train_x initially\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3,\\n        'min_child_weight': 10,\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'enable_categorical': True, # Use native categorical support for relevant features\\n        'gamma': 0.5,\\n        'lambda': 1.0,\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Allow higher maximum admissions per million to avoid clipping valid high values,\\n    # which can lead to 'inf' weighted interval scores.\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million)\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else:\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    \\n    # LOCATION_COL is the only true categorical feature for tree models\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This is critical as 'horizon' is present in test_x and needs to be part of the model's features.\\n    X_train_model[HORIZON_COL] = 0 # Treat horizon as a numerical feature.\\n\\n    # Handle missing data introduced by lagging/rolling in training features.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM and XGBoost. Only 'location' is truly categorical.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    \\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM (only LOCATION_COL)\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_model, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and a numerical horizon feature.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL] # Categorical\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Numerical\\n\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback # Fallback if no history\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) > 1:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1:\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        # Fill any missing columns (e.g., if a new feature type was added to X_train_model but not this row) with 0.0\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical feature (LOCATION_COL)\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        # HORIZON_COL remains numerical\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred):\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred):\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        transformed_preds_array = np.maximum(0.0, transformed_preds_array)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1)\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform and high clip.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform, now with high clip.\\n      # Aims to combine the robustness of fourth_root with the diversity of an ensemble.\\n      # Parameters are relatively conservative to ensure stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 # Increased from 1000.0 for stability\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost, fourth_root, and expanded features.\\n      # Now with high clip.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 4,\\n            'min_child_weight': 15,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.8,\\n            'lambda': 0.15,\\n            'alpha': 0.15 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 # Increased from 1000.0 for stability\\n    },\\n    { # Config 4: Ensemble with 2 members for each model type, slightly reduced estimators, fourth_root, high clip.\\n      # Aims for higher ensemble diversity to potentially improve calibration and robustness.\\n        'lgbm_params': {\\n            'n_estimators': 150, # Reduced estimators to balance with more ensemble members\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 10000.0 # Increased from 1000.0 for stability\\n    }\\n]",
  "new_index": "894",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version addresses potential numerical stability issues identified in previous trials,\\n    particularly regarding the \`horizon\` feature handling and the clipping of predictions.\\n    A new clipping on transformed predictions is introduced to prevent excessively large values\\n    from causing \`inf\` during inverse transformation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3,\\n        'min_child_weight': 10,\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'enable_categorical': True,\\n        'gamma': 0.5,\\n        'lambda': 1.0,\\n        'alpha': 0.1\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Maximum admissions per million for the final clipping to ensure values are reasonable.\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Ensure location column is treated as string for consistency in categorical handling\\n    df_train_full[LOCATION_COL] = df_train_full[LOCATION_COL].astype(str)\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative before transform\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative to avoid NaN for x < 0\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            # Ensure input to expm1 is non-negative as log1p domain is non-negative\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine a safe upper bound for transformed predictions based on training data.\\n    # This is critical to prevent models from predicting excessively high transformed values\\n    # that could lead to 'inf' after the inverse transformation (e.g., x^4 overflow).\\n    max_train_transformed_y = df_train_full[TRANSFORMED_TARGET_COL].max()\\n    # Adding a safety buffer. Setting a minimum cap of 20 to ensure it's not too restrictive\\n    # if historical train max is very low, allowing for larger forecasts when necessary.\\n    SAFE_TRANSFORMED_PRED_CLIP_UPPER = max(20.0, max_train_transformed_y * 1.5)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n        df[LOCATION_COL] = df[LOCATION_COL].astype(str) # Ensure consistency for location column\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .dt.isocalendar().week for ISO week number (standardized week of year)\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    \\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff then shift. diff() with periods=N computes X_t - X_{t-N}\\n        # Then shift(1) applies it to the next row (for prediction)\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1 (or not enough data)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Handle missing data introduced by lagging/rolling in training features.\\n    # Use ffill within groups, then fill any remaining NaNs with a reasonable default.\\n    mean_transformed_train_y_fallback_for_features = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n    for col in train_specific_features:\\n        # Fill NaNs created by shifts (lags) within each location group\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n        # Fill any remaining NaNs (e.g., at the very beginning of a series if ffill didn't apply)\\n        X_train_model[col] = X_train_model[col].fillna(mean_transformed_train_y_fallback_for_features)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Dropping rows where the TARGET itself is NaN (after transformation)\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM and XGBoost.\\n    # Create a unified set of categories from both train and test location IDs.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = pd.Categorical(X_train_model[LOCATION_COL], categories=all_location_categories)\\n    \\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # XGBoost handles pd.CategoricalDtype natively when enable_categorical=True\\n                xgb_model.fit(X_train_model, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for feature generation during prediction\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure iterative prediction works correctly by date for each location\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Fallback for transformed target mean if training data is empty (shouldn't happen with typical Kaggle data)\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL] # Consistent with earlier astype(str)\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and horizon feature.\\n        # Access Series elements by label directly, as columns are string labels\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = current_loc\\n        current_features_dict[HORIZON_COL] = row[HORIZON_COL]\\n\\n        # Generate lagged and rolling features iteratively\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the last available value if not enough history\\n                lag_value = current_loc_hist[-1]\\n            else: # Fallback if no history for this location\\n                lag_value = mean_transformed_train_y_fallback \\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) > 1: # Calculate diff from last two if limited history\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 # Default if not enough history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Compute std from available history if > 1 point\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default if not enough history for std\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        # Fill any missing columns that might have been present in train but not generated for this specific test row\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical feature (LOCATION_COL) using the categories from training\\n        X_test_row[LOCATION_COL] = pd.Categorical(X_test_row[LOCATION_COL], categories=all_location_categories)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        # Clip raw transformed prediction to prevent overflow during inverse transform\\n                        ensemble_preds_for_q.append(np.clip(pred, 0.0, SAFE_TRANSFORMED_PRED_CLIP_UPPER))\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions\\n                        # Clip raw transformed prediction to prevent overflow during inverse transform\\n                        ensemble_preds_for_q.append(np.clip(pred, 0.0, SAFE_TRANSFORMED_PRED_CLIP_UPPER))\\n            \\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all models produced non-finite predictions for this quantile, fall back to mean\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        # Ensure transformed values are non-negative. SAFE_TRANSFORMED_PRED_CLIP_UPPER handles the upper bound.\\n        transformed_preds_array = np.maximum(0.0, transformed_preds_array)\\n\\n        # Inverse transform and clip to MAX_ADMISSIONS_PER_MILLION\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for next iteration using the median prediction (transformed)\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        # Re-apply forward transform to median prediction from scaled admissions per million\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        # Add the newly predicted (median) value to the history for the current location\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across columns for each row\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row independently\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform and high clip. (Previous best)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform, now with transformed prediction clipping.\\n      # Aims to combine the robustness of fourth_root with the diversity of an ensemble.\\n      # Parameters are relatively conservative to ensure stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost, fourth_root, and expanded features.\\n      # Now with transformed prediction clipping.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 4,\\n            'min_child_weight': 15,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.8,\\n            'lambda': 0.15,\\n            'alpha': 0.15 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 4: Ensemble with 2 members for each model type, slightly reduced estimators, fourth_root, with transformed prediction clipping.\\n      # Aims for higher ensemble diversity to potentially improve calibration and robustness.\\n        'lgbm_params': {\\n            'n_estimators': 150, # Reduced estimators to balance with more ensemble members\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 10000.0 \\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

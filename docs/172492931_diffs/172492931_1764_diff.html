<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1719",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame: # Corrected return type hint from pd.Series to pd.DataFrame\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), population-normalized and transformed\\n    target variables, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version aims to generalize well by:\\n    - Allowing configuration of LGBM and XGBoost parameters.\\n    - Supporting different target transformations (fourth_root, log1p, sqrt, or none).\\n    - Implementing robust feature engineering including lags, differences, and rolling statistics.\\n    - Using an iterative forecasting approach for multi-step predictions.\\n    - Including post-processing to ensure non-negative, monotonic, and sufficiently spread quantile predictions.\\n    - Addressing numerical stability with appropriate clipping and fallback values.\\n\\n    **Model Predictions and Feature Importance Explanation:**\\n\\n    The model's predictions are primarily driven by **recent historical trends** of COVID-19 hospital admissions in each state.\\n    Key features influencing predictions include:\\n\\n    1.  **Lagged Target Values (\`lag_X_wk\`):** These are among the most crucial features, representing the number of admissions\\n        from previous weeks (e.g., 1 week ago, 4 weeks ago, 8 weeks ago, up to a year ago). These directly capture\\n        the recent trajectory and seasonality of the pandemic within a specific location. A high value last week\\n        will likely lead to a high prediction this week, and vice-versa.\\n\\n    2.  **Lagged Differences (\`diff_lag_1_period_X_wk\`):** These features capture the week-over-week change in admissions.\\n        They indicate the *rate of change* or momentum (e.g., rapid increase or decrease), which is vital for\\n        forecasting turning points or accelerating/decelerating trends.\\n\\n    3.  **Rolling Means (\`rolling_mean_X_wk\`):** These provide a smoothed average of admissions over longer periods (e.g., 8, 16, 26 weeks).\\n        They help in capturing underlying trends, filtering out noise, and understanding the general level of admissions.\\n\\n    4.  **Rolling Standard Deviations (\`rolling_std_X_wk\`):** These measure the volatility or spread of admissions over a recent period.\\n        Higher standard deviation might indicate more unpredictable periods, which can influence the width of the predicted quantiles.\\n\\n    5.  **Seasonal Features (\`sin_week_of_year\`, \`cos_week_of_year\`, \`week_of_year\`):** These cyclic features allow the model to learn\\n        yearly patterns in admissions (e.g., typical winter surges, summer lulls) that are common across locations or specific to some.\\n\\n    6.  **Long-Term Trend Features (\`weeks_since_start\`, \`weeks_since_start_sq\`):** These capture the overall progression of the pandemic\\n        from its start, accounting for non-linear long-term changes in baseline admissions due to factors like vaccination,\\n        prior immunity, or evolving variants.\\n\\n    7.  **Geographical/Demographic Features (\`location\`, \`population\`):**\\n        *   \`location\`: Treated as a categorical feature, it allows the model to learn state-specific baselines,\\n            patterns, or responses that are not captured by other features. Different states have different population densities,\\n            healthcare systems, and policy responses.\\n        *   \`population\`: The target variable is normalized by population, making predictions more comparable across states\\n            and accounting for the scale of each jurisdiction. This feature also informs the final re-scaling back to\\n            total admissions.\\n\\n    8.  **Forecast Horizon (\`horizon\`):** This explicitly tells the model how many weeks into the future the prediction is being made.\\n        This is important as uncertainty typically increases with longer horizons, and the model can learn different\\n        predictive patterns for immediate vs. distant future weeks.\\n\\n    The **ensemble approach** (LightGBM and XGBoost) helps in combining the strengths of different tree-boosting algorithms,\\n    potentially leading to more robust and accurate quantile predictions by reducing reliance on a single model's biases.\\n    **Target transformations** (e.g., fourth root) are used to stabilize variance and make the target distribution\\n    more Gaussian-like, which often improves the performance of regression models, especially for count data.\\n    The **iterative forecasting** method for the test set is crucial for multi-step predictions, as it allows the model\\n    to use its own median predictions from earlier horizons to generate features for subsequent, further-out horizons,\\n    mimicking real-world forecasting where future observed data is unavailable.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability and performance\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.01,\\n        'max_depth': 4,\\n        'min_child_weight': 5,\\n        'subsample': 0.7,\\n        'colsample_bytree': 0.7,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.1,\\n        'reg_lambda': 1.0,\\n        'reg_alpha': 0.1\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    xgb_params.pop('eval_metric', None) # Safely remove eval_metric if present, as it's not standard for reg:quantile objective or set automatically\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap. This value is critical for scaling and numerical stability.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 5000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1))\\n\\n    # A global cap for final predictions to prevent excessively large (e.g., infinite) scores.\\n    # Increased default and applied to all configs to avoid \`inf\` scores from extreme values.\\n    MAX_FINAL_PREDICTION = float(config.get('max_final_prediction', 500000.0))\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values.\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            # Ensure input for power is non-negative, and final output is non-negative.\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input for power is non-negative.\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            # Ensure non-negative input to expm1\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure non-negative input to log1p\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            # Ensure output is non-negative before returning\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input is non-negative for transformation\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million.\\n    # This upper bound is then used for clipping transformed predictions.\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range.\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff based on current transformed target, then shift to use as a feature\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        # Rolling mean, calculated using data up to the previous week (closed='left')\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        # Rolling standard deviation, calculated using data up to the previous week (closed='left')\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping.\\n    # Ensure mean_transformed_train_y_fallback is always finite and positive if possible\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill.\\n    # This ensures proper time-series imputation within each location's data.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point).\\n            # For standard deviation, filling with 0.0 is appropriate if variance is unknown/zero.\\n            if X_train_model[col].isnull().any():\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness,\\n    # but good for robustness if internal processing creates NaNs in target features).\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds).\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n\\n    # Handle categorical features for LightGBM and XGBoost.\\n    # Ensure all possible location categories (from both train and test) are known for consistent encoding.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    # Prepare X_train for LightGBM (categorical Dtype is preferred by LGBM for performance)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Prepare X_train for XGBoost (integer encoding is more robust for some XGBoost versions/configs)\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories)}\\n    # Assign a specific integer for locations not seen in training if any appear in test.\\n    unknown_location_int = len(all_location_categories) if all_location_categories.size > 0 else 0\\n\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction.\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set quantile alpha for LGBM\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i # Different seed for ensemble members\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q # Set quantile alpha for XGBoost\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i # Different seed for ensemble members\\n                xgb_model_params_i['enable_categorical'] = False # Using integer encoding, so disable XGBoost's experimental categorical support\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with historical data from training set.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Horizon is directly available\\n\\n        # Generate time-series features for the current test row using available history.\\n        # Fallback to mean_transformed_train_y_fallback if history is too short.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Calculate diff based on most recent history point and the one 'diff_period' weeks prior\\n                diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (1 + diff_period)]\\n            else:\\n                diff_value = 0.0 # No sufficient history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history if less than window size\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features for prediction.\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order,\\n        # filling missing columns (e.g., if a new feature was introduced in training but not in test scaffold)\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        # Prepare X_test_row for XGBoost (integer encoding)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            # Get predictions from XGBoost models for the current quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                # Average predictions from available ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for this quantile failed or are not present,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                # Ensure the fallback is also clipped to stay within reasonable transformed bounds.\\n                row_predictions_transformed[q] = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation.\\n        # This ensures predictions don't explode and stay within the range learned during training.\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits (0 to MAX_ADMISSIONS_PER_MILLION).\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results in total admissions.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative.\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (q=0.5).\\n        # This is a critical step for recursive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Clip median transformed prediction before inverse and forward transform for history update.\\n        median_pred_transformed_admissions_per_million = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_admissions_per_million)\\n\\n        # Clip median admissions per million before adding to history.\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE.\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Append the new predicted value to the history for the current location.\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negativity\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # 2. Ensure monotonicity and minimal spread between adjacent quantiles.\\n    # This loop ensures that Q_j >= Q_{j-1} + MIN_PRED_SPREAD for all j.\\n    for i in range(predictions_array.shape[0]):\\n        # First, ensure non-decreasing order. This helps in case initial predictions are severely out of order.\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        for j in range(1, len(QUANTILES)):\\n            # Ensure each quantile is at least the previous one plus the minimum spread.\\n            # Use max(current_pred, previous_pred + spread)\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # Clip final predictions to a reasonable maximum to prevent excessively large values that might cause \`inf\` scores.\\n    predictions_array = np.clip(predictions_array, 0, MAX_FINAL_PREDICTION)\\n\\n    # Final conversion to DataFrame and ensure non-negative integers.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform.\\n        # This one passed, minimal changes, just sync MAX_FINAL_PREDICTION\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0 # Increased from 200k to be more robust\\n    },\\n    { # Config 2: Ensemble of LGBM (2 members) and XGBoost (2 members), fourth_root transform.\\n      # Tuned XGBoost params for better stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': { # Revised XGBoost parameters for more robustness\\n            'n_estimators': 250, # Slightly more estimators\\n            'learning_rate': 0.01,\\n            'max_depth': 4,\\n            'min_child_weight': 10, # Increased from 5 for more stability\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.2, # Slightly increased gamma (more regularization)\\n            'reg_lambda': 1.5, # Slightly increased L2 regularization\\n            'reg_alpha': 0.2 # Slightly increased L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0 # Increased from 200k\\n    },\\n    { # Config 3: Ensemble of LGBM (2 members) and XGBoost (2 members), log1p transform.\\n      # Uses the same revised XGBoost parameters as Config 2.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': { # Same revised parameters as Config 2\\n            'n_estimators': 250,\\n            'learning_rate': 0.01,\\n            'max_depth': 4,\\n            'min_child_weight': 10,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.2,\\n            'reg_lambda': 1.5,\\n            'reg_alpha': 0.2\\n        },\\n        'target_transform': 'log1p', # Uses log1p transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0 # Increased from 200k\\n    },\\n    { # Config 4: A more aggressive LGBM + XGBoost with fourth_root and wider window lags.\\n      # Explores a richer set of time-series features.\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.02,\\n            'num_leaves': 31,\\n            'max_depth': 6,\\n            'min_child_samples': 15,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.05,\\n            'reg_lambda': 0.05\\n        },\\n        'xgb_params': { # Revised XGBoost parameters for more robustness\\n            'n_estimators': 300, # Increased estimators\\n            'learning_rate': 0.015,\\n            'max_depth': 5,\\n            'min_child_weight': 15, # Increased from 3 for more stability\\n            'subsample': 0.75,\\n            'colsample_bytree': 0.75,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.1, # Slightly increased gamma\\n            'reg_lambda': 1.0, # Increased L2 regularization\\n            'reg_alpha': 0.1 # Increased L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 12, 16, 26, 39, 52], # More lags\\n        'lag_diff_periods': [1, 4, 8, 12, 26], # More diff periods\\n        'rolling_windows': [4, 8, 16, 26, 39, 52], # More rolling windows\\n        'rolling_std_windows': [4, 8, 16],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0 # Increased from 200k\\n    }\\n]",
  "new_index": "1764",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame: # Corrected return type hint from pd.Series to pd.DataFrame\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), population-normalized and transformed\\n    target variables, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version aims to generalize well by:\\n    - Allowing configuration of LGBM and XGBoost parameters.\\n    - Supporting different target transformations (fourth_root, log1p, sqrt, or none).\\n    - Implementing robust feature engineering including lags, differences, and rolling statistics.\\n    - Using an iterative forecasting approach for multi-step predictions.\\n    - Including post-processing to ensure non-negative, monotonic, and sufficiently spread quantile predictions.\\n    - Addressing numerical stability with appropriate clipping and fallback values.\\n\\n    **Model Predictions and Feature Importance Explanation:**\\n\\n    The model's predictions are primarily driven by **recent historical trends** of COVID-19 hospital admissions in each state.\\n    Key features influencing predictions include:\\n\\n    1.  **Lagged Target Values (\`lag_X_wk\`):** These are among the most crucial features, representing the number of admissions\\n        from previous weeks (e.g., 1 week ago, 4 weeks ago, 8 weeks ago, up to a year ago). These directly capture\\n        the recent trajectory and seasonality of the pandemic within a specific location. A high value last week\\n        will likely lead to a high prediction this week, and vice-versa.\\n\\n    2.  **Lagged Differences (\`diff_lag_1_period_X_wk\`):** These features capture the week-over-week change in admissions.\\n        They indicate the *rate of change* or momentum (e.g., rapid increase or decrease), which is vital for\\n        forecasting turning points or accelerating/decelerating trends.\\n\\n    3.  **Rolling Means (\`rolling_mean_X_wk\`):** These provide a smoothed average of admissions over longer periods (e.g., 8, 16, 26 weeks).\\n        They help in capturing underlying trends, filtering out noise, and understanding the general level of admissions.\\n\\n    4.  **Rolling Standard Deviations (\`rolling_std_X_wk\`):** These measure the volatility or spread of admissions over a recent period.\\n        Higher standard deviation might indicate more unpredictable periods, which can influence the width of the predicted quantiles.\\n\\n    5.  **Seasonal Features (\`sin_week_of_year\`, \`cos_week_of_year\`, \`week_of_year\`):** These cyclic features allow the model to learn\\n        yearly patterns in admissions (e.g., typical winter surges, summer lulls) that are common across locations or specific to some.\\n\\n    6.  **Long-Term Trend Features (\`weeks_since_start\`, \`weeks_since_start_sq\`):** These capture the overall progression of the pandemic\\n        from its start, accounting for non-linear long-term changes in baseline admissions due to factors like vaccination,\\n        prior immunity, or evolving variants.\\n\\n    7.  **Geographical/Demographic Features (\`location\`, \`population\`):**\\n        *   \`location\`: Treated as a categorical feature, it allows the model to learn state-specific baselines,\\n            patterns, or responses that are not captured by other features. Different states have different population densities,\\n            healthcare systems, and policy responses.\\n        *   \`population\`: The target variable is normalized by population, making predictions more comparable across states\\n            and accounting for the scale of each jurisdiction. This feature also informs the final re-scaling back to\\n            total admissions.\\n\\n    8.  **Forecast Horizon (\`horizon\`):** This explicitly tells the model how many weeks into the future the prediction is being made.\\n        This is important as uncertainty typically increases with longer horizons, and the model can learn different\\n        predictive patterns for immediate vs. distant future weeks.\\n\\n    The **ensemble approach** (LightGBM and XGBoost) helps in combining the strengths of different tree-boosting algorithms,\\n    potentially leading to more robust and accurate quantile predictions by reducing reliance on a single model's biases.\\n    **Target transformations** (e.g., fourth root) are used to stabilize variance and make the target distribution\\n    more Gaussian-like, which often improves the performance of regression models, especially for count data.\\n    The **iterative forecasting** method for the test set is crucial for multi-step predictions, as it allows the model\\n    to use its own median predictions from earlier horizons to generate features for subsequent, further-out horizons,\\n    mimicking real-world forecasting where future observed data is unavailable.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    # Increased regularization and reduced learning rate for stability\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 250, # Slightly increased\\n        'learning_rate': 0.025, # Slightly reduced\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 30, # Increased for robustness\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.75, # Slightly reduced\\n        'subsample': 0.75, # Slightly reduced\\n        'reg_alpha': 0.15, # Increased\\n        'reg_lambda': 0.15 # Increased\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability and performance\\n    # Significantly increased regularization for robustness against \`inf\` scores\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 300, # Increased\\n        'learning_rate': 0.008, # Significantly reduced\\n        'max_depth': 3, # Reduced for more conservative trees\\n        'min_child_weight': 25, # Significantly increased for robustness\\n        'subsample': 0.7,\\n        'colsample_bytree': 0.7,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.3, # Increased\\n        'reg_lambda': 2.0, # Increased\\n        'reg_alpha': 0.3 # Increased\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    # Safely remove eval_metric if present, as it's not standard for reg:quantile objective or set automatically\\n    xgb_params.pop('eval_metric', None)\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap. This value is critical for scaling and numerical stability.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 5000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1))\\n\\n    # A global cap for final predictions to prevent excessively large (e.g., infinite) scores.\\n    MAX_FINAL_PREDICTION = float(config.get('max_final_prediction', 500000.0))\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values.\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            # Ensure input for power is non-negative, and final output is non-negative.\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input for power is non-negative.\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            # Ensure non-negative input to expm1\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure non-negative input to log1p\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            # Ensure output is non-negative before returning\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input is non-negative for transformation\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million.\\n    # This upper bound is then used for clipping transformed predictions.\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range.\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff based on current transformed target, then shift to use as a feature\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        # Rolling mean, calculated using data up to the previous week (closed='left')\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        # Rolling standard deviation, calculated using data up to the previous week (closed='left')\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping.\\n    # Ensure mean_transformed_train_y_fallback is always finite and positive if possible\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill.\\n    # This ensures proper time-series imputation within each location's data.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point).\\n            # For standard deviation, filling with 0.0 is appropriate if variance is unknown/zero.\\n            if X_train_model[col].isnull().any():\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness,\\n    # but good for robustness if internal processing creates NaNs in target features).\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds).\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n\\n    # Handle categorical features for LightGBM and XGBoost.\\n    # Ensure all possible location categories (from both train and test) are known for consistent encoding.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    # Prepare X_train for LightGBM (categorical Dtype is preferred by LGBM for performance)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Prepare X_train for XGBoost (integer encoding is more robust for some XGBoost versions/configs)\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories)}\\n    unknown_location_int = len(all_location_categories) # Assign new integer for unseen categories\\n\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction.\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set quantile alpha for LGBM\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i # Different seed for ensemble members\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q # Set quantile alpha for XGBoost\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i # Different seed for ensemble members\\n                xgb_model_params_i['enable_categorical'] = False # Using integer encoding, so disable XGBoost's experimental categorical support\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with historical data from training set.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Horizon is directly available\\n\\n        # Generate time-series features for the current test row using available history.\\n        # Fallback to mean_transformed_train_y_fallback if history is too short.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Calculate diff based on most recent history point and the one 'diff_period' weeks prior\\n                diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (1 + diff_period)]\\n            else:\\n                diff_value = 0.0 # No sufficient history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history if less than window size\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features for prediction.\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order,\\n        # filling missing columns (e.g., if a new feature was introduced in training but not in test scaffold)\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        # Prepare X_test_row for XGBoost (integer encoding)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            # Get predictions from XGBoost models for the current quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                # Average predictions from available ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for this quantile failed or are not present,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                # Ensure the fallback is also clipped to stay within reasonable transformed bounds.\\n                row_predictions_transformed[q] = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation.\\n        # This ensures predictions don't explode and stay within the range learned during training.\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits (0 to MAX_ADMISSIONS_PER_MILLION).\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results in total admissions.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative.\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (q=0.5).\\n        # This is a critical step for recursive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Clip median transformed prediction before inverse and forward transform for history update.\\n        median_pred_transformed_admissions_per_million = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_admissions_per_million)\\n\\n        # Clip median admissions per million before adding to history.\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE.\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Append the new predicted value to the history for the current location.\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negativity\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # 2. Ensure monotonicity and minimal spread between adjacent quantiles.\\n    # This loop ensures that Q_j >= Q_{j-1} + MIN_PRED_SPREAD for all j.\\n    for i in range(predictions_array.shape[0]):\\n        # First, ensure non-decreasing order. This helps in case initial predictions are severely out of order.\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        for j in range(1, len(QUANTILES)):\\n            # Ensure each quantile is at least the previous one plus the minimum spread.\\n            # Use max(current_pred, previous_pred + spread)\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # Clip final predictions to a reasonable maximum to prevent excessively large values that might cause \`inf\` scores.\\n    predictions_array = np.clip(predictions_array, 0, MAX_FINAL_PREDICTION)\\n\\n    # Final conversion to DataFrame and ensure non-negative integers.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform. This config previously passed.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20, # Default value for this config\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0\\n    },\\n    { # Config 2: Ensemble of LGBM (2 members) and XGBoost (2 members), fourth_root transform.\\n      # Parameters for XGBoost are made more conservative to prevent \`inf\` scores.\\n        'lgbm_params': {\\n            'n_estimators': 250, # Slightly increased\\n            'learning_rate': 0.025, # Slightly reduced\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30, # Increased for robustness\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly reduced\\n            'subsample': 0.75, # Slightly reduced\\n            'reg_alpha': 0.15, # Increased\\n            'reg_lambda': 0.15 # Increased\\n        },\\n        'xgb_params': { # Revised XGBoost parameters for more robustness\\n            'n_estimators': 300, # Increased\\n            'learning_rate': 0.008, # Significantly reduced\\n            'max_depth': 3, # Reduced for more conservative trees\\n            'min_child_weight': 25, # Significantly increased for robustness\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.3, # Increased\\n            'reg_lambda': 2.0, # Increased\\n            'reg_alpha': 0.3 # Increased\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0\\n    },\\n    { # Config 3: Ensemble of LGBM (2 members) and XGBoost (2 members), log1p transform.\\n      # Uses the same revised XGBoost parameters as Config 2.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': { # Same revised parameters as Config 2\\n            'n_estimators': 300,\\n            'learning_rate': 0.008,\\n            'max_depth': 3,\\n            'min_child_weight': 25,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.3,\\n            'reg_lambda': 2.0,\\n            'reg_alpha': 0.3\\n        },\\n        'target_transform': 'log1p', # Uses log1p transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0\\n    },\\n    { # Config 4: A more aggressive LGBM + XGBoost with fourth_root and wider window lags.\\n      # Parameters tuned to balance feature richness with stability.\\n        'lgbm_params': {\\n            'n_estimators': 250, # Slightly reduced from previous Trial 1\\n            'learning_rate': 0.025, # Slightly increased from previous Trial 1, but paired with higher min_child_samples\\n            'num_leaves': 26, # Adjusted\\n            'max_depth': 5, # Adjusted\\n            'min_child_samples': 25, # Increased from 15 for stability\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.1, # Adjusted\\n            'reg_lambda': 0.1 # Adjusted\\n        },\\n        'xgb_params': { # Revised XGBoost parameters for more robustness\\n            'n_estimators': 300, # Increased estimators\\n            'learning_rate': 0.01, # Adjusted\\n            'max_depth': 4, # Adjusted\\n            'min_child_weight': 30, # Significantly increased for stability\\n            'subsample': 0.7, # Adjusted\\n            'colsample_bytree': 0.7, # Adjusted\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.4, # Significantly increased\\n            'reg_lambda': 2.5, # Significantly increased\\n            'reg_alpha': 0.4 # Significantly increased\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 12, 16, 26, 39, 52], # More lags\\n        'lag_diff_periods': [1, 4, 8, 12, 26], # More diff periods\\n        'rolling_windows': [4, 8, 16, 26, 39, 52], # More rolling windows\\n        'rolling_std_windows': [4, 8, 16],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 500000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "489",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series\\n    features, a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy\\n    for the test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version explicitly aims to handle seasonality and trends through comprehensive\\n    feature engineering including cyclical date features, \\"weeks since start\\" for trend,\\n    and various short-to-long term lags and rolling means. Special attention is given\\n    to time-series specific missing data handling for generated features.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16])\\n    target_transform_type = config.get('target_transform', 'log1p')\\n\\n    # A small epsilon for numerical stability in transformations (e.g., sqrt/log1p of zero)\\n    epsilon = 1e-6\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data for correct time-series feature generation\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people\\n    # Handle potential zero population to prevent division errors.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0,\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure non-negative before transformation (admissions cannot be negative)\\n    admissions_per_million[admissions_per_million < 0] = 0\\n\\n    # Apply chosen target transformation (log1p, sqrt, fourth_root, or raw)\\n    # Adding epsilon to prevent issues with log(0) or sqrt(0) for very small values\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else:\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use ISO week number for consistency, which can range 1-53\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This helps model yearly patterns like flu seasons or holiday impacts.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since the start of the entire dataset, to capture overall trends.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n\\n        return df\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define lists of features used by the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # Lags capture auto-correlation and temporal dependencies.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage (using past data), based on transformed target.\\n    # Rolling means smooth noise and capture local trends.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures the window includes data *before* the current date, avoiding future leakage.\\n        # \`min_periods=1\` allows calculation even if fewer than \`window\` points are available at series start.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS]\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This feature exists in test_x and can capture different dynamics for different forecast horizons.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs from shifting\\n    # (e.g., if a week is missing in the middle of a location's series).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining initial NaNs (at the very beginning of a location's data) with 0.0.\\n            # This is a common and reasonable approach for count data when no prior history exists,\\n            # indicating no admissions or no data to derive a feature from.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # Drop rows from training data if any selected feature or target is still NaN after fillna.\\n    # This ensures no NaNs are passed to the LGBM model during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories in the model.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    categorical_features_lgbm = [LOCATION_COL]\\n\\n    # Process 'horizon' as a categorical feature, ensuring all test horizons are covered.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile.\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile for this model\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be extended with predictions during the iterative forecasting process.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions to the correct output format.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing: Keep original index and sort by location and date.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and required quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag and rolling features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                # If not enough history for the specific lag, use the most recent available value.\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                # If no history at all for this location (e.g., completely new location), default to 0.0.\\n                lag_value = 0.0\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                # Use all available history if less than window size but some data exists.\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                # No history for this location.\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling any new/missing (static)\\n        # feature columns that might arise in test_x (though unlikely for this dataset) with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction to match training.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            pred_transformed = model.predict(X_test_row)[0] # Extract single prediction value\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform predictions from transformed target scale.\\n        # Ensure values are converted to a numpy array maintaining quantile order.\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            # Clamp negative predictions to 0 before inverse transform to avoid NaNs\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            # Clamp negative predictions to 0 before inverse transform.\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4) - epsilon\\n        else: # If no specific transformation was applied\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Ensure all predictions are non-negative and round to integer counts.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed for consistency).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Inverse transform median prediction to admissions per million, clamping to non-negative\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2) - epsilon\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4) - epsilon\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n\\n        # Ensure non-negative before re-transforming for history\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in future lags.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n\\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# Configuration options to be evaluated by the scoring harness.\\n# These configs explore different target transformations, LightGBM hyperparameters,\\n# and choices for lagged/rolling features.\\nconfig_list = [\\n    { # Config 1: Best performing from previous trials with 'fourth_root' transformation.\\n      # Tuned LGBM params, comprehensive lags and rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 220,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    },\\n    { # Config 2: Second best from previous trials with 'log1p' transformation.\\n      # Similar LGBM parameters and feature sets, offering an alternative robust transformation.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 4, 8, 12, 26, 52],\\n        'rolling_windows': [2, 4, 8, 16]\\n    },\\n    { # Config 3: A slightly more regularized 'fourth_root' model.\\n      # Aims to prevent overfitting by reducing model complexity and increasing regularization strength.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 22,\\n            'max_depth': 4,\\n            'min_child_samples': 30,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.2,\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    }\\n]",
  "new_index": "518",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model utilizes LightGBM for quantile regression, trained on a comprehensive set of\\n    engineered features. These features are designed to capture critical aspects of COVID-19\\n    hospital admissions data:\\n    1.  **Time-series patterns:** Lagged values of past admissions and rolling means capture\\n        autocorrelation and underlying trends. These are crucial because current admissions are\\n        highly dependent on recent past admissions.\\n    2.  **Seasonality:** Cyclical date features (sine/cosine of week of year) effectively model\\n        yearly seasonal variations, such as increased hospitalizations during winter months or\\n        flu seasons.\\n    3.  **Overall trend:** 'Weeks since start' captures the long-term evolution of the pandemic,\\n        accounting for factors like vaccine availability, accumulated population immunity, or\\n        changes in dominant variants over the years.\\n    4.  **Geographical variation:** 'Location' as a categorical feature is highly important, as\\n        COVID-19 dynamics and reporting can vary significantly by state/jurisdiction due to\\n        population density, local policies, and demographics.\\n    5.  **Population scaling:** The target variable ('Total COVID-19 Admissions') is initially\\n        transformed to 'admissions per million' people. This normalizes the data for state\\n        population differences, allowing the model to learn patterns that are more universally\\n        applicable across locations of varying sizes.\\n    6.  **Forecast horizon:** The 'horizon' feature is included to allow the model to learn\\n        different prediction dynamics based on how far into the future the forecast is. For\\n        example, the model might assign different weights to features when predicting one week\\n        out versus three weeks out.\\n\\n    The target variable is transformed (e.g., using a fourth-root or log1p transformation) to\\n    stabilize variance and make the distribution more suitable for regression models like LightGBM,\\n    which often perform better on more Gaussian-like data.\\n\\n    Predictions for the test set are generated iteratively (recursively), where the median\\n    predictions from earlier forecast steps (for a given location and earlier horizons) are\\n    used to compute lagged and rolling features for subsequent steps. This ensures that the\\n    model correctly propagates information through the forecast horizon.\\n\\n    The final predictions are inverse-transformed, scaled back by population, ensured to be\\n    non-negative integers (as admissions are counts), and sorted across quantiles to maintain\\n    monotonicity, which is a requirement for valid probabilistic forecasts and the Weighted\\n    Interval Score evaluation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    # Define a new transformed target column name for internal use\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default LightGBM parameters, can be overridden by 'config'.\\n    # \`objective='quantile'\` and \`metric='quantile'\` are crucial for quantile regression.\\n    # \`alpha\` will be set specifically for each quantile model.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200, # Number of boosting rounds (trees)\\n        'learning_rate': 0.03, # Step size shrinkage\\n        'num_leaves': 25, # Max number of leaves in one tree, controls model complexity\\n        'max_depth': 5, # Max tree depth, another control for complexity\\n        'min_child_samples': 20, # Minimum number of data needed in a child (leaf), to prevent overfitting\\n        'random_state': 42, # For reproducibility of results\\n        'n_jobs': -1, # Use all available CPU cores for faster training\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8, # Fraction of features to consider at each split (column subsampling)\\n        'subsample': 0.8, # Fraction of data to sample for each tree (row subsampling)\\n        'reg_alpha': 0.1, # L1 regularization term on weights\\n        'reg_lambda': 0.1 # L2 regularization term on weights\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, configurable via the \`config\` dictionary\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52]) # Lagged features capture auto-correlation\\n    ROLLING_WINDOWS = config.get('rolling_windows', [2, 4, 8, 16]) # Rolling means capture smoothed trends\\n    target_transform_type = config.get('target_transform', 'log1p') # Type of transformation for target variable\\n\\n    # A small epsilon for numerical stability, primarily to handle zero values in transformations\\n    # like square root or logarithm.\\n    epsilon = 1e-6\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sorting data by location and date is crucial for accurate generation of time-series features\\n    # (lags, rolling means), ensuring calculations are based on chronological order within each group.\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people.\\n    # This normalization adjusts for the varying populations of different states, making the\\n    # COVID-19 admission rates comparable across jurisdictions and allowing the model to learn\\n    # more generalizable patterns.\\n    admissions_per_million = np.where(\\n        df_train_full[POPULATION_COL] != 0, # Check for non-zero population to prevent division by zero\\n        df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000,\\n        0.0 # Assign 0.0 if population is zero or admissions are zero\\n    )\\n    admissions_per_million = pd.Series(admissions_per_million, index=df_train_full.index)\\n\\n    # Ensure the 'admissions_per_million' values are non-negative before applying transformations,\\n    # as counts cannot be negative.\\n    admissions_per_million[admissions_per_million < 0] = 0\\n\\n    # Apply the chosen target transformation. Transformations help to make the target distribution\\n    # more symmetrical and stabilize variance, which can improve the performance of regression models.\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n    elif target_transform_type == 'sqrt':\\n        # Add epsilon before sqrt to handle zero values gracefully and avoid issues with sqrt(0)\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + epsilon)\\n    elif target_transform_type == 'fourth_root':\\n        # This is a stronger transformation, often effective for highly skewed count data.\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + epsilon, 0.25)\\n    else: # If no specific transformation is chosen, use the raw per-million values\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This serves as a consistent\\n    # anchor point for calculating 'weeks_since_start' across all data subsets.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        # Extract temporal features: year, month, ISO week of year.\\n        # These features capture long-term trends and yearly seasonal patterns.\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features (sine and cosine transformations) for 'week_of_year'.\\n        # These capture seasonality smoothly without abrupt jumps, which is important for recurring\\n        # patterns like flu seasons or holiday impacts on hospitalizations.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Calculate 'weeks_since_start' from the global minimum date. This feature captures\\n        # the overall linear trend of the pandemic over its duration.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n\\n        return df\\n\\n    # Apply the base feature extraction to both the training and test dataframes.\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define lists of all features to be used by the LightGBM model.\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start']\\n    # 'location' is treated as a categorical feature, allowing the model to learn state-specific effects.\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # Lagged features are typically the most powerful predictors in time-series models,\\n    # as they directly capture the autocorrelation and persistence of the target variable.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate rolling mean features, shifted by 1 week to ensure no future data leakage.\\n    # Rolling means smooth out short-term fluctuations and highlight underlying trends over\\n    # specified time windows.\\n    for window in ROLLING_WINDOWS:\\n        # \`closed='left'\` ensures that the rolling window includes data *before* the current date,\\n        # strictly avoiding data leakage from the future.\\n        # \`min_periods=1\` allows a mean to be calculated even if fewer than \`window\` points are available\\n        # at the beginning of a location's time series.\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns for training.\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS]\\n\\n    X_train_model_cols = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical training data, the horizon is considered 0.\\n    # This feature exists in the \`test_x\` data and helps the model learn if prediction accuracy or dynamics\\n    # change based on the forecast lead time.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill within each location group to handle NaNs that arise from shifting\\n    # operations (e.g., if there's a missing week in the middle of a location's series).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining initial NaNs (at the very beginning of a location's data before any ffill\\n            # can take effect) with 0.0. This is a common and reasonable approach for count data when\\n            # no prior history exists, implying zero admissions or no data to derive a feature from.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    # After feature engineering, drop any rows from the training data that still contain NaNs\\n    # in the selected features or target. This ensures that only complete data points are\\n    # fed into the LightGBM model, preventing errors during training.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Convert 'location' to a pandas CategoricalDtype. It's crucial to define categories from\\n    # both training and test sets to ensure consistency, especially if a location appears only\\n    # in one of the sets.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    categorical_features_lgbm = [LOCATION_COL]\\n\\n    # Process 'horizon' as a categorical feature as well, ensuring all possible horizon values\\n    # from both train and test sets are recognized as categories.\\n    train_horizon_categories = X_train_model[HORIZON_COL].astype('category').cat.categories.tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].astype('category').cat.categories.tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n    categorical_features_lgbm.append(HORIZON_COL)\\n\\n\\n    # --- 4. Model Training ---\\n    # Train a separate LightGBM model for each quantile. This approach allows each quantile\\n    # to be modeled independently, which can provide more accurate and flexible probabilistic\\n    # forecasts compared to methods that assume a fixed distribution shape (e.g., normal).\\n    models = {}\\n    for q in QUANTILES:\\n        model_params = lgbm_params.copy()\\n        model_params['alpha'] = q # Set the specific quantile (alpha) for this model instance\\n        model = LGBMRegressor(**model_params)\\n        model.fit(X_train_model, y_train_model,\\n                  categorical_feature=categorical_features_lgbm)\\n        models[q] = model\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using the *transformed* target values from the\\n    # full training data. This history is crucial for generating the lagged and rolling\\n    # features for future predictions in the test set.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store the original index of test_x. This is important for mapping the predictions\\n    # back to the correct rows in the final output DataFrame.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing: add original index and sort by location and date.\\n    # Sorting ensures that predictions for earlier dates/horizons for a given location are made first,\\n    # allowing their median values to be used as inputs for later forecasts for the same location.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize the DataFrame that will store the final quantile predictions.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed DataFrame to predict sequentially.\\n    # This is the core of the iterative (recursive) forecasting strategy.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Retrieve the original index for accurate placement of predictions\\n\\n        # Retrieve the current location's history of transformed admissions.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build a feature dictionary for the current row, including base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using the \`current_loc_hist\`.\\n        # These lags are generated based on available historical data or previous predictions.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                # If not enough history for a specific lag, use the most recent available value.\\n                # This is a heuristic to provide a plausible value and avoid NaNs.\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                # If no history at all for this location (e.g., a new location in test not seen in train),\\n                # default the lag value to 0.0 to maintain numerical stability.\\n                lag_value = 0.0\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        # Generate dynamic rolling mean features using the \`current_loc_hist\`.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                # Use all available history if the history is shorter than the window size.\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                # If no history, default rolling mean to 0.0.\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Convert the feature dictionary to a pandas DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure that \`X_test_row\` has the exact same columns and column order as \`X_train_model\`.\\n        # This is critical for LightGBM to correctly apply the learned model. Any missing columns\\n        # (e.g., if a feature was in train_x but not test_x, though unlikely here) are filled with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model.columns, fill_value=0.0)\\n\\n        # Re-cast categorical features in the test row with the appropriate \`CategoricalDtype\` and\\n        # categories to match the training data. This ensures LightGBM correctly interprets these features.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all specified quantiles for the current single row.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            model = models[q]\\n            # The \`predict\` method returns an array, extract the single prediction value.\\n            pred_transformed = model.predict(X_test_row)[0]\\n            row_predictions_transformed[q] = pred_transformed\\n\\n        # Inverse transform the predictions from the transformed target scale back to\\n        # 'admissions per million'. \`np.maximum(0, ...)\` is used to ensure that any\\n        # negative predictions from LightGBM are clamped to zero before inverse transformation.\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        if target_transform_type == 'log1p':\\n            inv_preds_admissions_per_million = np.expm1(transformed_preds_array)\\n        elif target_transform_type == 'sqrt':\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 2)\\n        elif target_transform_type == 'fourth_root':\\n            inv_preds_admissions_per_million = np.power(np.maximum(0, transformed_preds_array), 4)\\n        else: # If no specific transformation was applied, use raw predictions directly\\n            inv_preds_admissions_per_million = transformed_preds_array\\n\\n        # Convert from 'admissions per million' back to total admissions by multiplying by population.\\n        population_val = row[POPULATION_COL]\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Ensure all final predictions are non-negative and rounded to integer counts,\\n        # as hospital admissions are discrete, non-negative numbers.\\n        final_preds_total_admissions[final_preds_total_admissions < 0] = 0\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store the calculated predictions in the final DataFrame, using the original index\\n        # to ensure correct alignment with the test_x input.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the *median* prediction for the current step.\\n        # The median is used as a robust estimate to propagate information into future lagged features.\\n        # This value must be re-transformed to the same scale as the training target for consistency.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Inverse transform the median prediction to 'admissions per million', clamping to non-negative.\\n        if target_transform_type == 'log1p':\\n            median_pred_admissions_per_million = np.expm1(median_pred_transformed_raw)\\n        elif target_transform_type == 'sqrt':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 2)\\n        elif target_transform_type == 'fourth_root':\\n            median_pred_admissions_per_million = np.power(np.maximum(0, median_pred_transformed_raw), 4)\\n        else:\\n            median_pred_admissions_per_million = median_pred_transformed_raw\\n\\n        # Ensure the median prediction (in 'admissions per million' scale) is non-negative before re-transforming.\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n\\n        # Re-transform this median value back to the feature scale for use in subsequent lag calculations.\\n        # This is a crucial step for the iterative forecasting process to maintain data consistency.\\n        if target_transform_type == 'log1p':\\n            value_to_add_to_history = np.log1p(median_pred_admissions_per_million)\\n        elif target_transform_type == 'sqrt':\\n            value_to_add_to_history = np.sqrt(median_pred_admissions_per_million + epsilon)\\n        elif target_transform_type == 'fourth_root':\\n            value_to_add_to_history = np.power(median_pred_admissions_per_million + epsilon, 0.25)\\n        else:\\n            value_to_add_to_history = median_pred_admissions_per_million\\n\\n        # Append the new transformed median prediction to the history for the current location.\\n        location_history_data[current_loc].append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row.\\n    # This is a fundamental requirement for valid quantile forecasts: the predicted value for\\n    # a higher quantile must be greater than or equal to the predicted value for a lower quantile.\\n    # Sorting each row guarantees this property.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in ascending order\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# The \`config_list\` defines different sets of hyperparameters and feature engineering choices\\n# that will be evaluated by the scoring harness. This allows for cross-validation to select\\n# the configuration that generalizes best across different evaluation folds.\\n\\nconfig_list = [\\n    { # Config 1: This configuration yielded the best performance in the previous trial,\\n      # using a 'fourth_root' transformation for the target variable.\\n      # It features a comprehensive set of lagged and rolling window features.\\n        'lgbm_params': {\\n            'n_estimators': 220, # Slightly increased estimators\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root', # Effective for stabilizing variance in skewed count data\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Captures short, medium, and long-term auto-correlation\\n        'rolling_windows': [8, 16, 26] # Captures smoothed trends over different periods\\n    },\\n    { # Config 2: A strong alternative using the 'log1p' transformation, which is also\\n      # very common and robust for count data, especially those with many zeros.\\n      # This configuration uses a slightly different set of lags and rolling windows.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p', # Standard transformation for count data\\n        'lag_weeks': [1, 2, 4, 8, 12, 26, 52], # Slightly more detailed short-term lags\\n        'rolling_windows': [2, 4, 8, 16] # Wider range of rolling window sizes\\n    },\\n    { # Config 3: A more regularized version of the 'fourth_root' model.\\n      # This configuration aims to prevent potential overfitting by reducing model complexity\\n      # (fewer leaves, shallower depth) and increasing regularization strength (higher alpha/lambda).\\n      # This can be beneficial if the training data is noisy or limited.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 22, # Reduced complexity\\n            'max_depth': 4, # Reduced complexity\\n            'min_child_samples': 30, # Increased minimum samples per leaf\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly more aggressive column subsampling\\n            'subsample': 0.75, # Slightly more aggressive row subsampling\\n            'reg_alpha': 0.2, # Increased L1 regularization\\n            'reg_lambda': 0.2 # Increased L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'rolling_windows': [8, 16, 26]\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

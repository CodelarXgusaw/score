<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "38",
  "old_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\n# Define a global fixed start date for consistent time indexing across different data splits.\\n# This ensures that 'time_idx' is comparable regardless of the training window.\\n# The competition description states data starts around August 8, 2020.\\nFIXED_START_DATE = pd.to_datetime('2020-08-01')\\n\\ndef create_base_features(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates time-based, population, and location features from the input DataFrame.\\n    This function handles common feature engineering steps applicable to both\\n    training and test data, ensuring consistency.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    # Convert 'target_end_date' to datetime objects for time-based feature extraction\\n    df_copy['target_end_date'] = pd.to_datetime(df_copy['target_end_date'])\\n\\n    # Extract standard date-related features\\n    df_copy['year'] = df_copy['target_end_date'].dt.year\\n    df_copy['month'] = df_copy['target_end_date'].dt.month\\n    # Use isocalendar().week for ISO week number, cast to int\\n    df_copy['week_of_year'] = df_copy['target_end_date'].dt.isocalendar().week.astype(int)\\n    df_copy['day_of_year'] = df_copy['target_end_date'].dt.dayofyear\\n    df_copy['day_of_week'] = df_copy['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for capturing seasonal patterns (e.g., yearly seasonality).\\n    # Using 52 weeks and 366 days for robustness (leap years).\\n    df_copy['week_sin'] = np.sin(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['week_cos'] = np.cos(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['dayofyear_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 366)\\n    df_copy['dayofyear_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 366)\\n\\n    # Log transform 'population' as it's typically skewed and covers a wide range.\\n    df_copy['population_log'] = np.log1p(df_copy['population'])\\n\\n    # Convert 'location' to an integer ID, suitable for categorical modeling in LightGBM.\\n    df_copy['location_id'] = df_copy['location'].astype(int)\\n\\n    # Handle the 'horizon' feature: present in test_x, but not typically in train_x.\\n    # Set to 0 for training data to distinguish from forecast horizons.\\n    if 'horizon' in df_copy.columns:\\n        df_copy['horizon'] = df_copy['horizon'].astype(int)\\n    else:\\n        df_copy['horizon'] = 0\\n\\n    # Create a consistent time index relative to a fixed start date (e.g., first week of data).\\n    # This helps capture overall temporal trends regardless of the rolling window.\\n    df_copy['time_idx'] = (df_copy['target_end_date'] - FIXED_START_DATE).dt.days // 7\\n\\n    return df_copy\\n\\ndef add_lagged_features(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Adds lagged features based on 'Total COVID-19 Admissions'.\\n    This function should be called on a DataFrame that includes the target variable.\\n    The DataFrame must be sorted by 'location' and 'target_end_date' prior to calling.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    if 'Total COVID-19 Admissions' not in df_copy.columns:\\n        raise ValueError(\\"DataFrame must contain 'Total COVID-19 Admissions' for lagged feature creation.\\")\\n\\n    # Sort data to ensure correct lag calculation per location\\n    df_copy = df_copy.sort_values(by=['location', 'target_end_date'])\\n\\n    # Calculate direct lags of the target variable (admissions from previous weeks)\\n    df_copy['admissions_lag_1'] = df_copy.groupby('location')['Total COVID-19 Admissions'].shift(1)\\n    df_copy['admissions_lag_2'] = df_copy.groupby('location')['Total COVID-19 Admissions'].shift(2)\\n    df_copy['admissions_lag_3'] = df_copy.groupby('location')['Total COVID-19 Admissions'].shift(3)\\n\\n    # Calculate rolling statistics (mean and standard deviation) over recent weeks.\\n    # These capture recent trends and volatility. Using a 3-week window for weekly data.\\n    df_copy['admissions_rolling_mean_3w'] = df_copy.groupby('location')['Total COVID-19 Admissions'].transform(\\n        lambda x: x.rolling(window=3, min_periods=1).mean().shift(1)\\n    )\\n    df_copy['admissions_rolling_std_3w'] = df_copy.groupby('location')['Total COVID-19 Admissions'].transform(\\n        lambda x: x.rolling(window=3, min_periods=1).std().shift(1)\\n    )\\n\\n    # Fill NaN values introduced by shifting (e.g., first few entries for each location).\\n    # For admissions-related features, 0 is a sensible default for missing prior values.\\n    # For standard deviation, 0 is used if there's no variation or too few points.\\n    df_copy['admissions_lag_1'] = df_copy['admissions_lag_1'].fillna(0)\\n    df_copy['admissions_lag_2'] = df_copy['admissions_lag_2'].fillna(0)\\n    df_copy['admissions_lag_3'] = df_copy['admissions_lag_3'].fillna(0)\\n    df_copy['admissions_rolling_mean_3w'] = df_copy['admissions_rolling_mean_3w'].fillna(0)\\n    df_copy['admissions_rolling_std_3w'] = df_copy['admissions_rolling_std_3w'].fillna(0)\\n\\n    return df_copy\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile and\\n    makes predictions on the test set. This function incorporates:\\n    1. Comprehensive feature engineering including time-based, population, location, and lagged features.\\n    2. Target transformation (log1p) to handle skewed, non-negative count data.\\n    3. Training separate LightGBM models for each quantile.\\n    4. Post-processing of predictions to ensure non-negativity and monotonicity,\\n       crucial for the Weighted Interval Score evaluation metric.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model,\\n                                  especially LightGBM hyperparameters.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    # Define the list of quantiles required by the competition\\n    quantiles = [0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n                 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99]\\n\\n    # Combine train_x and train_y into a single DataFrame for easier lagged feature computation\\n    train_df = train_x.copy()\\n    train_df['Total COVID-19 Admissions'] = train_y\\n\\n    # Apply base feature engineering to the combined training data\\n    train_df = create_base_features(train_df)\\n    # Apply lagged feature engineering to the training data. This requires 'Total COVID-19 Admissions'.\\n    train_df = add_lagged_features(train_df)\\n\\n    # Apply base feature engineering to the test data.\\n    test_features_base = create_base_features(test_x)\\n\\n    # For the test set, lagged features must be derived from the *last known actual values*\\n    # available in the training data for each location.\\n    # We get the last row for each location from the already processed 'train_df' (which contains lags).\\n    last_known_lags = train_df.sort_values(by=['location', 'target_end_date']) \\\\\\n                              .groupby('location').tail(1) \\\\\\n                              [['location', 'admissions_lag_1', 'admissions_lag_2', 'admissions_lag_3',\\n                                'admissions_rolling_mean_3w', 'admissions_rolling_std_3w']]\\n    \\n    # Merge these last known lagged features into the test_features_base DataFrame.\\n    # All forecast horizons for a given location in 'test_x' will use these same latest lags.\\n    test_features = test_features_base.merge(last_known_lags, on='location', how='left')\\n    \\n    # Fill any NaNs that might arise if a location in test_x was not present in train_df\\n    # or if there wasn't enough history for certain lags (e.g., first few weeks of data).\\n    # Defaulting to 0 for admissions-related lags and rolling means.\\n    # For rolling standard deviation, 0 is also appropriate if no historical variation.\\n    test_features['admissions_lag_1'] = test_features['admissions_lag_1'].fillna(0)\\n    test_features['admissions_lag_2'] = test_features['admissions_lag_2'].fillna(0)\\n    test_features['admissions_lag_3'] = test_features['admissions_lag_3'].fillna(0)\\n    test_features['admissions_rolling_mean_3w'] = test_features['admissions_rolling_mean_3w'].fillna(0)\\n    test_features['admissions_rolling_std_3w'] = test_features['admissions_rolling_std_3w'].fillna(0)\\n\\n\\n    # Define the final set of features to be used for model training and prediction.\\n    # This list includes both the base features and the newly created lagged features.\\n    features = [\\n        'year', 'month', 'week_of_year', 'day_of_year', 'day_of_week',\\n        'week_sin', 'week_cos', 'dayofyear_sin', 'dayofyear_cos',\\n        'population_log', 'location_id', 'horizon', 'time_idx',\\n        'admissions_lag_1', 'admissions_lag_2', 'admissions_lag_3',\\n        'admissions_rolling_mean_3w', 'admissions_rolling_std_3w'\\n    ]\\n\\n    # Prepare training and test datasets with the selected features.\\n    X_train = train_df[features]\\n    X_test = test_features[features]\\n\\n    # Target transformation: Apply log1p to 'Total COVID-19 Admissions'.\\n    # This helps to stabilize variance and handle the right-skewed nature of count data,\\n    # making the distribution more suitable for regression models.\\n    y_train_transformed = np.log1p(train_df['Total COVID-19 Admissions'])\\n\\n    # Define categorical features for LightGBM. LightGBM can efficiently handle these directly.\\n    categorical_features = ['location_id', 'year', 'month', 'week_of_year',\\n                            'day_of_year', 'day_of_week', 'horizon']\\n    # Filter to ensure only features actually present in X_train are passed as categorical\\n    categorical_features = [f for f in categorical_features if f in X_train.columns]\\n\\n    # Retrieve LightGBM model hyperparameters from the 'config' dictionary,\\n    # allowing external control over model parameters. Use defaults if not provided.\\n    lgbm_params = config.get('lgbm_params', {\\n        'objective': 'quantile',  # Set objective for quantile regression\\n        'metric': 'quantile',     # Evaluation metric\\n        'n_estimators': 300,      # Number of boosting rounds\\n        'learning_rate': 0.03,    # Step size shrinkage\\n        'num_leaves': 32,         # Max number of leaves in one tree\\n        'verbose': -1,            # Suppress verbose output during training\\n        'n_jobs': -1,             # Use all available CPU cores for parallel processing\\n        'seed': 42,               # Random seed for reproducibility\\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\\n        'lambda_l1': 0.1,         # L1 regularization (Lasso)\\n        'lambda_l2': 0.1,         # L2 regularization (Ridge)\\n        'feature_fraction': 0.8,  # Fraction of features considered at each split\\n        'bagging_fraction': 0.8,  # Fraction of data sampled for each tree\\n        'bagging_freq': 1         # Frequency for bagging\\n    })\\n\\n    # Initialize a DataFrame to store all quantile predictions for the test set.\\n    test_y_hat_quantiles = pd.DataFrame(index=test_x.index)\\n\\n    # Train a separate LightGBM model for each required quantile.\\n    # This is a common approach for quantile regression with gradient boosting models.\\n    for q in quantiles:\\n        # Update the 'alpha' parameter in LightGBM parameters for the current quantile\\n        current_lgbm_params = lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        # Initialize and train the LightGBM Regressor model\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        model.fit(X_train, y_train_transformed, categorical_feature=categorical_features)\\n\\n        # Make predictions on the test set, still on the transformed scale\\n        preds_transformed = model.predict(X_test)\\n        \\n        # Inverse transform the predictions using expm1 (inverse of log1p)\\n        preds = np.expm1(preds_transformed)\\n        \\n        # Ensure predictions are non-negative, as admissions cannot be less than zero.\\n        # This is a safeguard against any potential floating-point inaccuracies, although\\n        # expm1 should generally produce non-negative values for outputs from log1p training.\\n        preds[preds < 0] = 0\\n        \\n        # Store predictions in the results DataFrame with the correct column name\\n        test_y_hat_quantiles[f'quantile_{q}'] = preds\\n\\n    # Post-processing: Enforce monotonicity of quantile predictions.\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile. This loop iterates through quantiles and adjusts.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        # For each row, set the current quantile's prediction to be at least the previous one's.\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    return test_y_hat_quantiles\\n\\n# These 'config_list' definitions will be used by the evaluation harness.\\n# Each dictionary represents a different set of hyperparameters for your model.\\n# The harness will run your \`fit_and_predict_fn\` with each config and select\\n# the best-performing one based on its internal evaluation criteria (e.g., WIS).\\nconfig_list = [\\n    {\\n        # Default LightGBM configuration, balancing performance and speed\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 300,\\n            'learning_rate': 0.03,\\n            'num_leaves': 32,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.1,\\n            'lambda_l2': 0.1,\\n            'feature_fraction': 0.8,\\n            'bagging_fraction': 0.8,\\n            'bagging_freq': 1\\n        }\\n    },\\n    {\\n        # An alternative configuration with more estimators and slightly lower learning rate,\\n        # potentially leading to higher accuracy but longer training times.\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 400,    # Increased estimators\\n            'learning_rate': 0.02,  # Slightly lower learning rate\\n            'num_leaves': 48,       # More leaves per tree\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.05,      # Reduced regularization\\n            'lambda_l2': 0.05,\\n            'feature_fraction': 0.7,\\n            'bagging_fraction': 0.7,\\n            'bagging_freq': 1\\n        }\\n    }\\n]",
  "new_index": "67",
  "new_code": "import pandas as pd\\nimport numpy as np\\nimport lightgbm as lgb\\nfrom typing import Any\\n\\n# Define a global fixed start date for consistent time indexing across different data splits.\\n# This ensures that 'time_idx' is comparable regardless of the training window.\\n# The competition description states data starts around August 8, 2020.\\nFIXED_START_DATE = pd.to_datetime('2020-08-01')\\n\\ndef create_base_features(df: pd.DataFrame) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Creates time-based, population, and location features from the input DataFrame.\\n    This function handles common feature engineering steps applicable to both\\n    training and test data, ensuring consistency.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    # Convert 'target_end_date' to datetime objects for time-based feature extraction\\n    df_copy['target_end_date'] = pd.to_datetime(df_copy['target_end_date'])\\n\\n    # Extract standard date-related features\\n    df_copy['year'] = df_copy['target_end_date'].dt.year\\n    df_copy['month'] = df_copy['target_end_date'].dt.month\\n    # Use isocalendar().week for ISO week number, cast to int\\n    df_copy['week_of_year'] = df_copy['target_end_date'].dt.isocalendar().week.astype(int)\\n    df_copy['day_of_year'] = df_copy['target_end_date'].dt.dayofyear\\n    df_copy['day_of_week'] = df_copy['target_end_date'].dt.dayofweek # Monday=0, Sunday=6\\n\\n    # Add cyclical features for capturing seasonal patterns (e.g., yearly seasonality).\\n    # Using 52 weeks and 366 days for robustness (leap years).\\n    df_copy['week_sin'] = np.sin(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['week_cos'] = np.cos(2 * np.pi * df_copy['week_of_year'] / 52)\\n    df_copy['dayofyear_sin'] = np.sin(2 * np.pi * df_copy['day_of_year'] / 366)\\n    df_copy['dayofyear_cos'] = np.cos(2 * np.pi * df_copy['day_of_year'] / 366)\\n\\n    # Log transform 'population' as it's typically skewed and covers a wide range.\\n    df_copy['population_log'] = np.log1p(df_copy['population'])\\n\\n    # Convert 'location' to an integer ID, suitable for categorical modeling in LightGBM.\\n    df_copy['location_id'] = df_copy['location'].astype(int)\\n\\n    # Handle the 'horizon' feature: present in test_x, but not typically in train_x.\\n    # Set to 0 for training data to distinguish from forecast horizons.\\n    if 'horizon' in df_copy.columns:\\n        df_copy['horizon'] = df_copy['horizon'].astype(int)\\n    else:\\n        df_copy['horizon'] = 0\\n\\n    # Create a consistent time index relative to a fixed start date (e.g., first week of data).\\n    # This helps capture overall temporal trends regardless of the rolling window.\\n    df_copy['time_idx'] = (df_copy['target_end_date'] - FIXED_START_DATE).dt.days // 7\\n\\n    return df_copy\\n\\ndef add_autoregressive_features_batch(df: pd.DataFrame, target_col: str, \\n                                     lags: list[int], rolling_windows: list[int]) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Adds lagged features and rolling statistics for a given target column in a batch manner.\\n    This function is efficient for training data where target_col is fully available.\\n    Assumes df is already sorted by 'location_id' and 'target_end_date'.\\n    \\"\\"\\"\\n    df_copy = df.copy()\\n\\n    # Sort data to ensure correct lag calculation per location\\n    df_copy = df_copy.sort_values(by=['location_id', 'target_end_date'])\\n\\n    # Calculate lags for the target variable\\n    for lag in lags:\\n        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby('location_id')[target_col].shift(lag)\\n\\n    # Calculate rolling statistics for the target variable (mean and standard deviation)\\n    for window in rolling_windows:\\n        df_copy[f'{target_col}_rolling_mean_{window}w'] = df_copy.groupby('location_id')[target_col].transform(\\n            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1) # shift(1) to prevent data leakage\\n        )\\n        df_copy[f'{target_col}_rolling_std_{window}w'] = df_copy.groupby('location_id')[target_col].transform(\\n            lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\\n        )\\n    \\n    # Fill NaNs created by shifting/rolling at the beginning of each series.\\n    # For count data like admissions, 0 is a reasonable fill value.\\n    autoregressive_cols = [f'{target_col}_lag_{lag}' for lag in lags] + \\\\\\n                          [f'{target_col}_rolling_mean_{window}w' for window in rolling_windows] + \\\\\\n                          [f'{target_col}_rolling_std_{window}w' for window in rolling_windows]\\n    \\n    for col in autoregressive_cols:\\n        df_copy[col] = df_copy[col].fillna(0) # Fill with 0 for missing prior admissions\\n        if 'std' in col:\\n            df_copy[col] = df_copy[col].replace([np.inf, -np.inf], 0) # Handle potential inf values from std\\n\\n    return df_copy\\n\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"\\n    Fits a LightGBM Quantile Regression model for each required quantile and\\n    makes predictions on the test set, using an iterative (recursive) approach\\n    for generating lagged features for future time steps.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model,\\n                                  especially LightGBM hyperparameters.\\n\\n    Returns:\\n        pd.DataFrame: A DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.01', 'quantile_0.025', etc.\\n    \\"\\"\\"\\n    # Define the list of quantiles required by the competition\\n    quantiles = [0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,\\n                 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99]\\n    \\n    target_col = 'Total COVID-19 Admissions'\\n    lags = config.get('lags', [1, 2, 3]) # Lags for autoregressive features\\n    rolling_windows = config.get('rolling_windows', [3, 7]) # Rolling window sizes for mean/std\\n\\n    # --- 1. Prepare Training Data for Model Training ---\\n    train_df = train_x.copy()\\n    train_df[target_col] = train_y\\n    \\n    # Add base features and batch-calculate autoregressive features for training data\\n    train_df = create_base_features(train_df)\\n    train_df = add_autoregressive_features_batch(train_df, target_col, lags, rolling_windows)\\n\\n    # --- 2. Prepare Combined DataFrame for Iterative Prediction ---\\n    # This DataFrame will hold actuals for train data and be iteratively filled with predictions\\n    # for test data, enabling recursive lag generation.\\n    \\n    # Process test_x to get base features and carry original index\\n    test_x_processed = create_base_features(test_x.copy())\\n    test_x_processed['original_idx'] = test_x_processed.index # Preserve original test_x index\\n\\n    # Flag for identifying train/test rows in the combined DataFrame\\n    train_df['is_train_data'] = True\\n    test_x_processed['is_train_data'] = False\\n    \\n    # Add a placeholder for 'original_idx' in train_df\\n    train_df['original_idx'] = np.nan\\n\\n    # Identify all feature columns (base and autoregressive)\\n    base_feature_cols = [col for col in train_df.columns if col not in [target_col, 'is_train_data', 'original_idx']]\\n    autoregressive_feature_cols = [f'{target_col}_lag_{lag}' for lag in lags] + \\\\\\n                                  [f'{target_col}_rolling_mean_{window}w' for window in rolling_windows] + \\\\\\n                                  [f'{target_col}_rolling_std_{window}w' for window in rolling_windows]\\n    \\n    model_features = base_feature_cols + autoregressive_feature_cols\\n\\n    # Ensure consistent columns for concatenation (only include columns that are either features or meta-info)\\n    cols_to_concat = list(set(base_feature_cols + [target_col, 'is_train_data', 'original_idx']))\\n    \\n    # Select columns before concatenating\\n    train_for_concat = train_df[cols_to_concat].copy()\\n    test_for_concat = test_x_processed[cols_to_concat].copy() # target_col will be NaN here, original_idx populated\\n\\n    full_data_for_iteration = pd.concat([train_for_concat, test_for_concat], ignore_index=True)\\n    full_data_for_iteration['target_end_date'] = pd.to_datetime(full_data_for_iteration['target_end_date'])\\n    full_data_for_iteration['location_id'] = full_data_for_iteration['location_id'].astype(int)\\n\\n    # Sort the combined dataset by location and date, crucial for iterative lag generation\\n    full_data_for_iteration = full_data_for_iteration.sort_values(by=['location_id', 'target_end_date']).reset_index(drop=True)\\n\\n    # --- 3. Target Transformation for Training ---\\n    y_train_transformed = np.log1p(train_df[target_col])\\n\\n    # --- 4. Define Categorical Features for LightGBM ---\\n    categorical_features = ['location_id', 'year', 'month', 'week_of_year',\\n                            'day_of_year', 'day_of_week', 'horizon']\\n    categorical_features = [f for f in categorical_features if f in model_features]\\n\\n    # --- 5. Retrieve LightGBM Model Hyperparameters ---\\n    lgbm_params = config.get('lgbm_params', {\\n        'objective': 'quantile',  # Set objective for quantile regression\\n        'metric': 'quantile',     # Evaluation metric\\n        'n_estimators': 300,      # Number of boosting rounds\\n        'learning_rate': 0.03,    # Step size shrinkage\\n        'num_leaves': 32,         # Max number of leaves in one tree\\n        'verbose': -1,            # Suppress verbose output during training\\n        'n_jobs': -1,             # Use all available CPU cores for parallel processing\\n        'seed': 42,               # Random seed for reproducibility\\n        'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\\n        'lambda_l1': 0.1,         # L1 regularization (Lasso)\\n        'lambda_l2': 0.1,         # L2 regularization (Ridge)\\n        'feature_fraction': 0.8,  # Fraction of features considered at each split\\n        'bagging_fraction': 0.8,  # Fraction of data sampled for each tree\\n        'bagging_freq': 1         # Frequency for bagging\\n    })\\n\\n    # Initialize DataFrame to store all quantile predictions for the test set.\\n    test_y_hat_quantiles = pd.DataFrame(index=test_x.index)\\n\\n    # --- 6. Train and Predict for each Quantile iteratively ---\\n    # Loop over each quantile to train a separate model\\n    for q in quantiles:\\n        current_lgbm_params = lgbm_params.copy()\\n        current_lgbm_params['alpha'] = q\\n\\n        model = lgb.LGBMRegressor(**current_lgbm_params)\\n        \\n        # Train the model using the pre-computed features on the training data\\n        model.fit(train_df[model_features], y_train_transformed, categorical_feature=categorical_features)\\n\\n        # Create a working copy of \`full_data_for_iteration\` for predictions of this specific quantile.\\n        # This DataFrame's \`target_col\` will be filled with actuals for training rows\\n        # and with current quantile's predictions for test rows.\\n        temp_prediction_df = full_data_for_iteration.copy()\\n        \\n        # Fill the target column in \`temp_prediction_df\` with actual historical values\\n        temp_prediction_df.loc[temp_prediction_df['is_train_data'], target_col] = \\\\\\n            temp_prediction_df.loc[temp_prediction_df['is_train_data'], target_col]\\n        \\n        # Iterate through each location and predict its future steps iteratively\\n        # Ensure iteration order respects locations and dates for correct lag calculation\\n        \\n        # Get sorted list of unique locations\\n        unique_locations = sorted(temp_prediction_df['location_id'].unique())\\n        \\n        for loc_id in unique_locations:\\n            # Get all rows for this location, sorted by date (already sorted due to initial sort)\\n            loc_rows_indices = temp_prediction_df[temp_prediction_df['location_id'] == loc_id].index.tolist()\\n            \\n            for idx in loc_rows_indices:\\n                if not temp_prediction_df.loc[idx, 'is_train_data']: # Only predict for test rows\\n                    # Get historical data for lag calculation up to the previous timestamp\\n                    # This includes actuals from the training period AND previously predicted\\n                    # values for earlier horizons for the current location.\\n                    history_for_lags = temp_prediction_df[\\n                        (temp_prediction_df['location_id'] == loc_id) &\\n                        (temp_prediction_df['target_end_date'] < temp_prediction_df.loc[idx, 'target_end_date'])\\n                    ].copy()\\n\\n                    # Manually calculate autoregressive features for the current row based on history\\n                    current_ar_features = {}\\n                    \\n                    # Lags\\n                    for lag in lags:\\n                        if len(history_for_lags) >= lag:\\n                            current_ar_features[f'{target_col}_lag_{lag}'] = history_for_lags[target_col].iloc[-lag]\\n                        else:\\n                            current_ar_features[f'{target_col}_lag_{lag}'] = 0.0 # Default if not enough history\\n                    \\n                    # Rolling mean/std\\n                    for window in rolling_windows:\\n                        if len(history_for_lags) >= 1: # Need at least one point to consider a rolling window\\n                            rolling_mean_val = history_for_lags[target_col].rolling(window=window, min_periods=1).mean().iloc[-1]\\n                            rolling_std_val = history_for_lags[target_col].rolling(window=window, min_periods=1).std().iloc[-1]\\n                            current_ar_features[f'{target_col}_rolling_mean_{window}w'] = rolling_mean_val\\n                            current_ar_features[f'{target_col}_rolling_std_{window}w'] = rolling_std_val\\n                        else:\\n                            current_ar_features[f'{target_col}_rolling_mean_{window}w'] = 0.0\\n                            current_ar_features[f'{target_col}_rolling_std_{window}w'] = 0.0\\n\\n                    # Prepare the feature vector for the current row for prediction\\n                    X_single_row = temp_prediction_df.loc[[idx], base_feature_cols].copy() # Extract base features\\n                    for k, v in current_ar_features.items():\\n                        X_single_row[k] = v # Add autoregressive features\\n                    \\n                    # Ensure categorical features are set correctly for prediction\\n                    for c_feat in categorical_features:\\n                        if c_feat in X_single_row.columns:\\n                            X_single_row[c_feat] = X_single_row[c_feat].astype('category')\\n                    \\n                    # Make prediction for the current row\\n                    preds_transformed = model.predict(X_single_row[model_features])\\n                    pred = np.expm1(preds_transformed).item()\\n                    pred = max(0, pred) # Ensure non-negative predictions\\n\\n                    # Store the prediction back into \`temp_prediction_df[target_col]\`\\n                    # This makes the prediction available for calculating lags for *subsequent* horizons\\n                    temp_prediction_df.loc[idx, target_col] = pred\\n                    \\n                    # Store prediction in the final results DataFrame using original test_x index\\n                    original_test_index = temp_prediction_df.loc[idx, 'original_idx']\\n                    if not pd.isna(original_test_index): # Ensure it's a valid original index\\n                        test_y_hat_quantiles.loc[original_test_index, f'quantile_{q}'] = pred\\n\\n    # --- 7. Post-processing: Enforce monotonicity of quantile predictions. ---\\n    # This is crucial for the Weighted Interval Score (WIS) evaluation.\\n    # Predictions for a higher quantile must be greater than or equal to predictions\\n    # for a lower quantile.\\n    for i in range(1, len(quantiles)):\\n        prev_q_col = f'quantile_{quantiles[i-1]}'\\n        current_q_col = f'quantile_{quantiles[i]}'\\n        test_y_hat_quantiles[current_q_col] = test_y_hat_quantiles[[prev_q_col, current_q_col]].max(axis=1)\\n\\n    return test_y_hat_quantiles.astype(float) # Ensure output is float data type\\n\\n# These 'config_list' definitions will be used by the evaluation harness.\\n# Each dictionary represents a different set of hyperparameters for your model.\\n# The harness will run your \`fit_and_predict_fn\` with each config and select\\n# the best-performing one based on its internal evaluation criteria (e.g., WIS).\\nconfig_list = [\\n    {\\n        # Default LightGBM configuration, balancing performance and speed\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 300,\\n            'learning_rate': 0.03,\\n            'num_leaves': 32,\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.1,\\n            'lambda_l2': 0.1,\\n            'feature_fraction': 0.8,\\n            'bagging_fraction': 0.8,\\n            'bagging_freq': 1\\n        },\\n        'lags': [1, 2, 3], # Use 1, 2, 3 week lags\\n        'rolling_windows': [3, 7] # Use 3-week and 7-week rolling features\\n    },\\n    {\\n        # An alternative configuration with more estimators and slightly lower learning rate,\\n        # potentially leading to higher accuracy but longer training times.\\n        'lgbm_params': {\\n            'objective': 'quantile',\\n            'metric': 'quantile',\\n            'n_estimators': 400,    # Increased estimators\\n            'learning_rate': 0.02,  # Slightly lower learning rate\\n            'num_leaves': 48,       # More leaves per tree\\n            'verbose': -1,\\n            'n_jobs': -1,\\n            'seed': 42,\\n            'boosting_type': 'gbdt',\\n            'lambda_l1': 0.05,      # Reduced regularization\\n            'lambda_l2': 0.05,\\n            'feature_fraction': 0.7,\\n            'bagging_fraction': 0.7,\\n            'bagging_freq': 1\\n        },\\n        'lags': [1, 2, 3, 4], # Try more lags\\n        'rolling_windows': [4, 8] # Adjust rolling windows\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

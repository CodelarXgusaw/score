<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1219",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version aims to enhance robustness by:\\n    - Retaining robust handling of 'location' as a categorical feature.\\n    - Robust handling of missing data using time-series specific methods (ffill/bfill).\\n    - Improved numerical stability with clipping transformed target and predictions, ensuring non-negativity.\\n    - Incorporating additional date features.\\n    - Further revised XGBoost parameters for increased stability (reduced max_depth, increased min_child_weight, gamma, lambda, alpha, reduced n_estimators).\\n    - Simplified history update to directly use clipped transformed median, avoiding redundant transformations.\\n    - Streamlined inverse transformation assuming inputs are already clipped to a valid range.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost (adjusted for better stability)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 150, # Reduced estimators for robustness\\n        'learning_rate': 0.03,\\n        'max_depth': 2, # Reduced max_depth for simplicity/robustness\\n        'min_child_weight': 10, # Increased for more robustness\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.2, # Increased for more robustness\\n        'lambda': 3.0, # Increased for stronger L2 regularization\\n        'alpha': 0.2,   # Increased for stronger L1 regularization\\n        'verbosity': 0 # Change to 0 for less verbose output\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    # IMPORTANT: Enable categorical feature handling in XGBoost\\n    xgb_params['enable_categorical'] = True\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Maximum admissions per million for clipping\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 3000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    # Note: \`inverse_transform\` functions assume input \`x\` is already clipped to [0.0, MAX_TRANSFORMED_VALUE]\\n    def forward_transform_fourth_root(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    def inverse_transform_fourth_root(x): return np.power(x, 4) - 1.0\\n\\n    def forward_transform_log1p(x): return np.log1p(np.maximum(0.0, x))\\n    def inverse_transform_log1p(x): return np.expm1(x)\\n\\n    def forward_transform_sqrt(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    def inverse_transform_sqrt(x): return np.power(x, 2) - 1.0\\n\\n    def forward_transform_none(x): return x\\n    def inverse_transform_none(x): return x\\n\\n    if target_transform_type == 'fourth_root':\\n        forward_transform = forward_transform_fourth_root\\n        inverse_transform = inverse_transform_fourth_root\\n    elif target_transform_type == 'log1p':\\n        forward_transform = forward_transform_log1p\\n        inverse_transform = inverse_transform_log1p\\n    elif target_transform_type == 'sqrt':\\n        forward_transform = forward_transform_sqrt\\n        inverse_transform = inverse_transform_sqrt\\n    else: # No transformation\\n        forward_transform = forward_transform_none\\n        inverse_transform = inverse_transform_none\\n\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million)\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, cast to int\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            if X_train_model[col].isnull().any():\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n    train_combined = train_combined[np.isfinite(train_combined[TRANSFORMED_TARGET_COL])]\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row[HORIZON_COL]\\n\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            else:\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1:\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred):\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred):\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (transformed and clipped)\\n        median_pred_transformed_for_history = row_predictions_transformed[0.5]\\n        median_pred_transformed_for_history = np.clip(median_pred_transformed_for_history, 0.0, MAX_TRANSFORMED_VALUE)\\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_for_history)\\n\\n    # Ensure monotonicity of quantiles across all predictions\\n    # This must be done on the final integer predictions\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform. This performed best previously.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 3000.0\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform. ADJUSTED XGBoost for more robustness.\\n        'lgbm_params': {\\n            'n_estimators': 220,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced\\n            'learning_rate': 0.03,\\n            'max_depth': 2, # Reduced\\n            'min_child_weight': 10, # Increased\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.2, # Increased\\n            'lambda': 3.0, # Increased\\n            'alpha': 0.2, # Increased\\n            'verbosity': 0\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 3000.0\\n    },\\n    { # Config 3: Ensemble of LGBM and XGBoost, using 'log1p' transform. ADJUSTED XGBoost for more robustness.\\n        'lgbm_params': {\\n            'n_estimators': 220,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150,\\n            'learning_rate': 0.03,\\n            'max_depth': 2,\\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.2,\\n            'lambda': 3.0,\\n            'alpha': 0.2,\\n            'verbosity': 0\\n        },\\n        'target_transform': 'log1p', # Changed to log1p\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 3000.0\\n    }\\n]",
  "new_index": "1260",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version builds upon the previous attempt, focusing on stabilizing the\\n    XGBoost model parameters and ensuring explicit data access for robustness.\\n    The primary goal is to resolve the 'inf' average scores encountered by the\\n    ensemble configurations.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost (adjusted for better stability and learning capacity)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 250, # Increased for better learning\\n        'learning_rate': 0.03,\\n        'max_depth': 3, # Slightly increased from 2 for more complexity\\n        'min_child_weight': 10,\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.1, # Slightly reduced regularization\\n        'lambda': 1.5, # Slightly reduced regularization\\n        'alpha': 0.1,   # Slightly reduced regularization\\n        'verbosity': 0\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    # IMPORTANT: Enable categorical feature handling in XGBoost\\n    xgb_params['enable_categorical'] = True\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Maximum admissions per million for clipping\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 3000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    # Note: \`inverse_transform\` functions assume input \`x\` is already clipped to [0.0, MAX_TRANSFORMED_VALUE]\\n    def forward_transform_fourth_root(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    def inverse_transform_fourth_root(x): return np.power(x, 4) - 1.0\\n\\n    def forward_transform_log1p(x): return np.log1p(np.maximum(0.0, x))\\n    def inverse_transform_log1p(x): return np.expm1(x)\\n\\n    def forward_transform_sqrt(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    def inverse_transform_sqrt(x): return np.power(x, 2) - 1.0\\n\\n    def forward_transform_none(x): return x\\n    def inverse_transform_none(x): return x\\n\\n    if target_transform_type == 'fourth_root':\\n        forward_transform = forward_transform_fourth_root\\n        inverse_transform = inverse_transform_fourth_root\\n    elif target_transform_type == 'log1p':\\n        forward_transform = forward_transform_log1p\\n        inverse_transform = inverse_transform_log1p\\n    elif target_transform_type == 'sqrt':\\n        forward_transform = forward_transform_sqrt\\n        inverse_transform = inverse_transform_sqrt\\n    else: # No transformation\\n        forward_transform = forward_transform_none\\n        inverse_transform = inverse_transform_none\\n\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million)\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, cast to int\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    X_train_model[HORIZON_COL] = 0 # Horizon is 0 for historical training data\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    # Ensure mean_transformed_train_y_fallback is a scalar and finite\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    if not np.isfinite(mean_transformed_train_y_fallback):\\n        mean_transformed_train_y_fallback = forward_transform(1.0) # Fallback to a safe value if mean is not finite\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n        # After ffill/bfill, if there are still NaNs, it means the whole group was NaN.\\n        # Fill these remaining NaNs with the global fallback.\\n        if X_train_model[col].isnull().any():\\n            fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n    train_combined = train_combined[np.isfinite(train_combined[TRANSFORMED_TARGET_COL])]\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    # Prepare categorical features for LightGBM and XGBoost\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Store history for each location, will be updated with median predictions\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        # Use .loc for explicit label-based indexing to prevent FutureWarnings\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Use horizon from test_x\\n\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            else:\\n                diff_value = 0.0 # Default to 0 if not enough history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # If less than window, but some history exists\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default to 0 for std if not enough history\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Ensure test categorical features match training categories\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for a quantile failed or were not present, use fallback\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction (transformed and clipped)\\n        # Ensure that the median prediction is finite and within valid bounds for history.\\n        median_pred_transformed_for_history = row_predictions_transformed.get(0.5, mean_transformed_train_y_fallback)\\n        if not np.isfinite(median_pred_transformed_for_history):\\n            median_pred_transformed_for_history = mean_transformed_train_y_fallback\\n\\n        median_pred_transformed_for_history = np.clip(median_pred_transformed_for_history, 0.0, MAX_TRANSFORMED_VALUE)\\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_for_history)\\n\\n    # Ensure monotonicity of quantiles across all predictions\\n    # This must be done on the final integer predictions to guarantee non-decreasing order\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform. This performed best previously.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 3000.0\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform. Adjusted XGBoost for robustness.\\n        'lgbm_params': {\\n            'n_estimators': 250, # Matched n_estimators with config 1\\n            'learning_rate': 0.03,\\n            'num_leaves': 26, # Matched num_leaves with config 1\\n            'max_depth': 5, # Matched max_depth with config 1\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250, # Increased for better learning\\n            'learning_rate': 0.03,\\n            'max_depth': 3, # Slightly increased from 2 for more complexity\\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.1, # Slightly reduced regularization\\n            'lambda': 1.5, # Slightly reduced regularization\\n            'alpha': 0.1, # Slightly reduced regularization\\n            'verbosity': 0\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 3000.0\\n    },\\n    { # Config 3: Ensemble of LGBM and XGBoost, using 'log1p' transform. Adjusted XGBoost for robustness.\\n        'lgbm_params': {\\n            'n_estimators': 250, # Matched n_estimators with config 1\\n            'learning_rate': 0.03,\\n            'num_leaves': 26, # Matched num_leaves with config 1\\n            'max_depth': 5, # Matched max_depth with config 1\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250, # Increased for better learning\\n            'learning_rate': 0.03,\\n            'max_depth': 3, # Slightly increased from 2 for more complexity\\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.1, # Slightly reduced regularization\\n            'lambda': 1.5, # Slightly reduced regularization\\n            'alpha': 0.1, # Slightly reduced regularization\\n            'verbosity': 0\\n        },\\n        'target_transform': 'log1p', # Changed to log1p\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 3000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "871",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom xgboost import XGBRegressor # Import XGBoost\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost for quantile regression.\\n    It incorporates time-series features (including lagged target variables),\\n    a population-normalized and transformed target variable, and location information.\\n    The approach uses an iterative prediction strategy for the test set to correctly\\n    calculate lagged and rolling features for future steps, using median predictions\\n    to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    default_xgb_params = {\\n        'objective': 'reg:quantile', # XGBoost's quantile regression objective\\n        'eval_metric': 'rmse', # Quantile loss is not a standard eval metric for XGB\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 1,\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # For faster training\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1,\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of ensemble members *per model type* per quantile (e.g., 2 LGBM + 2 XGB for a given quantile)\\n    n_ensemble_members = config.get('n_ensemble_members', 1)\\n    \\n    # Enable/disable specific model types in the ensemble\\n    use_lgbm = config.get('use_lgbm', True)\\n    use_xgb = config.get('use_xgb', False)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default in case of empty training data or other issues.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    POPULATION_EPSILON = 1.0 # Using 1.0 as the baseline for 'per million' scale.\\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON)) # Ensure minimum value\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        # Ensure DATE_COL is datetime type\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n        \\n        # The 'horizon' column is only present in test_x. For train_x, we implicitly set it to 0.\\n        if HORIZON_COL not in df.columns:\\n            df[HORIZON_COL] = 0 \\n\\n        return df\\n\\n    # Features that are always numerical\\n    BASE_NUMERICAL_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                               'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                               'weeks_since_start_sq', HORIZON_COL]\\n\\n    # Features that are always categorical for LGBM\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaNs from std of single/few elements with 0 (e.g., only 1 data point in window)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    # Create a base DataFrame for training features, will be modified for each model type\\n    X_train_model_base = train_features_df[BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    for col in train_specific_features:\\n        if X_train_model_base[col].isnull().any():\\n            # Use ffill to propagate last known value, then fill any remaining NaNs (at the very beginning) with 0.0\\n            X_train_model_base[col] = X_train_model_base.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model_base[col] = X_train_model_base[col].fillna(0.0) \\n\\n    train_combined = X_train_model_base.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where essential features or target are NaN AFTER ffill/fillna attempts\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL] + [POPULATION_COL] + train_specific_features, inplace=True)\\n\\n    if train_combined.empty:\\n        return predictions_df \\n\\n    X_train_model_base = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM/XGBoost ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model_base[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    \\n    # Apply categorical type for LGBM\\n    X_train_model_base[LOCATION_COL] = X_train_model_base[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    test_x_processed[LOCATION_COL] = test_x_processed[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # For XGBoost, convert 'location' to numerical (label encoding).\\n    location_mapping = {loc: i for i, loc in enumerate(all_location_categories)}\\n    X_train_model_base['location_encoded'] = X_train_model_base[LOCATION_COL].map(location_mapping)\\n    test_x_processed['location_encoded'] = test_x_processed[LOCATION_COL].map(location_mapping)\\n\\n    # Define feature sets for each model type explicitly\\n    lgbm_feature_cols = BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    # XGBoost uses the encoded location, not the categorical object\\n    xgb_feature_cols = [col for col in BASE_NUMERICAL_FEATURES if col != LOCATION_COL] + ['location_encoded'] + train_specific_features\\n\\n\\n    # --- 4. Model Training (LightGBM and XGBoost models) ---\\n    models = {q: {'lgbm': [], 'xgb': []} for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        # Train LightGBM models\\n        if use_lgbm:\\n            for i in range(n_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q \\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i \\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                if not X_train_model_base.empty and not y_train_model.empty:\\n                    lgbm_model.fit(X_train_model_base[lgbm_feature_cols], y_train_model,\\n                                   categorical_feature=CATEGORICAL_FEATURES_LIST)\\n                    models[q]['lgbm'].append(lgbm_model)\\n\\n        # Train XGBoost models\\n        if use_xgb:\\n            for i in range(n_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q # Specific parameter for reg:quantile\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n                \\n                xgb_model = XGBRegressor(**xgb_model_params_i)\\n                if not X_train_model_base.empty and not y_train_model.empty:\\n                    xgb_model.fit(X_train_model_base[xgb_feature_cols], y_train_model)\\n                    models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # Convert series elements to float lists to avoid potential pandas FutureWarnings during slicing.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(lambda x: x.tolist()).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base (numerical and categorical) features.\\n        # Include 'location_encoded' directly as it's needed for xgb.\\n        current_features_dict = {col: row.get(col, 0) for col in BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + ['location_encoded']}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = 0.0 # Default if no history at all\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: \\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points to calculate std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # If insufficient data, std is 0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        current_features_series = pd.Series(current_features_dict)\\n\\n        # Create the LGBM specific feature row\\n        X_test_row_lgbm = pd.DataFrame([current_features_series.reindex(lgbm_feature_cols).fillna(0.0)])\\n        # Ensure 'location' column in X_test_row_lgbm is of CategoricalDtype with full categories\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        \\n        # Create the XGBoost specific feature row\\n        X_test_row_xgb = pd.DataFrame([current_features_series.reindex(xgb_feature_cols).fillna(0.0)])\\n        # Ensure 'location_encoded' is integer type for XGBoost\\n        X_test_row_xgb['location_encoded'] = X_test_row_xgb['location_encoded'].astype(int)\\n        \\n        # Make predictions for all quantiles for this single row using all models.\\n        row_predictions_transformed = {}\\n        for q_idx, q in enumerate(QUANTILES):\\n            all_model_preds_for_q = []\\n            \\n            # Predict with LightGBM\\n            if use_lgbm and models[q]['lgbm']:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    all_model_preds_for_q.append(lgbm_model_q.predict(X_test_row_lgbm)[0])\\n            \\n            # Predict with XGBoost\\n            if use_xgb and models[q]['xgb']:\\n                for xgb_model_q in models[q]['xgb']:\\n                    all_model_preds_for_q.append(xgb_model_q.predict(X_test_row_xgb)[0])\\n\\n            if all_model_preds_for_q:\\n                # Average predictions from ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(all_model_preds_for_q)\\n            else:\\n                # Fallback if no models were trained or enabled for this quantile\\n                row_predictions_transformed[q] = 0.0 \\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n        \\n        # CRUCIAL FIX: Clamp transformed predictions to be non-negative before inverse transformation.\\n        # This helps prevent extreme values or NaNs resulting from unstable predictions,\\n        # especially for lower quantiles or during extrapolation.\\n        transformed_preds_array = np.maximum(0.0, transformed_preds_array)\\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Ensure non-negative after inverse transform, as admission counts cannot be negative.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        safe_population_test = POPULATION_EPSILON\\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed and clamped).\\n        # This value is directly suitable for future lag feature calculations.\\n        median_pred_transformed_clamped = transformed_preds_array[QUANTILES.index(0.5)] \\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_clamped)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Base LGBM parameters (similar to previous best performing config) - baseline\\n        'lgbm_params': {\\n            'n_estimators': 250, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # Not used in this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_ensemble_members': 1, # Number of models of each type for a given quantile\\n        'use_lgbm': True,\\n        'use_xgb': False, # Explicitly disable XGBoost for this config\\n    },\\n    { # Config 2: Ensemble LGBM + XGBoost - balanced feature set & params\\n        'lgbm_params': {\\n            'n_estimators': 200, 'learning_rate': 0.03, 'num_leaves': 25, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.7, 'subsample': 0.7, 'reg_alpha': 0.05, 'reg_lambda': 0.05\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 1,\\n            'subsample': 0.7, 'colsample_bytree': 0.7, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist', 'reg_alpha': 0.05, 'reg_lambda': 0.05\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52], # Slightly more granular lags\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16, 26], # Wider range of rolling windows\\n        'rolling_std_windows': [4, 8, 16],\\n        'n_ensemble_members': 1,\\n        'use_lgbm': True,\\n        'use_xgb': True, # Enable XGBoost for this ensemble config\\n    },\\n    { # Config 3: Ensemble LGBM + XGBoost - more complex models, longer lags\\n        'lgbm_params': {\\n            'n_estimators': 300, 'learning_rate': 0.025, 'num_leaves': 30, 'max_depth': 6,\\n            'min_child_samples': 25, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.6, 'subsample': 0.6, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 300, 'learning_rate': 0.025, 'max_depth': 6, 'min_child_weight': 1,\\n            'subsample': 0.6, 'colsample_bytree': 0.6, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist', 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 12, 26, 39, 52], # More lags, including quarterly\\n        'lag_diff_periods': [1, 4, 8, 12],\\n        'rolling_windows': [8, 16, 26, 52], # Focus on longer-term smoothing\\n        'rolling_std_windows': [8, 16, 26],\\n        'n_ensemble_members': 1,\\n        'use_lgbm': True,\\n        'use_xgb': True,\\n    },\\n    { # Config 4: Only XGBoost, for direct comparison (similar to Config 1 but with XGB)\\n        'lgbm_params': {}, # Not used\\n        'xgb_params': {\\n            'n_estimators': 250, 'learning_rate': 0.03, 'max_depth': 5, 'min_child_weight': 1,\\n            'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist', 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_ensemble_members': 1,\\n        'use_lgbm': False, # Explicitly disable LGBM\\n        'use_xgb': True,\\n    }\\n]",
  "new_index": "897",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    This function implements a LightGBM-based quantile regression model for\\n    COVID-19 hospital admissions. It incorporates time-series features such\\n    as lagged admissions, lagged differences, and rolling statistics.\\n    A key aspect is the transformation of the target variable (admissions\\n    per million population) to better handle its skewed distribution and\\n    the recursive prediction strategy for generating future forecasts.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model,\\n                                  including LGBM hyperparameters and feature engineering choices.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX' (e.g., 'quantile_0.5').\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default LightGBM parameters (tuned from previous trials)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 250,\\n        'learning_rate': 0.03,\\n        'num_leaves': 26,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1  # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    # Target transformation type\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of LightGBM models to train for each quantile (ensemble within quantile)\\n    n_ensemble_members = config.get('n_ensemble_members', 1)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default; these will be overwritten.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    # Use a small epsilon for population to avoid division by zero and provide a baseline.\\n    POPULATION_EPSILON = 1.0 \\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON)) # Ensure population is at least epsilon\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative admissions\\n\\n    # Define transform and inverse transform functions based on config.\\n    # Adding +1.0 before power transforms to handle zero values gracefully.\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        # For forward_transform during recursive prediction, ensure input is non-negative\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set to anchor 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # isocalendar().week can return 53 for some years, ensure consistency if needed, but usually fine\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n        \\n        # The 'horizon' column is specific to test_x. For train_x, it's implicitly 0.\\n        if HORIZON_COL not in df.columns:\\n            df[HORIZON_COL] = 0 \\n\\n        return df\\n\\n    # Define feature sets\\n    BASE_NUMERICAL_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                               'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                               'weeks_since_start_sq', HORIZON_COL]\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # For LightGBM\\n\\n    # Apply base feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged features\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged difference features\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaNs (e.g., from std of single point) with 0\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns for the model\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    # Create the full feature set for training\\n    X_train_model = train_features_df[BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Forward fill (ffill) and then fill remaining NaNs (at beginning of series) with 0.0\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    # Combine X and y for final filtering of rows with NaNs in target or essential features\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL, POPULATION_COL] + train_specific_features, inplace=True)\\n\\n    if train_combined.empty:\\n        # If no valid training data, return the initialized predictions_df (filled with zeros)\\n        return predictions_df \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    \\n    # Apply categorical type for LGBM, using all_location_categories\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    test_x_processed[LOCATION_COL] = test_x_processed[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    lgbm_feature_cols = BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n\\n    # --- 4. Model Training (LightGBM models for each quantile) ---\\n    models = {q: [] for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        for i in range(n_ensemble_members): # Train 'n_ensemble_members' LGBM models per quantile\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set the quantile parameter\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Vary seed for ensemble diversity\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            if not X_train_model.empty and not y_train_model.empty:\\n                lgbm_model.fit(X_train_model[lgbm_feature_cols], y_train_model,\\n                               categorical_feature=CATEGORICAL_FEATURES_LIST)\\n                models[q].append(lgbm_model)\\n            else:\\n                print(f\\"Warning: Training data is empty for quantile {q} in ensemble member {i}, skipping model training.\\")\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(lambda x: x.tolist()).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base features.\\n        current_features_dict = {col: row.get(col, 0) for col in BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = 0.0 # Default if no history at all\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: \\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points to calculate std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # If insufficient data, std is 0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        current_features_series = pd.Series(current_features_dict)\\n\\n        # Create the LightGBM specific feature row\\n        X_test_row_lgbm = pd.DataFrame([current_features_series.reindex(lgbm_feature_cols).fillna(0.0)])\\n        # Ensure 'location' column in X_test_row_lgbm is of CategoricalDtype with full categories\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        \\n        # Make predictions for all quantiles for this single row.\\n        row_predictions_transformed = {}\\n        for q_idx, q in enumerate(QUANTILES):\\n            all_lgbm_preds_for_q = []\\n            \\n            if models[q]: # Check if any LGBM models were trained for this quantile\\n                for lgbm_model_q in models[q]:\\n                    all_lgbm_preds_for_q.append(lgbm_model_q.predict(X_test_row_lgbm)[0])\\n            \\n            if all_lgbm_preds_for_q:\\n                # Average predictions from ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(all_lgbm_preds_for_q)\\n            else:\\n                # Fallback if no models were trained for this quantile or training failed\\n                # This ensures a default value if predictions can't be made.\\n                row_predictions_transformed[q] = 0.0 \\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n        \\n        # Clamp transformed predictions to be non-negative before inverse transformation.\\n        transformed_preds_array = np.maximum(0.0, transformed_preds_array)\\n\\n        # Inverse transform predictions from the transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Ensure non-negative after inverse transform, as admission counts cannot be negative.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        safe_population_test = POPULATION_EPSILON\\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed and clamped).\\n        # This value will be used for future lag feature calculations for this location.\\n        median_pred_transformed_clamped = transformed_preds_array[QUANTILES.index(0.5)] \\n        location_history_data.setdefault(current_loc, []).append(median_pred_transformed_clamped)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    # Sorting ensures that lower quantiles are always less than or equal to higher quantiles.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity and convert to integer counts\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: A balanced LightGBM model with a \\"fourth_root\\" transformation.\\n      # This configuration aims for a good balance between model complexity and performance,\\n      # using a comprehensive set of lagged and rolling features.\\n        'lgbm_params': {\\n            'n_estimators': 250, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Lags capturing weekly, monthly, quarterly, and yearly patterns\\n        'lag_diff_periods': [1, 2, 4, 8], # Differences help capture trends\\n        'rolling_windows': [8, 16, 26], # Rolling means for smoothing short to medium term trends\\n        'rolling_std_windows': [4, 8], # Rolling standard deviations for capturing volatility\\n        'n_ensemble_members': 1, # Using a single LGBM model per quantile for simplicity\\n    },\\n    { # Config 2: A slightly more aggressive LightGBM model with more estimators and features.\\n      # This config explores if more complex models and a wider range of features lead to better scores.\\n        'lgbm_params': {\\n            'n_estimators': 350, 'learning_rate': 0.02, 'num_leaves': 31, 'max_depth': 6,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.7, 'subsample': 0.7, 'reg_alpha': 0.05, 'reg_lambda': 0.05\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 12, 26, 52], # More granular and longer lags\\n        'lag_diff_periods': [1, 2, 4, 8, 12], # More difference periods\\n        'rolling_windows': [4, 8, 16, 26, 52], # Wider range of rolling windows\\n        'rolling_std_windows': [4, 8, 16],\\n        'n_ensemble_members': 1,\\n    },\\n    { # Config 3: A simpler LightGBM model with fewer estimators and a reduced feature set.\\n      # This tests if a more streamlined model can still perform competitively, potentially\\n      # reducing training time and risk of overfitting.\\n        'lgbm_params': {\\n            'n_estimators': 150, 'learning_rate': 0.05, 'num_leaves': 16, 'max_depth': 4,\\n            'min_child_samples': 15, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.9, 'subsample': 0.9, 'reg_alpha': 0.2, 'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 26], # Fewer lags\\n        'lag_diff_periods': [1, 4], # Fewer difference periods\\n        'rolling_windows': [8, 26], # Fewer rolling windows\\n        'rolling_std_windows': [8],\\n        'n_ensemble_members': 1,\\n    },\\n    { # Config 4: Exploring 'log1p' transformation with the balanced LightGBM model.\\n      # This config investigates the impact of a different target transformation,\\n      # which might be more suitable depending on the underlying data distribution.\\n        'lgbm_params': {\\n            'n_estimators': 250, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p', # Changed target transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_ensemble_members': 1,\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

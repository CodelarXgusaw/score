<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1930",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    Missing data is handled using a combination of time-series specific methods:\\n    1. Forward-fill and backward-fill (\`ffill().bfill()\`) for initial gaps in time-series features in training data.\\n    2. Fallback to location-specific median (derived from training data), or global median/zero if\\n       location-specific history is insufficient or location is new. This provides a more refined\\n       imputation for initial values in a time series.\\n    3. Iterative prediction for future steps in the test set, where previous median predictions are used to\\n       generate lagged features for subsequent predictions. This effectively imputes future unknown values\\n       by using the model's own central forecast.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    # Number of LightGBM models to train for each quantile (ensemble members)\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Max admissions per million cap to prevent extremely large or unstable target values.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1)) # Must be at least 1 for integer counts\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Determine median population from train data for robust imputation across both train and test.\\n    median_population_train_initial = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train_initial) or median_population_train_initial == 0:\\n        median_population_train_initial = 1.0 # Absolute fallback for median if it's NaN/Inf/0\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        def inverse_transform(x):\\n            # Ensure input to expm1 is non-negative and output is non-negative\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure input to log1p is non-negative\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to sqrt is non-negative\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n\\n    # --- 2. Function to add common date-based features and handle population ---\\n    min_date_global = df_train_full[DATE_COL].min() # Global min date for 'weeks_since_start' consistency\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp, median_pop_fallback: float) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add horizon feature directly, as it's specific to test_x but can be used in train_x (as 0)\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data if not explicitly provided\\n        \\n        # Ensure population is float for all base features and handle potential NaNs/zeros\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_pop_fallback).astype(float)\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, 1.0, df[POPULATION_COL]) # Replace any zeros with 1.0\\n        \\n        return df\\n\\n    # Apply base feature engineering to training data\\n    df_train_full = add_base_features(df_train_full, min_date_global, median_population_train_initial)\\n\\n    # Calculate admissions per million and transform for training data\\n    admissions_per_million_train = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000\\n    admissions_per_million_train = np.clip(admissions_per_million_train, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million_train)\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Prepare test_x\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global, median_population_train_initial)\\n\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Pre-calculate fallbacks for missing history.\\n    # Global median of the transformed target. This is the absolute fallback.\\n    global_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(global_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        global_transformed_train_y_fallback = forward_transform(1.0) # Corresponds to 1 admission per million after inverse transform\\n    global_transformed_train_y_fallback = np.clip(global_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Location-specific medians for fallback (used more heavily in test set prediction where history is critical)\\n    location_median_fallbacks = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].median().to_dict()\\n    for loc, val in location_median_fallbacks.items():\\n        if not np.isfinite(val):\\n            location_median_fallbacks[loc] = global_transformed_train_y_fallback\\n        else:\\n            location_median_fallbacks[loc] = np.clip(val, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    # Use ffill/bfill for time-series specific imputation within groups\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            # For any remaining NaNs (e.g., entire series for a location is NaN after target dropna),\\n            # use the global fallback. Location-specific filling here is complex with transform.\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else global_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness,\\n    # but good safeguard if transformed target could be NaN for some reason)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n\\n    # Handle categorical features for LightGBM.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Only LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} # List of models for each quantile\\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q\\n            lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with training data's transformed target values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Determine the most appropriate fallback value for this specific location.\\n        # If the location has a calculated median from training data, use it; otherwise, use the global median.\\n        loc_specific_transformed_fallback = location_median_fallbacks.get(current_loc, global_transformed_train_y_fallback)\\n\\n        # Populate current features dictionary, population is already cleaned by add_base_features\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n\\n        # Generate time-series features for the current test row using available history\\n        if not current_loc_hist:\\n            # If no history for this location (e.g., new location in test set or very early in history),\\n            # use the determined fallback for all time-series features.\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = loc_specific_transformed_fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0 # Diffs are 0 if no history\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = loc_specific_transformed_fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0 # Std dev is 0 if no history or single point\\n        else:\\n            # Generate features based on available history\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[-lag]\\n                else:\\n                    # Fallback to location-specific/global median if history is shorter than required lag\\n                    lag_value = loc_specific_transformed_fallback\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                if len(current_loc_hist) >= diff_period + 1:\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-(diff_period + 1)]\\n                elif len(current_loc_hist) >= 2: # Fallback to 1-period diff if longer diff not possible\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n                else:\\n                    diff_value = 0.0 # If not enough history for any diff\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    # Fallback if window calculation results in empty history (e.g., if current_loc_hist became empty somehow)\\n                    rolling_mean_val = loc_specific_transformed_fallback\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1: # Need at least 2 points for std dev\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n            for lgbm_model_q in models[q]:\\n                try:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred):\\n                        pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                        ensemble_preds_for_q.append(pred_clipped)\\n                except Exception:\\n                    pass # Silently fail for individual model prediction to allow ensemble to continue\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.median(ensemble_preds_for_q) # Use median for robustness\\n            else:\\n                row_predictions_transformed[q] = loc_specific_transformed_fallback # Fallback if all ensemble models fail/produce non-finite output\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL]\\n        final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Use the median prediction from the current forecasts to update the history for next iteration.\\n        # This is crucial for iterative multi-step ahead forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_transformed_clipped = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        # If the median prediction is NaN/Inf, use the robust location-specific fallback for history\\n        if not np.isfinite(median_pred_transformed_clipped):\\n             value_to_add_to_history = loc_specific_transformed_fallback\\n        else:\\n             value_to_add_to_history = median_pred_transformed_clipped\\n\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array = np.maximum(0, predictions_array) # Ensure non-negative counts\\n\\n    for i in range(predictions_array.shape[0]):\\n        # Sort each row (quantiles) to ensure strict monotonicity\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n        \\n        # Ensure minimum spread between adjacent quantiles (to avoid zero-width intervals)\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Optimized base (similar to previous best with slight adjustments)\\n        'lgbm_params': {\\n            'n_estimators': 280,\\n            'learning_rate': 0.028,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.15, # Slight increase in L1 regularization\\n            'reg_lambda': 0.15 # Slight increase in L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1, # Using only 1 LGBM per quantile\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 2: More Aggressive Regularization\\n        'lgbm_params': {\\n            'n_estimators': 350, # More trees to compensate for smaller LR\\n            'learning_rate': 0.02, # Reduced learning rate\\n            'num_leaves': 20, # Reduced complexity per tree\\n            'max_depth': 4, # Reduced max depth per tree\\n            'min_child_samples': 30, # Increased minimum samples per leaf\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7, # Slightly more feature subsampling\\n            'subsample': 0.7, # Slightly more data subsampling\\n            'reg_alpha': 0.5, # Stronger L1 regularization\\n            'reg_lambda': 0.5 # Stronger L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    }\\n]",
  "new_index": "1965",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\nimport warnings\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM models for quantile regression, trained to predict\\n    transformed COVID-19 hospital admissions per million population. It incorporates\\n    extensive time-series features, including lagged target variables, lagged differences,\\n    and rolling statistics, alongside date-based features and location information.\\n\\n    A key aspect is the iterative prediction strategy for the test set. For multi-step\\n    ahead forecasts (horizons > 0), the model uses its own median predictions from\\n    previous time steps to generate lagged and rolling features for subsequent predictions.\\n    This simulates a real-world forecasting scenario where future observed data is unknown.\\n\\n    Missing data is handled via:\\n    1. Initial forward-fill and backward-fill within location groups for time-series features\\n       in the training data.\\n    2. Fallback to location-specific median (derived from training data) or a global median\\n       for missing historical values, particularly crucial for generating features in the\\n       iterative test prediction loop. This ensures robustness even when a location's\\n       history is short or new.\\n\\n    Probabilistic forecasts are generated by training a separate LightGBM model for each\\n    desired quantile. To enhance robustness, multiple ensemble members (models with different\\n    random seeds) are trained for each quantile, and their predictions are median-aggregated.\\n    Post-processing ensures monotonicity and a minimum spread for the predicted quantiles.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    # Define the quantiles that need to be predicted\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    # Define standard column names used in the dataset\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # This column indicates the forecast horizon\\n\\n    # Name for the transformed target variable\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default parameters for LightGBM, providing a solid baseline\\n    default_lgbm_params = {\\n        'objective': 'quantile',  # Specifies quantile regression\\n        'metric': 'quantile',     # Evaluation metric for quantile regression\\n        'n_estimators': 200,      # Number of boosting rounds\\n        'learning_rate': 0.03,    # Step size shrinkage to prevent overfitting\\n        'num_leaves': 25,         # Max number of leaves in one tree\\n        'max_depth': 5,           # Max depth of the tree\\n        'min_child_samples': 20,  # Minimum number of data needed in a child (leaf)\\n        'random_state': 42,       # Seed for reproducibility\\n        'n_jobs': -1,             # Use all available cores\\n        'verbose': -1,            # Suppress verbose output\\n        'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\\n        'subsample': 0.8,         # Subsample ratio of the training instance\\n        'reg_alpha': 0.1,         # L1 regularization term on weights\\n        'reg_lambda': 0.1         # L2 regularization term on weights\\n    }\\n    # Override default LGBM parameters with any specified in the config\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters, with sensible defaults\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52]) # Lagged target values\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8]) # Lagged differences in target\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26]) # Windows for rolling mean\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8]) # Windows for rolling standard deviation\\n\\n    # Type of transformation to apply to the target variable\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    # Number of LightGBM models to train for each quantile (for ensembling per quantile)\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Maximum admissions per million cap to prevent extremely large or unstable target values.\\n    # This helps clip outliers and stabilize training.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals.\\n    # Essential for valid quantile output for Weighted Interval Score.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1)) # Must be at least 1 for integer counts\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Ensure data is sorted by location and date, critical for time-series operations\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Determine a robust median population from train data. This is used as a fallback\\n    # if population data is missing or zero for a given row.\\n    median_population_train_initial = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train_initial) or median_population_train_initial == 0:\\n        median_population_train_initial = 1.0 # Absolute fallback to avoid division by zero\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        # Transforms (x+1)^0.25, inverse is (x^4) - 1\\n        # Adding 1 before transform and subtracting 1 after inverse transform handles zero values gracefully.\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        # Transforms log(x+1), inverse is exp(x) - 1\\n        def inverse_transform(x):\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        # Transforms sqrt(x+1), inverse is (x^2) - 1\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Calculate the maximum possible transformed value based on the clipping limit\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n\\n    # --- 2. Function to add common date-based features and handle population ---\\n    # Global minimum date from training data to ensure 'weeks_since_start' is consistent\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp, median_pop_fallback: float) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        # Extract temporal features\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality without abrupt changes\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Time elapsed since the start of the data\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Quadratic term for trend\\n\\n        # Add horizon feature. For training data, it's typically 0 or derived from internal split.\\n        # For test data, it's explicitly provided.\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data (or if missing)\\n\\n        # Ensure population is numerical and handle NaNs/zeros using the robust median fallback\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_pop_fallback).astype(float)\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, median_pop_fallback, df[POPULATION_COL])\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL].isna(), median_pop_fallback, df[POPULATION_COL]) # Final check for NaNs after fillna\\n\\n        return df\\n\\n    # Apply base feature engineering to the full training dataset\\n    df_train_full = add_base_features(df_train_full, min_date_global, median_population_train_initial)\\n\\n    # Calculate admissions per million and apply the chosen transformation\\n    admissions_per_million_train = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000\\n    # Clip values to prevent extreme outliers before transformation\\n    admissions_per_million_train = np.clip(admissions_per_million_train, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = forward_transform(admissions_per_million_train)\\n    # Clip transformed values as well to ensure they stay within reasonable bounds\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Apply base feature engineering to the test dataset\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global, median_population_train_initial)\\n\\n    # Define the list of base features to be used in the model\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    # Define categorical features for LightGBM\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Calculate global and location-specific fallback values for missing historical data.\\n    # These are crucial for handling new locations or periods with short history.\\n    global_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(global_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        # If no valid median (e.g., all NaNs or empty data), fall back to a small non-zero value\\n        global_transformed_train_y_fallback = forward_transform(1.0) # corresponds to 1 admission per million\\n    global_transformed_train_y_fallback = np.clip(global_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    location_median_fallbacks = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].median().to_dict()\\n    for loc, val in location_median_fallbacks.items():\\n        if not np.isfinite(val):\\n            location_median_fallbacks[loc] = global_transformed_train_y_fallback\\n        else:\\n            location_median_fallbacks[loc] = np.clip(val, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Generate lagged features\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged difference features\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    # Prepare training features (X) and target (y) for the model\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data in generated features (e.g., at the beginning of a time series)\\n    # Using ffill/bfill within groups for time-series features.\\n    # For any remaining NaNs (e.g., if a group is entirely NaN for a feature, or \`std\` on single point),\\n    # fill with 0.0 (for diffs/stds) or the global fallback (for lags/means).\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else global_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Combine X and y for dropping rows where target is missing (after feature generation)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after preprocessing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        warnings.warn(\\"Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n    # Prepare categorical features for LightGBM. It needs to know all possible categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Convert 'location' column to categorical Dtype for LightGBM for training data\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models for each quantile) ---\\n    models = {q: [] for q in QUANTILES} # Stores a list of ensemble members for each quantile\\n\\n    # Train a separate LightGBM model for each quantile\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set alpha for quantile regression\\n            # Vary random state for ensemble members to introduce diversity\\n            lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            with warnings.catch_warnings(): # Suppress common LGBM warnings about categorical features\\n                warnings.simplefilter(\\"ignore\\")\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n            models[q].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with the historical training data's transformed target values.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index # Keep original index for final output alignment\\n\\n    # Sort test_x chronologically within each location. This is crucial for iterative predictions.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Iterate row by row for test set prediction to enable iterative feature generation\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n        current_date = row[DATE_COL]\\n\\n        # Get the history for the current location. If no history, it's an empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Determine the most appropriate fallback value for this location's history.\\n        loc_specific_transformed_fallback = location_median_fallbacks.get(current_loc, global_transformed_train_y_fallback)\\n\\n        # Populate current features dictionary with base features\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n\\n        # Dynamically generate time-series features for the current test row\\n        # using the available history (actual and previously predicted values)\\n        if not current_loc_hist:\\n            # If no history for this location (e.g., new location or very early in time),\\n            # use fallbacks for all time-series features.\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = loc_specific_transformed_fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0 # Diffs are 0 if no history\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = loc_specific_transformed_fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0 # Std dev is 0 if no history or single point\\n        else:\\n            # Generate features based on available history\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[-lag]\\n                else:\\n                    # Fallback if history is shorter than required lag\\n                    lag_value = loc_specific_transformed_fallback\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                if len(current_loc_hist) >= diff_period: # Need at least \`diff_period\` values + current point in history for diff\\n                    # Difference of last point with \`diff_period\` points before it\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-(diff_period)]\\n                else:\\n                    diff_value = 0.0 # If not enough history for any diff, assume no change\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_mean_val = loc_specific_transformed_fallback # Fallback if no history for mean\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1: # Need at least 2 points for std dev\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0 # Std dev is 0 if one or zero points\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features, ensuring correct order and types\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0) # Ensure all train columns exist\\n\\n        # Convert 'location' column to categorical Dtype for LightGBM for prediction\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n            for lgbm_model_q in models[q]:\\n                try:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to the ensemble\\n                        pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                        ensemble_preds_for_q.append(pred_clipped)\\n                except Exception as e:\\n                    # Log the error for debugging if needed, but continue\\n                    # print(f\\"Warning: Error predicting for quantile {q}, model {lgbm_model_q}: {e}\\")\\n                    pass\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.median(ensemble_preds_for_q) # Use median for robustness against outliers\\n            else:\\n                # If all ensemble models fail, use location-specific/global fallback for the prediction\\n                row_predictions_transformed[q] = loc_specific_transformed_fallback\\n\\n        # Convert transformed predictions back to original scale (Total COVID-19 Admissions)\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL] # Use the processed population value for the current row\\n        final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative counts\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        # Store predictions in the DataFrame, aligned by original index\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Crucially, update the history for the current location with the median prediction.\\n        # This predicted value will be used as a lagged feature for future steps in the test set.\\n        median_pred_transformed_for_history = row_predictions_transformed[0.5]\\n        median_pred_transformed_for_history = np.clip(median_pred_transformed_for_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        if not np.isfinite(median_pred_transformed_for_history):\\n             # Fallback to robust median if the median prediction for history is invalid\\n             value_to_add_to_history = loc_specific_transformed_fallback\\n        else:\\n             value_to_add_to_history = median_pred_transformed_for_history\\n\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array = np.maximum(0, predictions_array) # Ensure all predictions are non-negative\\n\\n    for i in range(predictions_array.shape[0]):\\n        # Sort each row (quantiles) to ensure strict monotonicity: Q(0.01) <= Q(0.025) <= ...\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        # Ensure minimum spread between adjacent quantiles to avoid zero-width intervals.\\n        # This is vital for the Weighted Interval Score, which penalizes narrow intervals.\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # Convert back to DataFrame and set integer type for final output\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Base with slightly more estimators and gentle regularization\\n        'lgbm_params': {\\n            'n_estimators': 280,\\n            'learning_rate': 0.028,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1, # Single model per quantile\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 2: Robust ensemble approach with more models per quantile\\n        'lgbm_params': {\\n            'n_estimators': 250, # Slightly reduced estimators per model, but more models total\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7, # Slightly more feature randomness for ensemble diversity\\n            'subsample': 0.7,     # Slightly more data randomness for ensemble diversity\\n            'reg_alpha': 0.2,     # Slightly increased regularization\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 3, # Use 3 LightGBM models per quantile for robustness\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 3: Log1p transformation to compare\\n        'lgbm_params': {\\n            'n_estimators': 280,\\n            'learning_rate': 0.028,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'log1p', # Use log1p transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1672",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), population-normalized and transformed\\n    target variables, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version aims to generalize well by:\\n    - Allowing configuration of LGBM and XGBoost parameters.\\n    - Supporting different target transformations (fourth_root, log1p, sqrt, or none).\\n    - Implementing robust feature engineering including lags, differences, and rolling statistics.\\n    - Using an iterative forecasting approach for multi-step predictions.\\n    - Including post-processing to ensure non-negative, monotonic, and sufficiently spread quantile predictions.\\n    - Addressing numerical stability with appropriate clipping and fallback values.\\n\\n    **Model Predictions and Feature Importance Explanation:**\\n\\n    The model's predictions are primarily driven by **recent historical trends** of COVID-19 hospital admissions in each state.\\n    Key features influencing predictions include:\\n\\n    1.  **Lagged Target Values (\`lag_X_wk\`):** These are among the most crucial features, representing the number of admissions\\n        from previous weeks (e.g., 1 week ago, 4 weeks ago, 8 weeks ago, up to a year ago). These directly capture\\n        the recent trajectory and seasonality of the pandemic within a specific location. A high value last week\\n        will likely lead to a high prediction this week, and vice-versa.\\n\\n    2.  **Lagged Differences (\`diff_lag_1_period_X_wk\`):** These features capture the week-over-week change in admissions.\\n        They indicate the *rate of change* or momentum (e.g., rapid increase or decrease), which is vital for\\n        forecasting turning points or accelerating/decelerating trends.\\n\\n    3.  **Rolling Means (\`rolling_mean_X_wk\`):** These provide a smoothed average of admissions over longer periods (e.g., 8, 16, 26 weeks).\\n        They help in capturing underlying trends, filtering out noise, and understanding the general level of admissions.\\n\\n    4.  **Rolling Standard Deviations (\`rolling_std_X_wk\`):** These measure the volatility or spread of admissions over a recent period.\\n        Higher standard deviation might indicate more unpredictable periods, which can influence the width of the predicted quantiles.\\n\\n    5.  **Seasonal Features (\`sin_week_of_year\`, \`cos_week_of_year\`, \`week_of_year\`, \`day_of_week\`):** These cyclic features allow the model to learn\\n        yearly patterns in admissions (e.g., typical winter surges, summer lulls) that are common across locations or specific to some.\\n        \`day_of_week\` captures any potential weekly patterns, though for weekly aggregated data, it might be constant.\\n\\n    6.  **Long-Term Trend Features (\`weeks_since_start\`, \`weeks_since_start_sq\`):** These capture the overall progression of the pandemic\\n        from its start, accounting for non-linear long-term changes in baseline admissions due to factors like vaccination,\\n        prior immunity, or evolving variants.\\n\\n    7.  **Geographical/Demographic Features (\`location\`, \`population\`):**\\n        *   \`location\`: Treated as a categorical feature, it allows the model to learn state-specific baselines,\\n            patterns, or responses that are not captured by other features. Different states have different population densities,\\n            healthcare systems, and policy responses.\\n        *   \`population\`: The target variable is normalized by population, making predictions more comparable across states\\n            and accounting for the scale of each jurisdiction. This feature also informs the final re-scaling back to\\n            total admissions.\\n\\n    8.  **Forecast Horizon (\`horizon\`):** This explicitly tells the model how many weeks into the future the prediction is being made.\\n        This is important as uncertainty typically increases with longer horizons, and the model can learn different\\n        predictive patterns for immediate vs. distant future weeks.\\n\\n    The **ensemble approach** (LightGBM and XGBoost) helps in combining the strengths of different tree-boosting algorithms,\\n    potentially leading to more robust and accurate quantile predictions by reducing reliance on a single model's biases.\\n    **Target transformations** (e.g., fourth root) are used to stabilize variance and make the target distribution\\n    more Gaussian-like, which often improves the performance of regression models, especially for count data.\\n    The **iterative forecasting** method for the test set is crucial for multi-step predictions, as it allows the model\\n    to use its own median predictions from earlier horizons to generate features for subsequent, further-out horizons,\\n    mimicking real-world forecasting where future observed data is unavailable.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability (more regularization)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.008,\\n        'max_depth': 4, # Slightly increased\\n        'min_child_weight': 50, # Increased for more stability\\n        'subsample': 0.7,\\n        'colsample_bytree': 0.7,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.5, # Increased for more regularization\\n        'reg_lambda': 10.0, # Increased for more L2 regularization\\n        'reg_alpha': 3.0 # Increased for more L1 regularization\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    xgb_params.pop('eval_metric', None) # Safely remove if present, not used for reg:quantile\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap. This value is critical for scaling and numerical stability.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 5000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1))\\n\\n    # A global cap for final predictions to prevent excessively large (e.g., infinite) scores.\\n    MAX_FINAL_PREDICTION = float(config.get('max_final_prediction', 200000.0))\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    # Also, ensure population is float to avoid integer division issues.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL]).astype(float)\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values.\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million.\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['day_of_week'] = df[DATE_COL].dt.dayofweek # Monday=0, Sunday=6.\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'day_of_week',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff first, then shift\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Horizon is always 0 for training data relative to observation, if not already present\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    for col in train_specific_features:\\n        # Fill NaNs within groups using forward/backward fill first\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n        # Fill any remaining global NaNs (e.g., if a whole group was NaN)\\n        if X_train_model[col].isnull().any():\\n            fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n    # Prepare categorical features for LightGBM and XGBoost\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    \\n    # LightGBM: use Pandas categorical type\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # XGBoost: map to integers\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories)}\\n    unknown_location_int = len(all_location_categories) if all_location_categories.size > 0 else 0\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names_lgbm = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names_lgbm)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i\\n                # XGBoost can handle categorical features for some objectives, but for reg:quantile\\n                # and consistency with integer mapping, explicitly disable internal categorical handling.\\n                xgb_model_params_i['enable_categorical'] = False \\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Store history for each location in a dictionary.\\n    # We use transformed target values for history to maintain consistency with features.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test data by location and date to process in time order for iterative forecasting\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Use the actual horizon for test set\\n\\n        # Generate time-series features based on current_loc_hist\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Difference between latest historical value and value \`diff_period\` weeks prior to that.\\n                diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (1 + diff_period)]\\n            else:\\n                diff_value = 0.0 # No sufficient history for difference\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # If some history exists, take mean of available\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback # Fallback for no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Fallback for insufficient history for std dev\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Ensure that the test row has all expected columns from training, filling missing ones with 0.0\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare test row for LightGBM\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        # Prepare test row for XGBoost\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q_idx, q in enumerate(QUANTILES):\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback if no valid predictions are generated by ensemble (e.g., all models fail/return NaN)\\n                fallback_value = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n                row_predictions_transformed[q] = fallback_value\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to valid range before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform predictions from per million transformed scale to per million original scale\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Scale predictions back to total admissions based on population\\n        # Ensure population_val is float for division\\n        population_val = float(row.loc[POPULATION_COL])\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Use the median prediction (0.5 quantile) for the current step's history for the next iteration\\n        # The value to add to history must be in the TRANSFORMED target space.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5] # This is already transformed\\n        \\n        # Clip again for safety, just in case \`row_predictions_transformed\` somehow went out of bounds\\n        value_to_add_to_history = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE) \\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negative\\n    predictions_array = np.maximum(0.0, predictions_array)\\n\\n    # Apply post-processing row by row\\n    for i in range(predictions_array.shape[0]):\\n        # 2. Ensure monotonicity (sort quantiles)\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        # 3. Enforce minimum spread between adjacent quantiles\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # 4. Global clipping to prevent excessively large predictions (applied after monotonicity for safety)\\n    predictions_array = np.clip(predictions_array, 0, MAX_FINAL_PREDICTION)\\n\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    # Round to nearest integer for final submission format\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    },\\n    { # Config 2: Ensemble of LGBM (3 members) and XGBoost (3 members), fourth_root transform.\\n      # XGBoost params adjusted for more regularization to improve stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.008,\\n            'max_depth': 4, # Increased for more learning capacity given regularization\\n            'min_child_weight': 50, # Increased for more stability\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.5, # Increased for more regularization\\n            'reg_lambda': 10.0, # Increased for more L2 regularization\\n            'reg_alpha': 3.0 # Increased for more L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 3,\\n        'n_xgb_ensemble_members': 3,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    },\\n    { # Config 3: Ensemble of LGBM (3 members) and XGBoost (3 members), log1p transform.\\n      # XGBoost params adjusted for more regularization to improve stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.008,\\n            'max_depth': 4, # Increased for more learning capacity given regularization\\n            'min_child_weight': 50, # Increased for more stability\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.5, # Increased for more regularization\\n            'reg_lambda': 10.0, # Increased for more L2 regularization\\n            'reg_alpha': 3.0 # Increased for more L1 regularization\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 3,\\n        'n_xgb_ensemble_members': 3,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    }\\n]",
  "new_index": "1717",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), population-normalized and transformed\\n    target variables, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version aims to generalize well by:\\n    - Allowing configuration of LGBM and XGBoost parameters.\\n    - Supporting different target transformations (fourth_root, log1p, sqrt, or none).\\n    - Implementing robust feature engineering including lags, differences, and rolling statistics.\\n    - Using an iterative forecasting approach for multi-step predictions.\\n    - Including post-processing to ensure non-negative, monotonic, and sufficiently spread quantile predictions.\\n    - Addressing numerical stability with appropriate clipping and fallback values.\\n\\n    **Model Predictions and Feature Importance Explanation:**\\n\\n    The model's predictions are primarily driven by **recent historical trends** of COVID-19 hospital admissions in each state.\\n    Key features influencing predictions include:\\n\\n    1.  **Lagged Target Values (\`lag_X_wk\`):** These are among the most crucial features, representing the number of admissions\\n        from previous weeks (e.g., 1 week ago, 4 weeks ago, 8 weeks ago, up to a year ago). These directly capture\\n        the recent trajectory and seasonality of the pandemic within a specific location. A high value last week\\n        will likely lead to a high prediction this week, and vice-versa.\\n\\n    2.  **Lagged Differences (\`diff_lag_1_period_X_wk\`):** These features capture the week-over-week change in admissions.\\n        They indicate the *rate of change* or momentum (e.g., rapid increase or decrease), which is vital for\\n        forecasting turning points or accelerating/decelerating trends.\\n\\n    3.  **Rolling Means (\`rolling_mean_X_wk\`):** These provide a smoothed average of admissions over longer periods (e.g., 8, 16, 26 weeks).\\n        They help in capturing underlying trends, filtering out noise, and understanding the general level of admissions.\\n\\n    4.  **Rolling Standard Deviations (\`rolling_std_X_wk\`):** These measure the volatility or spread of admissions over a recent period.\\n        Higher standard deviation might indicate more unpredictable periods, which can influence the width of the predicted quantiles.\\n\\n    5.  **Seasonal Features (\`sin_week_of_year\`, \`cos_week_of_year\`, \`week_of_year\`, \`day_of_week\`):** These cyclic features allow the model to learn\\n        yearly patterns in admissions (e.g., typical winter surges, summer lulls) that are common across locations or specific to some.\\n        \`day_of_week\` captures any potential weekly patterns, though for weekly aggregated data, it might be constant.\\n\\n    6.  **Long-Term Trend Features (\`weeks_since_start\`, \`weeks_since_start_sq\`):** These capture the overall progression of the pandemic\\n        from its start, accounting for non-linear long-term changes in baseline admissions due to factors like vaccination,\\n        prior immunity, or evolving variants.\\n\\n    7.  **Geographical/Demographic Features (\`location\`, \`population\`):**\\n        *   \`location\`: Treated as a categorical feature, it allows the model to learn state-specific baselines,\\n            patterns, or responses that are not captured by other features. Different states have different population densities,\\n            healthcare systems, and policy responses.\\n        *   \`population\`: The target variable is normalized by population, making predictions more comparable across states\\n            and accounting for the scale of each jurisdiction. This feature also informs the final re-scaling back to\\n            total admissions.\\n\\n    8.  **Forecast Horizon (\`horizon\`):** This explicitly tells the model how many weeks into the future the prediction is being made.\\n        This is important as uncertainty typically increases with longer horizons, and the model can learn different\\n        predictive patterns for immediate vs. distant future weeks.\\n\\n    The **ensemble approach** (LightGBM and XGBoost) helps in combining the strengths of different tree-boosting algorithms,\\n    potentially leading to more robust and accurate quantile predictions by reducing reliance on a single model's biases.\\n    **Target transformations** (e.g., fourth root) are used to stabilize variance and make the target distribution\\n    more Gaussian-like, which often improves the performance of regression models, especially for count data.\\n    The **iterative forecasting** method for the test set is crucial for multi-step predictions, as it allows the model\\n    to use its own median predictions from earlier horizons to generate features for subsequent, further-out horizons,\\n    mimicking real-world forecasting where future observed data is unavailable.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability (less aggressive regularization)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.05, # Increased from 0.008\\n        'max_depth': 6, # Increased from 4\\n        'min_child_weight': 1, # Decreased from 50 (default is 1)\\n        'subsample': 0.7,\\n        'colsample_bytree': 0.7,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.0, # Decreased from 0.5 (default is 0)\\n        'reg_lambda': 1.0, # Decreased from 10.0 (default is 1)\\n        'reg_alpha': 0.0 # Decreased from 3.0 (default is 0)\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    xgb_params.pop('eval_metric', None) # Safely remove if present, not used for reg:quantile\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap. This value is critical for scaling and numerical stability.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 5000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals.\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1))\\n\\n    # A global cap for final predictions to prevent excessively large (e.g., infinite) scores.\\n    MAX_FINAL_PREDICTION = float(config.get('max_final_prediction', 200000.0))\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    # Also, ensure population is float to avoid integer division issues.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL]).astype(float)\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values.\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million.\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['day_of_week'] = df[DATE_COL].dt.dayofweek # Monday=0, Sunday=6.\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'day_of_week',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff first, then shift\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Horizon is always 0 for training data relative to observation, if not already present\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    # Ensure fallback value is within valid range of transformed target\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    for col in train_specific_features:\\n        # Fill NaNs within groups using forward/backward fill first\\n        X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n        # Fill any remaining global NaNs (e.g., if a whole group was NaN)\\n        if X_train_model[col].isnull().any():\\n            fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n    # Prepare categorical features for LightGBM and XGBoost\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    \\n    # LightGBM: use Pandas categorical type\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # XGBoost: map to integers\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories)}\\n    unknown_location_int = len(all_location_categories) if all_location_categories.size > 0 else 0\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names_lgbm = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names_lgbm)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i\\n                # XGBoost can handle categorical features for some objectives, but for reg:quantile\\n                # and consistency with integer mapping, explicitly disable internal categorical handling.\\n                xgb_model_params_i['enable_categorical'] = False \\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Store history for each location in a dictionary.\\n    # We use transformed target values for history to maintain consistency with features.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test data by location and date to process in time order for iterative forecasting\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Use the actual horizon for test set\\n\\n        # Generate time-series features based on current_loc_hist\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Difference between latest historical value and value \`diff_period\` weeks prior to that.\\n                diff_value = current_loc_hist[len(current_loc_hist) - 1] - current_loc_hist[len(current_loc_hist) - (1 + diff_period)]\\n            else:\\n                diff_value = 0.0 # No sufficient history for difference\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # If some history exists, take mean of available\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback # Fallback for no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Fallback for insufficient history for std dev\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Ensure that the test row has all expected columns from training, filling missing ones with 0.0\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare test row for LightGBM\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        # Prepare test row for XGBoost\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q_idx, q in enumerate(QUANTILES):\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                    if np.isfinite(pred): # Only include finite predictions\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback if no valid predictions are generated by ensemble (e.g., all models fail/return NaN)\\n                fallback_value = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n                row_predictions_transformed[q] = fallback_value\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to valid range before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform predictions from per million transformed scale to per million original scale\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Scale predictions back to total admissions based on population\\n        # Ensure population_val is float for division\\n        population_val = float(row.loc[POPULATION_COL])\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Use the median prediction (0.5 quantile) for the current step's history for the next iteration\\n        # The value to add to history must be in the TRANSFORMED target space.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5] # This is already transformed\\n        \\n        # Clip again for safety, just in case \`row_predictions_transformed\` somehow went out of bounds\\n        value_to_add_to_history = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE) \\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negative\\n    predictions_array = np.maximum(0.0, predictions_array)\\n\\n    # Apply post-processing row by row\\n    for i in range(predictions_array.shape[0]):\\n        # 2. Ensure monotonicity (sort quantiles)\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        # 3. Enforce minimum spread between adjacent quantiles\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # 4. Global clipping to prevent excessively large predictions (applied after monotonicity for safety)\\n    predictions_array = np.clip(predictions_array, 0, MAX_FINAL_PREDICTION)\\n\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    # Round to nearest integer for final submission format\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    },\\n    { # Config 2: Ensemble of LGBM (3 members) and XGBoost (3 members), fourth_root transform.\\n      # XGBoost params adjusted for less aggressive regularization.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.05,\\n            'max_depth': 6, \\n            'min_child_weight': 1, \\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.0, \\n            'reg_lambda': 1.0, \\n            'reg_alpha': 0.0 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 3,\\n        'n_xgb_ensemble_members': 3,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    },\\n    { # Config 3: Ensemble of LGBM (3 members) and XGBoost (3 members), log1p transform.\\n      # XGBoost params adjusted for less aggressive regularization.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.05,\\n            'max_depth': 6, \\n            'min_child_weight': 1, \\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.0, \\n            'reg_lambda': 1.0, \\n            'reg_alpha': 0.0 \\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 3,\\n        'n_xgb_ensemble_members': 3,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1,\\n        'max_final_prediction': 200000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1171",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses solely on LightGBM due to its better performance in previous trials\\n    and to simplify the model, while maintaining robust data handling, feature engineering,\\n    and quantile prediction methodologies.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    # Define common constants\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM, can be overridden by 'lgbm_params' in config\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1, # Use all available CPU cores\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters and target transformation type\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1) # Number of LightGBM models per quantile\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0) # Clip extreme values\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data to ensure correct time-series operations (lags, rolling)\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Normalize target by population to create a 'per million' metric\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme outliers\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    # These functions will be used for both training target and for prediction\\n    def identity_transform(x): return x\\n    def identity_inverse_transform(x): return x\\n\\n    inverse_transform_fn = identity_inverse_transform\\n    forward_transform_fn = identity_transform\\n\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform_fn(x):\\n            # Ensure non-negative and prevent issues with very small values before inverse power\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform_fn(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform_fn(x):\\n            return np.expm1(x)\\n        def forward_transform_fn(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform_fn(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform_fn(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform_fn(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min() # Base date for 'weeks_since_start'\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, cast to int\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        # Add cyclical features for week of year to capture seasonality\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Time elapsed since the beginning of data for trend modeling\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Add a quadratic term for non-linear trend\\n\\n        return df\\n\\n    # Apply base feature engineering to both training and test data\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Only location is treated as categorical for tree models\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Create lagged features for the transformed target\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Create lagged difference features\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff based on lagged data to avoid data leakage\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Create rolling mean features\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Create rolling standard deviation features\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Collect all generated time-series specific features\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    # Combine base features and time-series specific features\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean for imputation (after target transformation and clipping)\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform_fn(1.0)\\n    # Ensure fallback value is within the clipped range\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            # This fills NaNs using the nearest valid observation within the same location's history\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point).\\n            # This is critical for robustness in early folds where history might be very short.\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Combine X and y for training, then drop rows where target is NaN (due to lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Drop rows where target is NaN\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM.\\n    # Collect all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    # Prepare X_train for LightGBM by setting 'location' as a categorical Dtype\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        models[q]['lgbm'] = []\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set quantile for LGBM's 'quantile' objective\\n            # Vary seed for ensemble members to introduce diversity\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q]['lgbm'].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation.\\n    # Initialize with transformed values from the training set.\\n    # This dictionary will store lists of historical transformed admissions for each location.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index # Preserve original index for output DataFrame\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize DataFrame to store predictions\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Iterate through each row in the sorted test data to make predictions\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.at[LOCATION_COL]\\n        original_idx = row.at['original_index']\\n\\n        # Get the history for the current location. If no history, use an empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base features and horizon\\n        current_features_dict = {col: row.at[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.at[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.at[HORIZON_COL]\\n\\n        # Generate time-series features for the current test row using available history\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            else:\\n                # If history is too short for this specific lag, use global mean fallback\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Calculate difference relative to the previous week (lag 1)\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # If not enough for full diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 # No sufficient history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history if less than window\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback # Fallback if no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # This is critical for consistent feature input to the trained models.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM by setting 'location' as a categorical Dtype\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                # Average predictions from available ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for this quantile failed or are not present,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        # Convert dictionary to array for easy processing\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform the predictions to the original scale (admissions per million)\\n        inv_preds_admissions_per_million = inverse_transform_fn(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Scale by population to get total admissions\\n        population_val = row.at[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results in final predictions\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative values\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        # Store predictions in the output DataFrame\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        # This is a crucial step for autoregressive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5] # Use the median quantile prediction\\n\\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_admissions_per_million = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform the median prediction\\n        median_pred_admissions_per_million = inverse_transform_fn(median_pred_transformed_admissions_per_million)\\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Forward transform the median prediction before adding to history to keep history in transformed scale\\n        value_to_add_to_history = forward_transform_fn(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Append the new predicted value to the history for the current location\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Final Post-processing ---\\n    # Ensure monotonicity of quantiles across all predictions (a requirement for submission)\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as admissions counts cannot be negative or fractional\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# This list defines different model configurations to be evaluated by the harness.\\n# The harness will iterate through these configs and report the performance of each.\\nconfig_list = [\\n    { # Config 0: Baseline LGBM-only with fourth_root transform.\\n        # This configuration prioritizes a robust transformation and a balanced LGBM setup.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Comprehensive set of lags\\n        'lag_diff_periods': [1, 2, 4, 8], # Short to medium term differences\\n        'rolling_windows': [8, 16, 26], # Medium to long term rolling means\\n        'rolling_std_windows': [4, 8], # Short to medium term rolling standard deviations\\n        'n_lgbm_ensemble_members': 1, # Using 1 for speed, can be increased for robustness (e.g., 3-5)\\n        'max_admissions_per_million': 10000.0 # Upper bound for target normalization\\n    },\\n    { # Config 1: LGBM-only with fourth_root transform, slightly more estimators and adjusted features.\\n        # This config explores a slightly deeper and more complex model with adjusted feature windows.\\n        'lgbm_params': {\\n            'n_estimators': 300, # Increased estimators to potentially capture more patterns\\n            'learning_rate': 0.025, # Slightly lower LR for better convergence with more estimators\\n            'num_leaves': 31, # Slightly more leaves\\n            'max_depth': 6, # Slightly deeper trees\\n            'min_child_samples': 25, # Slightly more regularization\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly more column subsampling\\n            'subsample': 0.75, # Slightly more row subsampling\\n            'reg_alpha': 0.15, # Slightly more L1 reg\\n            'reg_lambda': 0.15 # Slightly more L2 reg\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 12, 24, 36, 52], # Adjusted lags including 12, 24, 36 weeks\\n        'lag_diff_periods': [1, 3, 6, 12], # Adjusted diff periods\\n        'rolling_windows': [12, 24, 36], # Adjusted rolling windows to align with new lags\\n        'rolling_std_windows': [6, 12], # Adjusted rolling std windows\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 2: LGBM-only with log1p transform, similar parameters to Config 1.\\n        # This config tests an alternative target transformation (\`log1p\`) with the adjusted model parameters.\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.025,\\n            'num_leaves': 31,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'log1p', # Changed target transformation\\n        'lag_weeks': [1, 4, 8, 12, 24, 36, 52],\\n        'lag_diff_periods': [1, 3, 6, 12],\\n        'rolling_windows': [12, 24, 36],\\n        'rolling_std_windows': [6, 12],\\n        'n_lgbm_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0\\n    }\\n]",
  "new_index": "1229",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses solely on LightGBM due to its better performance in previous trials\\n    and to simplify the model, while maintaining robust data handling, feature engineering,\\n    and quantile prediction methodologies. It enhances robustness by supporting an ensemble\\n    of LightGBM models for each quantile.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    # Define common constants\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM, can be overridden by 'lgbm_params' in config\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1, # Use all available CPU cores\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Feature engineering parameters and target transformation type\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1) # Number of LightGBM models per quantile\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0) # Clip extreme values\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    # Sort data to ensure correct time-series operations (lags, rolling)\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Normalize target by population to create a 'per million' metric\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme outliers\\n    admissions_per_million = admissions_per_million.clip(lower=0.0, upper=MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    def identity_transform(x): return x\\n    def identity_inverse_transform(x): return x\\n\\n    inverse_transform_fn = identity_inverse_transform\\n    forward_transform_fn = identity_transform\\n\\n    if target_transform_type == 'fourth_root':\\n        # Apply transformation: (x + 1)^(1/4) to handle zeros and positive values\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform_fn(x):\\n            # Inverse: x^4 - 1.0. Ensure result is non-negative.\\n            return (np.power(x, 4) - 1.0).clip(lower=0.0)\\n        def forward_transform_fn(x):\\n            # Forward transform for new data: (x + 1)^(1/4). Ensure non-negative input.\\n            return np.power(x.clip(lower=0.0) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        # Apply transformation: log(x + 1)\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform_fn(x):\\n            # Inverse: exp(x) - 1.0. Ensure result is non-negative.\\n            return (np.expm1(x)).clip(lower=0.0)\\n        def forward_transform_fn(x):\\n            # Forward transform for new data: log(x + 1). Ensure non-negative input.\\n            return np.log1p(x.clip(lower=0.0))\\n    elif target_transform_type == 'sqrt':\\n        # Apply transformation: sqrt(x + 1)\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform_fn(x):\\n            # Inverse: x^2 - 1.0. Ensure result is non-negative.\\n            return (np.power(x, 2) - 1.0).clip(lower=0.0)\\n        def forward_transform_fn(x):\\n            # Forward transform for new data: sqrt(x + 1). Ensure non-negative input.\\n            return np.sqrt(x.clip(lower=0.0) + 1.0)\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    # Pass a Series to forward_transform_fn to use its clipping logic, then extract scalar.\\n    MAX_TRANSFORMED_VALUE = forward_transform_fn(pd.Series([MAX_ADMISSIONS_PER_MILLION])).iloc[0]\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = df_train_full[TRANSFORMED_TARGET_COL].clip(lower=0.0, upper=MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min() # Base date for 'weeks_since_start'\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, cast to int\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        # Add cyclical features for week of year to capture seasonality\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Time elapsed since the beginning of data for trend modeling\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Add a quadratic term for non-linear trend\\n\\n        return df\\n\\n    # Apply base feature engineering to both training and test data\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Only location is treated as categorical for tree models\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Create lagged features for the transformed target\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Create lagged difference features\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff based on lagged data to avoid data leakage\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Create rolling mean features\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Create rolling standard deviation features\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Collect all generated time-series specific features\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    # Combine base features and time-series specific features\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0 # All historical data corresponds to horizon 0\\n\\n    # Calculate fallback mean for imputation (after target transformation and clipping)\\n    # Ensure fallback value is obtained through the forward transform and within clipped range\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform_fn(pd.Series([1.0])).iloc[0]\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point).\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For standard deviations, 0.0 is appropriate when data is constant or very sparse.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Combine X and y for training, then drop rows where target is NaN (due to lags)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) # Drop rows where target is NaN\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle categorical features for LightGBM.\\n    # Collect all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n\\n    # Prepare X_train for LightGBM by setting 'location' as a categorical Dtype\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        models[q]['lgbm'] = []\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set quantile for LGBM's 'quantile' objective\\n            # Vary seed for ensemble members to introduce diversity\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q]['lgbm'].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation.\\n    # Initialize with transformed values from the training set.\\n    # This dictionary will store lists of historical transformed admissions for each location.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index # Preserve original index for output DataFrame\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize DataFrame to store predictions\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Iterate through each row in the sorted test data to make predictions\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.at[LOCATION_COL]\\n        original_idx = row.at['original_index']\\n\\n        # Get the history for the current location. If no history, use an empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base features and horizon\\n        current_features_dict = {col: row.at[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.at[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.at[HORIZON_COL]\\n\\n        # Generate time-series features for the current test row using available history\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            else:\\n                # If history is too short for this specific lag, use global mean fallback\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Calculate difference relative to the previous week (lag 1)\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # If not enough for full diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 # No sufficient history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history if less than window\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback # Fallback if no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # This is critical for consistent feature input to the trained models.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM by setting 'location' as a categorical Dtype\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                    if np.isfinite(pred): # Only add finite predictions to ensemble\\n                        ensemble_preds_for_q.append(pred)\\n\\n            if ensemble_preds_for_q:\\n                # Average predictions from available ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for this quantile failed or are not present,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        # Convert dictionary to array for easy processing\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = transformed_preds_array.clip(lower=0.0, upper=MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform the predictions to the original scale (admissions per million)\\n        inv_preds_admissions_per_million = inverse_transform_fn(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = inv_preds_admissions_per_million.clip(lower=0.0, upper=MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Scale by population to get total admissions\\n        population_val = row.at[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results in final predictions\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative values\\n        final_preds_total_admissions = np.round(final_preds_total_admissions.clip(lower=0)).astype(int)\\n\\n        # Store predictions in the output DataFrame\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        # This is a crucial step for autoregressive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5] # Use the median quantile prediction\\n\\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_admissions_per_million = median_pred_transformed_raw.clip(lower=0.0, upper=MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform the median prediction\\n        median_pred_admissions_per_million = inverse_transform_fn(pd.Series([median_pred_transformed_admissions_per_million])).iloc[0]\\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = median_pred_admissions_per_million.clip(lower=0.0, upper=MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Forward transform the median prediction before adding to history to keep history in transformed scale\\n        value_to_add_to_history = forward_transform_fn(pd.Series([median_pred_admissions_per_million])).iloc[0]\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE\\n        value_to_add_to_history = value_to_add_to_history.clip(lower=0.0, upper=MAX_TRANSFORMED_VALUE)\\n\\n        # Append the new predicted value to the history for the current location\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # --- 6. Final Post-processing ---\\n    # Ensure monotonicity of quantiles across all predictions (a requirement for submission)\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as admissions counts cannot be negative or fractional\\n    predictions_df = predictions_df.clip(lower=0).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# This list defines different model configurations to be evaluated by the harness.\\n# The harness will iterate through these configs and report the performance of each.\\nconfig_list = [\\n    { # Config 0: Based on previous best, now with n_lgbm_ensemble_members = 3\\n        # This configuration prioritizes a robust transformation and a balanced LGBM setup,\\n        # with enhanced robustness from ensembling.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Comprehensive set of lags\\n        'lag_diff_periods': [1, 2, 4, 8], # Short to medium term differences\\n        'rolling_windows': [8, 16, 26], # Medium to long term rolling means\\n        'rolling_std_windows': [4, 8], # Short to medium term rolling standard deviations\\n        'n_lgbm_ensemble_members': 3, # Increased for robustness by training 3 models per quantile\\n        'max_admissions_per_million': 10000.0 # Upper bound for target normalization\\n    },\\n    { # Config 1: LGBM-only with fourth_root transform, slightly more estimators and adjusted features, with ensemble.\\n        # This config explores a slightly deeper and more complex model with adjusted feature windows, ensembled.\\n        'lgbm_params': {\\n            'n_estimators': 300, # Increased estimators to potentially capture more patterns\\n            'learning_rate': 0.025, # Slightly lower LR for better convergence with more estimators\\n            'num_leaves': 31, # Slightly more leaves\\n            'max_depth': 6, # Slightly deeper trees\\n            'min_child_samples': 25, # Slightly more regularization\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75, # Slightly more column subsampling\\n            'subsample': 0.75, # Slightly more row subsampling\\n            'reg_alpha': 0.15, # Slightly more L1 reg\\n            'reg_lambda': 0.15 # Slightly more L2 reg\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 12, 24, 36, 52], # Adjusted lags including 12, 24, 36 weeks\\n        'lag_diff_periods': [1, 3, 6, 12], # Adjusted diff periods\\n        'rolling_windows': [12, 24, 36], # Adjusted rolling windows to align with new lags\\n        'rolling_std_windows': [6, 12], # Adjusted rolling std windows\\n        'n_lgbm_ensemble_members': 3, # Enabled ensemble\\n        'max_admissions_per_million': 10000.0\\n    },\\n    { # Config 2: LGBM-only with log1p transform, similar parameters to Config 1, with ensemble.\\n        # This config tests an alternative target transformation (\`log1p\`) with the adjusted model parameters and ensembling.\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.025,\\n            'num_leaves': 31,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'log1p', # Changed target transformation\\n        'lag_weeks': [1, 4, 8, 12, 24, 36, 52],\\n        'lag_diff_periods': [1, 3, 6, 12],\\n        'rolling_windows': [12, 24, 36],\\n        'rolling_std_windows': [6, 12],\\n        'n_lgbm_ensemble_members': 3, # Enabled ensemble\\n        'max_admissions_per_million': 10000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "953",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on robustness:\\n    - Numerical stability with robust clipping and imputation.\\n    - Consistent handling of 'location' as an integer ID.\\n    - Defensive programming against NaN/Inf values during feature generation and prediction.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location' # This column represents location ID, which is a string in raw data (e.g., '01')\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3,\\n        'min_child_weight': 10, # Increased later by config\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.5,\\n        'lambda': 1.0,\\n        'alpha': 0.1 # This 'alpha' is L1 regularization, not quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 2500.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    \\n    # Ensure LOCATION_COL is an integer type consistently across train and test\\n    # (e.g., '01' becomes 1, '06' becomes 6)\\n    df_train_full[LOCATION_COL] = df_train_full[LOCATION_COL].astype(int)\\n    \\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate admissions per million, handling zero population gracefully\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    \\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n    \\n    # Ensure LOCATION_COL is an integer type in test_x_processed as well\\n    test_x_processed[LOCATION_COL] = test_x_processed[LOCATION_COL].astype(int)\\n\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    \\n    # LOCATION_COL is an integer. LightGBM can explicitly treat it as categorical.\\n    # XGBoost will treat it as a numerical feature.\\n    CATEGORICAL_FEATURES_FOR_LGBM = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + [LOCATION_COL] + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    # Handle cases where y_train_model might be empty or all NaNs (should not happen with valid input)\\n    mean_transformed_train_y_fallback = y_train_model.mean()\\n    if pd.isna(mean_transformed_train_y_fallback):\\n        mean_transformed_train_y_fallback = 0.0 # Default if mean is NaN (e.g., all y_train_model values were NaN)\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point)\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y)\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                # Pass integer-encoded LOCATION_COL as categorical_feature explicitly\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=CATEGORICAL_FEATURES_FOR_LGBM)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Quantile for quantile regression\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # LOCATION_COL is an integer. XGBoost treats it numerically.\\n                xgb_model.fit(X_train_model, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation\\n    # history should contain transformed values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL] # This is already an integer\\n        original_idx = row.loc['original_index']\\n        \\n        # Access history for the current location. If not present, initialize with empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and a numerical horizon feature.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL] # Integer location ID\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Numerical\\n\\n        # Generate time-series features\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the last available value if history is too short for specific lag\\n                lag_value = current_loc_hist[-1]\\n            else: # If no history for this location, use global fallback mean\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) > 1: # If not enough for diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # No history or only one point for diff calculation\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else: # No history for this location\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Use all available history\\n                rolling_std_val = np.std(current_loc_hist)\\n            else: # Not enough history for std dev\\n                rolling_std_val = 0.0 \\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        # Fill any missing columns (e.g., new features not handled above) with 0.0\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only append if finite\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only append if finite\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                # Average finite predictions. This is safe.\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If no finite predictions from any model for this quantile, use fallback\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        \\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle zero or very small population values to avoid division by zero or extreme scaling\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round and ensure non-negativity and integer type\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        \\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n        \\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles\\n    # This step is crucial to satisfy the competition requirements\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1)\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform and improved clipping/imputation.\\n      # This config previously scored 41.39 and is the most stable.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 2500.0 \\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform.\\n      # XGBoost parameters are made more conservative to avoid 'inf' scores.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators to balance with higher regularization\\n            'learning_rate': 0.03,\\n            'max_depth': 2, # More conservative depth\\n            'min_child_weight': 50, # Significantly increased regularization\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0, # Increased regularization\\n            'lambda': 1.5, # Increased L2 regularization\\n            'alpha': 0.5 # Increased L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2500.0\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and very conservative XGBoost, fourth_root, and expanded features.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.025,\\n            'max_depth': 2, # More conservative depth\\n            'min_child_weight': 70, # Significantly increased regularization\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.5, # Increased regularization\\n            'lambda': 2.0, # Increased L2 regularization\\n            'alpha': 0.8 # Increased L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2500.0\\n    },\\n    { # Config 4: Ensemble with 2 members for each model type, very conservative XGBoost.\\n        'lgbm_params': {\\n            'n_estimators': 150, \\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 100, # Further reduced estimators\\n            'learning_rate': 0.03,\\n            'max_depth': 2, # Very conservative depth\\n            'min_child_weight': 100, # Very high regularization\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 2.0, # Very high regularization\\n            'lambda': 2.5, # Very high L2 regularization\\n            'alpha': 1.0 # Very high L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 2500.0\\n    }\\n]",
  "new_index": "996",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on robustness:\\n    - Numerical stability with robust clipping and imputation.\\n    - Consistent handling of 'location' as an integer ID.\\n    - Defensive programming against NaN/Inf values during feature generation and prediction.\\n    - Corrected XGBoost parameters for quantile objective, using \`reg_alpha\` and \`reg_lambda\`\\n      for L1/L2 regularization to avoid conflicts with the quantile \`alpha\` parameter.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location' # This column represents location ID, which is a string in raw data (e.g., '01')\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Corrected XGBoost parameters for quantile objective:\\n    # 'alpha' is specifically for the quantile level when objective='reg:quantile'.\\n    # L1/L2 regularization should be specified using 'reg_alpha' and 'reg_lambda'.\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile', \\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3,\\n        'min_child_weight': 10,\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 2500.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    \\n    # Ensure LOCATION_COL is an integer type consistently across train and test\\n    # (e.g., '01' becomes 1, '06' becomes 6)\\n    df_train_full[LOCATION_COL] = df_train_full[LOCATION_COL].astype(int)\\n    \\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate admissions per million, handling zero population gracefully\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    \\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n    \\n    # Ensure LOCATION_COL is an integer type in test_x_processed as well\\n    test_x_processed[LOCATION_COL] = test_x_processed[LOCATION_COL].astype(int)\\n\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    \\n    # LOCATION_COL is an integer. LightGBM can explicitly treat it as categorical.\\n    # XGBoost will treat it as a numerical feature.\\n    CATEGORICAL_FEATURES_FOR_LGBM = [LOCATION_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + [LOCATION_COL] + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    X_train_model[HORIZON_COL] = 0\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    # Handle cases where y_train_model might be empty or all NaNs (should not happen with valid input)\\n    mean_transformed_train_y_fallback = y_train_model.mean()\\n    if pd.isna(mean_transformed_train_y_fallback):\\n        mean_transformed_train_y_fallback = 0.0 # Default if mean is NaN (e.g., all y_train_model values were NaN)\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point)\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                # Pass integer-encoded LOCATION_COL as categorical_feature explicitly\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=CATEGORICAL_FEATURES_FOR_LGBM)\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                # Crucial: For 'reg:quantile' objective, 'alpha' is the quantile level itself.\\n                xgb_model_params_i['alpha'] = q \\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # LOCATION_COL is an integer. XGBoost treats it numerically.\\n                xgb_model.fit(X_train_model, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation\\n    # history should contain transformed values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL] # This is already an integer\\n        original_idx = row.loc['original_index']\\n        \\n        # Access history for the current location. If not present, initialize with empty list.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and a numerical horizon feature.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL] # Integer location ID\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Numerical\\n\\n        # Generate time-series features\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the last available value if history is too short for specific lag\\n                lag_value = current_loc_hist[-1] # Fallback to most recent\\n            else: # If no history for this location, use global fallback mean\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) > 1: # If not enough for diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # No history or only one point for diff calculation\\n                diff_value = 0.0 # Default to 0 difference if insufficient history\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else: # No history for this location\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Use all available history\\n                rolling_std_val = np.std(current_loc_hist)\\n            else: # Not enough history for std dev\\n                rolling_std_val = 0.0 # Default to 0 std if insufficient history\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only append if finite\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    # Use try-except for XGBoost prediction to catch potential issues\\n                    try:\\n                        pred = xgb_model_q.predict(X_test_row)[0]\\n                        if np.isfinite(pred): # Only append if finite\\n                            ensemble_preds_for_q.append(pred)\\n                    except Exception as e:\\n                        # Silently skip problematic XGBoost predictions to prevent 'inf' scores.\\n                        # In a real debugging scenario, one might log this error.\\n                        pass \\n\\n            if ensemble_preds_for_q:\\n                # Average finite predictions. This is safe.\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If no finite predictions from any model for this quantile, use fallback\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        \\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle zero or very small population values to avoid division by zero or extreme scaling\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round and ensure non-negativity and integer type\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        # Ensure that the value added to history is also non-negative and clipped,\\n        # following the same transformation path.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        \\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n        \\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE and non-negative\\n        value_to_add_to_history = np.clip(np.maximum(0.0, value_to_add_to_history), 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles\\n    # This step is crucial to satisfy the competition requirements\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1)\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform and improved clipping/imputation.\\n      # This config previously scored 41.39 and is the most stable.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 2500.0 \\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform.\\n      # Corrected XGBoost parameters for quantile objective (using reg_alpha/reg_lambda).\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, \\n            'learning_rate': 0.03,\\n            'max_depth': 3,\\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'reg_alpha': 0.1, \\n            'reg_lambda': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2500.0\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost, fourth_root, and expanded features.\\n      # Still with corrected XGBoost params.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, \\n            'learning_rate': 0.025,\\n            'max_depth': 4, \\n            'min_child_weight': 10, \\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'reg_alpha': 0.15, \\n            'reg_lambda': 0.15 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2500.0\\n    },\\n    { # Config 4: Ensemble with 2 members for each model type.\\n      # Use moderate parameters for robustness.\\n        'lgbm_params': {\\n            'n_estimators': 150, \\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 30, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 100, \\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'reg_alpha': 0.1, \\n            'reg_lambda': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 2500.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

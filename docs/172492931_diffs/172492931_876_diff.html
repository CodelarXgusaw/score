<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "718",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features (including lagged target variables),\\n    a population-normalized and transformed target variable, and location information.\\n    The approach uses an iterative prediction strategy for the test set to correctly\\n    calculate lagged and rolling features for future steps, using median predictions\\n    to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # This column is present in test_x, but not in train_x directly\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of LightGBM models per quantile (for internal ensemble/robustness)\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default in case of empty training data or other issues.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    # Ensure population is not zero or NaN to prevent division by zero/NaNs.\\n    # A small positive epsilon ensures numerical stability for 'per million' calculations.\\n    POPULATION_EPSILON = 1.0 # Using 1.0 as the baseline for 'per million' scale.\\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON)) # Ensure minimum value\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before log1p\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        # Ensure DATE_COL is datetime type\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n        \\n        # The 'horizon' column is only present in test_x. For train_x, we implicitly set it to 0.\\n        # It's treated as a numerical feature.\\n        if HORIZON_COL not in df.columns:\\n            df[HORIZON_COL] = 0 \\n\\n        return df\\n\\n    # Features that are always numerical\\n    BASE_NUMERICAL_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                               'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                               'weeks_since_start_sq', HORIZON_COL]\\n\\n    # Features that are always categorical\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    # test_x already contains the 'horizon' column, so add_base_features will pick it up\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaNs from std of single/few elements with 0 (e.g., if only one value in window, std is 0)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_dropna = BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_dropna].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # This is critical for handling missing values at the beginning of a series\\n    # which result from lag/rolling operations, ensuring no NaNs are passed to LGBM.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining NaNs at the very beginning of a series (if ffill couldn't apply)\\n            # Defaulting to 0.0 assumes minimal or zero activity before observation.\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target or essential (lagged) features are NaN *after* filling.\\n    # This ensures that the model is trained only on complete and valid data points.\\n    # We include \`POPULATION_COL\` as it's critical for target scaling.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL] + [POPULATION_COL], inplace=True)\\n\\n    # Check if training data is empty after dropping NaNs.\\n    # If it is, no models can be trained, return placeholder predictions (all zeros).\\n    if train_combined.empty:\\n        print(\\"WARNING: Training data became empty after feature engineering and NaN dropping. Returning default predictions (zeros).\\")\\n        return predictions_df \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    models = {q: [] for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q \\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i \\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            \\n            # Check if X_train_model and y_train_model are not empty before fitting\\n            if not X_train_model.empty and not y_train_model.empty:\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q].append(lgbm_model)\\n            else:\\n                print(f\\"WARNING: LightGBM skipped fitting for quantile {q} due to empty training data.\\")\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This history will be updated with median predictions for future steps.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base (numerical and categorical) features.\\n        current_features_dict = {col: row.get(col, 0) for col in BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = 0.0 # Default if no history at all\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: \\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 \\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points to calculate std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # If insufficient data, std is 0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        \\n        # Make predictions for all quantiles for this single row using LGBM models.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            lgbm_preds_for_q = []\\n            if models[q]: # Check if any models were trained for this quantile\\n                for lgbm_model_q in models[q]:\\n                    lgbm_preds_for_q.append(lgbm_model_q.predict(X_test_row)[0])\\n                row_predictions_transformed[q] = np.mean(lgbm_preds_for_q) # Average predictions from ensemble members\\n            else:\\n                # Fallback if no models were trained (e.g., empty training data)\\n                row_predictions_transformed[q] = 0.0 \\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle cases where population is NaN or zero for predictions in the test set.\\n        safe_population_test = POPULATION_EPSILON\\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        # This median prediction is crucial for generating future lagged features in the iterative process.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.maximum(0.0, median_pred_admissions_per_million)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    # This step is critical for producing valid quantile forecasts.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Base LGBM parameters (from previous best performing config)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 1, # Single LGBM model per quantile for simplicity\\n    },\\n    { # Config 2: Slightly increased complexity in LGBM and more varied features\\n        'lgbm_params': {\\n            'n_estimators': 300,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # More granular lags\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26], # Wider range of rolling windows\\n        'rolling_std_windows': [4, 8, 12],\\n        'n_lgbm_ensemble_members': 1,\\n    },\\n    { # Config 3: Focus on longer-term trends with LGBM parameters tuned for robustness\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 20, # Slightly fewer leaves for more generalized trees\\n            'max_depth': 4, # Slightly shallower trees\\n            'min_child_samples': 30, # More samples per leaf\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n            'reg_alpha': 0.2, # Stronger L1 regularization\\n            'reg_lambda': 0.2 # Stronger L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 8, 26, 52], # Focus on immediate and annual seasonality\\n        'lag_diff_periods': [1, 8], \\n        'rolling_windows': [12, 26, 52], # Longer-term smoothing\\n        'rolling_std_windows': [12, 26],\\n        'n_lgbm_ensemble_members': 1,\\n    }\\n]",
  "new_index": "876",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression.\\n    It incorporates time-series features (including lagged target variables),\\n    a population-normalized and transformed target variable, and location information.\\n    The approach uses an iterative prediction strategy for the test set to correctly\\n    calculate lagged and rolling features for future steps, using median predictions\\n    to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' # This column is present in test_x, but not in train_x directly\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Model and Feature Engineering ---\\n    # Default LightGBM parameters for quantile regression.\\n    # 'objective': 'quantile' makes it predict specific quantiles directly.\\n    # 'alpha' parameter for quantile is set per model instance.\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200, # Number of boosting rounds\\n        'learning_rate': 0.03, # Step size shrinkage to prevent overfitting\\n        'num_leaves': 25, # Max number of leaves in one tree\\n        'max_depth': 5, # Max depth of a tree, controls complexity\\n        'min_child_samples': 20, # Minimum data in a leaf, prevents overfitting\\n        'random_state': 42, # For reproducibility\\n        'n_jobs': -1, # Use all available cores\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8, # Subsample ratio of columns when constructing each tree\\n        'subsample': 0.8, # Subsample ratio of the training instance\\n        'reg_alpha': 0.1, # L1 regularization\\n        'reg_lambda': 0.1 # L2 regularization\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Lagged features: how many weeks back to look for past admissions.\\n    # These are typically the most important features in time-series forecasting,\\n    # as past values strongly predict future values due to autocorrelation.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    # Lagged differences: capture changes in trend over different periods.\\n    # They indicate acceleration or deceleration of admissions.\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    # Rolling means: smooth out short-term fluctuations and capture trends.\\n    # These provide information about the average level of recent admissions.\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    # Rolling standard deviations: capture volatility or uncertainty in the data.\\n    # Higher std suggests more unpredictable changes in admissions.\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    # Target transformation: Helps handle skewed data (like counts) and stabilize variance.\\n    # Fourth root is often effective for count data, making the distribution more Gaussian-like\\n    # and improving model performance for regression tasks.\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # Number of LightGBM models per quantile. Averaging predictions from multiple models\\n    # (even with slight randomness, e.g., different random_state offsets) can improve robustness\\n    # and slightly reduce variance, akin to a small ensemble.\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n    \\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n    # Fill with zeros as a safe default in case of empty training data or other issues.\\n    for col in predictions_df.columns:\\n        predictions_df[col] = 0\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    # Ensure date column is datetime type and sort the dataframe by location and date.\\n    # This sorting is crucial for correct calculation of time-series features.\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # --- Data Preprocessing: Handle Population and Transform Target ---\\n    # Population is a critical feature, as it allows normalization of admissions\\n    # so that comparisons and modeling can be done on a \\"per capita\\" basis.\\n    # This prevents the model from simply correlating with state size.\\n    # Ensure population is not zero or NaN to prevent division by zero/NaNs when normalizing.\\n    POPULATION_EPSILON = 1.0 # Ensures a minimum population of 1 for 'per million' calculation.\\n    \\n    safe_population_train = df_train_full[POPULATION_COL].fillna(POPULATION_EPSILON)\\n    safe_population_train = safe_population_train.apply(lambda x: max(x, POPULATION_EPSILON))\\n\\n    admissions_per_million_train = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n    admissions_per_million_train = np.maximum(0.0, admissions_per_million_train) # Ensure non-negative\\n\\n    # Define transform and inverse transform functions based on config.\\n    # Adding/subtracting 1.0 to handle zero values gracefully with power/log transforms.\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million_train)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before log1p\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million_train + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million_train + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million_train\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min() if not df_train_full.empty else pd.Timestamp('2020-01-01')\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        # Ensure DATE_COL is datetime type\\n        if not pd.api.types.is_datetime64_any_dtype(df[DATE_COL]):\\n            df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        # Extract temporal components. These capture seasonality (e.g., winter surges,\\n        # holiday impact on healthcare seeking/reporting).\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        # This prevents abrupt jumps at the end/beginning of the year, allowing\\n        # the model to see, for example, that week 52 is \\"close\\" to week 1.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend (e.g., pandemic progression,\\n        # long-term changes in public health measures or population immunity).\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Quadratic term for non-linear trend\\n\\n        # The 'horizon' column indicates how far into the future the prediction is.\\n        # It's a key feature for multi-step forecasts, allowing the model to learn\\n        # different patterns or error growth at different horizons (e.g., more uncertainty further out).\\n        if HORIZON_COL not in df.columns:\\n            df[HORIZON_COL] = 0 \\n\\n        return df\\n\\n    # Features that are always numerical\\n    BASE_NUMERICAL_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                               'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                               'weeks_since_start_sq', HORIZON_COL]\\n\\n    # Location is treated as a categorical feature, allowing the model to learn\\n    # state-specific patterns and baselines, accounting for unobserved heterogeneity\\n    # such as different demographics, healthcare systems, or local policies.\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    # test_x already contains the 'horizon' column, so add_base_features will pick it up\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # These are the most direct predictors, capturing the memory of the time series.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features.\\n    # These capture rates of change (e.g., weekly increase/decrease), which are\\n    # important indicators of accelerating or decelerating trends.\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    # Rolling means provide a smoothed view of recent history, capturing\\n    # the underlying trend without being overly influenced by daily fluctuations.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    # Rolling std captures recent volatility or variability. This is highly important\\n    # for quantile regression, as it helps the model understand the expected spread\\n    # of the target variable.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaNs (e.g., std of a single element) with 0, as std of one value is undefined.\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_dropna = BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_dropna].copy()\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # This is critical for handling missing values that result from lag/rolling operations,\\n    # especially at the beginning of a time series for a given location, ensuring no NaNs\\n    # are passed to LightGBM.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Fill NaNs using the last valid observation for each location.\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # Fill any remaining NaNs at the very beginning of a series (if ffill couldn't apply).\\n            # Defaulting to 0.0 assumes minimal or zero activity before observation,\\n            # which is a reasonable fallback for admissions counts.\\n            X_train_model[col] = X_train_model[col].fillna(0.0) \\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where target or essential (lagged) features are NaN *after* filling.\\n    # This ensures that the model is trained only on complete and valid data points.\\n    # Population is included in subset as it's critical for target scaling.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL] + [POPULATION_COL], inplace=True)\\n\\n    # Check if training data is empty after dropping NaNs.\\n    # If it is, no models can be trained, and we must return placeholder predictions (all zeros).\\n    if train_combined.empty:\\n        print(\\"WARNING: Training data became empty after feature engineering and NaN dropping. Returning default predictions (zeros).\\")\\n        return predictions_df \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    # This is important to prevent errors if a location appears only in test_x\\n    # but not in train_x.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    # LightGBM requires feature columns to be in the same order during training and prediction.\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (LightGBM models) ---\\n    # Train a separate LightGBM model for each quantile. This is a common and effective\\n    # strategy for quantile regression with tree-based models, as each model optimizes\\n    # directly for its specific quantile.\\n    models = {q: [] for q in QUANTILES} \\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members):\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set the quantile for the current model\\n            lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Vary seed for ensemble members\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            \\n            # Check if X_train_model and y_train_model are not empty before fitting\\n            if not X_train_model.empty and not y_train_model.empty:\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q].append(lgbm_model)\\n            else:\\n                print(f\\"WARNING: LightGBM skipped fitting for quantile {q} due to empty training data.\\")\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # For multi-step ahead forecasting (horizon > 0), future lagged/rolling features\\n    # are not yet observed. We must use the model's own median predictions to\\n    # recursively generate these features for subsequent forecast steps. This\\n    # \\"forecast-as-input\\" strategy is essential for autoregressive models.\\n    \\n    # Initialize history for each location using full training data's transformed target values.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    # Sorting ensures that predictions are made in chronological order for each location,\\n    # which is vital for correctly updating the historical features.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] \\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base (numerical and categorical) features.\\n        current_features_dict = {col: row.get(col, 0) for col in BASE_NUMERICAL_FEATURES + CATEGORICAL_FEATURES_LIST}\\n\\n        # Dynamically generate lagged features for the current prediction step.\\n        # This uses the most recent actual or forecasted history available for the location.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                # If not enough history for specific lag, use the most recent available value.\\n                # This is a heuristic to prevent NaNs in features for early predictions.\\n                lag_value = current_loc_hist[-1] \\n            else:\\n                lag_value = 0.0 # Default if no history at all for this location (e.g., new location)\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Dynamically generate lagged difference features\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Need at least two points for a difference\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2] # Fallback to 1-period diff\\n            else:\\n                diff_value = 0.0 # Default if insufficient history\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Dynamically generate rolling mean features\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history if not enough for window\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Dynamically generate rolling std features\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points to calculate std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # If insufficient data, std is 0\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert the feature dictionary to a DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        # This is critical for consistent feature input to the trained models, preventing errors.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        \\n        # Make predictions for all quantiles for this single row using LGBM models.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            lgbm_preds_for_q = []\\n            if models[q]: # Check if any models were trained for this quantile\\n                for lgbm_model_q in models[q]:\\n                    # Predicts on the single row, taking the first element of the result array.\\n                    lgbm_preds_for_q.append(lgbm_model_q.predict(X_test_row)[0])\\n                # Average predictions from ensemble members to get a more robust estimate for the quantile.\\n                row_predictions_transformed[q] = np.mean(lgbm_preds_for_q) \\n            else:\\n                # Fallback if no models were trained (e.g., empty training data)\\n                row_predictions_transformed[q] = 0.0 \\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) \\n\\n        # Inverse transform predictions from the transformed target scale back to 'admissions per million'.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million) # Ensure non-negative\\n\\n        # Convert from 'admissions per million' back to total admissions count.\\n        population_val = row[POPULATION_COL]\\n        # Handle cases where population is NaN or zero for predictions in the test set.\\n        safe_population_test = POPULATION_EPSILON\\n        if pd.notna(population_val) and population_val > 0:\\n            safe_population_test = population_val\\n        \\n        final_preds_total_admissions = inv_preds_admissions_per_million * safe_population_test / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction.\\n        # This median prediction (transformed back to feature scale) is crucial for\\n        # generating future lagged and rolling features in the iterative process.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.maximum(0.0, median_pred_admissions_per_million)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row and non-negativity.\\n    # This step is critical for producing valid quantile forecasts, as ordered quantiles\\n    # are a requirement for Weighted Interval Score evaluation. Quantiles must monotonically\\n    # increase (or stay same). Admissions counts cannot be negative.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to enforce monotonicity\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int) # Ensure non-negativity\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# The config_list allows for hyperparameter tuning via the evaluation harness.\\n# Each dictionary represents a different set of parameters for the fit_and_predict_fn.\\n# The harness will run your function with each config and report performance,\\n# identifying the best performing configuration.\\nconfig_list = [\\n    { # Config 1: A robust baseline with a good balance of features and model complexity.\\n      # This configuration generally performed well in previous trials and serves as a strong starting point.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root', # Often good for count data, helps normalize distribution\\n        'lag_weeks': [1, 4, 8, 16, 26, 52], # Captures short-term dynamics, medium-term trends, and annual seasonality\\n        'lag_diff_periods': [1, 2, 4, 8], # Captures changes over different short-to-medium terms, indicating acceleration/deceleration\\n        'rolling_windows': [8, 16, 26], # Smooths trends over various periods, providing more stable trend estimates\\n        'rolling_std_windows': [4, 8], # Captures short-term volatility, important for quantile spread\\n        'n_lgbm_ensemble_members': 1, # Using 1 for faster training; consider >1 for added robustness via small ensemble\\n    },\\n    { # Config 2: More aggressive feature engineering with slightly deeper LGBM trees.\\n      # Aims to capture more intricate patterns and potentially gain more from the data, but risks overfitting.\\n        'lgbm_params': {\\n            'n_estimators': 300, # More trees for potentially higher accuracy\\n            'learning_rate': 0.025, # Slightly slower learning for precision\\n            'num_leaves': 30, # More complex trees to capture finer details\\n            'max_depth': 6, # Deeper trees\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # More granular short-term lags\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8], # More detailed rate-of-change features\\n        'rolling_windows': [4, 8, 12, 16, 26], # Wider range of smoothing windows to capture diverse trends\\n        'rolling_std_windows': [4, 8, 12],\\n        'n_lgbm_ensemble_members': 1,\\n    },\\n    { # Config 3: A more conservative approach focusing on longer-term trends and simpler models.\\n      # Aims for better generalization by using more regularized trees and fewer very short-term features,\\n      # potentially robust to noise.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 20, # Simpler trees for better generalization\\n            'max_depth': 4, # Shallower trees\\n            'min_child_samples': 30, # More data per leaf, less prone to fitting noise\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.9,\\n            'subsample': 0.9,\\n            'reg_alpha': 0.2, # Stronger L1 regularization\\n            'reg_lambda': 0.2 # Stronger L2 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 8, 26, 52], # Focus on immediate and major seasonal lags\\n        'lag_diff_periods': [1, 8], \\n        'rolling_windows': [12, 26, 52], # Longer-term smoothing to capture overall trends\\n        'rolling_std_windows': [12, 26], # Longer-term volatility\\n        'n_lgbm_ensemble_members': 1,\\n    },\\n    { # Config 4: Explore a different target transformation (log1p) and introduce ensemble members.\\n      # Aims to test the robustness of the model to different data transformations and leverage ensembling\\n      # for potentially more stable and accurate quantile predictions.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'log1p', # Alternative transformation for count data\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'n_lgbm_ensemble_members': 3, # Use 3 ensemble members per quantile for increased robustness and variance reduction\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

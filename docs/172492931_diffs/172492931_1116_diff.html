<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1065",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version enhances robustness by:\\n    - Properly handling 'location' and 'horizon' as categorical features for both LGBM and XGBoost\\n      (using \`pd.CategoricalDtype\` and \`enable_categorical=True\` for XGBoost).\\n    - Continuing robust handling of missing data using time-series specific methods (ffill/bfill).\\n    - Improved numerical stability by clipping transformed target and predictions aggressively.\\n    - Incorporating additional date features.\\n    - Enhanced regularization parameters for LightGBM and XGBoost to prevent overfitting and improve stability,\\n      especially targeting the \`inf\` score issue with XGBoost.\\n    - Explicitly handling cases where the history for a location is very short during prediction.\\n    - Added \`try-except\` blocks around model predictions to gracefully handle potential errors during prediction,\\n      falling back to the mean transformed value if a model fails to predict.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon' \\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost (tuned for more robustness)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 180, \\n        'learning_rate': 0.03,\\n        'max_depth': 3,\\n        'min_child_weight': 20, \\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # Recommended for categorical features and performance\\n        'gamma': 1.0, \\n        'lambda': 2.0, \\n        'alpha': 0.2,   \\n        'enable_categorical': True \\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Allow higher maximum admissions per million but still with a cap\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 2000.0)\\n    \\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    \\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative, and result of power is non-negative before subtracting 1\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    # Adding a small buffer (+1.0) to MAX_ADMISSIONS_PER_MILLION before forward_transform ensures MAX_TRANSFORMED_VALUE\\n    # is slightly above the max possible transformed training target, preventing clipping issues.\\n    MAX_TRANSFORMED_VALUE = float(forward_transform(MAX_ADMISSIONS_PER_MILLION + 1.0)) \\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, cast to int\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n        df['weekday'] = df[DATE_COL].dt.weekday # Add weekday (Monday=0, Sunday=6)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    \\n    # HORIZON_COL is now explicitly categorized\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL, HORIZON_COL] \\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n    # Add 'horizon' to training data as 0 for historical observations\\n    train_features_df[HORIZON_COL] = 0\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Calculate diff based on shifted data to avoid data leakage\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Calculate fallback mean after target transformation and clipping\\n    # Use 1.0 transformed if y_train_model is empty (highly unlikely in competitive setting)\\n    # Ensure fallback value is within the clipped range\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0) \\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features using ffill/bfill\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Group by location before ffill/bfill to ensure imputation within each time series\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n            \\n            # Fill any remaining NaNs (e.g., if an entire location has NaNs for a feature, or only one point)\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For standard deviations, 0.0 is appropriate when data is sparse or constant.\\n                # For other features (lags, means, diffs), use the mean transformed value.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    # or where *after* imputation, some features are still NaN (e.g., location has no data at all).\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True) \\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- NEW: Handle categorical features for LightGBM and XGBoost consistently ---\\n    # Collect all possible categories for 'location' and 'horizon'\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_horizon_categories = pd.unique(pd.concat([X_train_model[HORIZON_COL], test_x_processed[HORIZON_COL]]))\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    X_train_lgbm[HORIZON_COL] = X_train_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories)) \\n\\n    # Prepare X_train for XGBoost (categorical Dtype with enable_categorical=True)\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    X_train_xgb[HORIZON_COL] = X_train_xgb[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories)) \\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM and XGBoost\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set quantile for LGBM\\n                lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i # Vary seed for ensemble members\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names) # Use proper categorical features\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Set quantile for XGBoost\\n                xgb_model_params_i['random_state'] = xgb_model_params_i['random_state'] + i # Vary seed for ensemble members\\n                \\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # XGBoost can auto-detect categorical features if enable_categorical=True and dtype is Categorical\\n                xgb_model.fit(X_train_xgb, y_train_model) \\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Prepare historical data for recursive feature generation.\\n    # history should contain transformed values.\\n    # Initialize with historical data from the training set.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        # Get the history for the current location, or an empty list if no history.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base features and horizon.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row.loc[LOCATION_COL]\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL] # Horizon is now explicitly in CATEGORICAL_FEATURES_LIST\\n\\n        # Generate time-series features for the current test row using available history\\n        # Note: Lag and rolling features require at least some history.\\n        # Fallback to mean_transformed_train_y_fallback or 0.0 for std if history is too short.\\n        \\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # Use the latest available if history is shorter than lag\\n                lag_value = current_loc_hist[-1]\\n            else:\\n                lag_value = mean_transformed_train_y_fallback # Fallback if no history for this location\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                # Calculate difference relative to the previous week (lag 1)\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # If not enough for full diff_period, try diff with 1 period\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 # No sufficient history for diff\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for std dev\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # fill_value=0.0 is used for any columns that might genuinely be missing (should be rare\\n        # given how current_features_dict is populated).\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row_lgbm[HORIZON_COL] = X_test_row_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories)) \\n\\n        # Prepare X_test_row for XGBoost (categorical Dtype)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row_xgb[HORIZON_COL] = X_test_row_xgb[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories)) \\n        \\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    try:\\n                        pred = lgbm_model_q.predict(X_test_row_lgbm)[0] # Use LGBM-specific X\\n                        if np.isfinite(pred): # Only add finite predictions to ensemble\\n                            ensemble_preds_for_q.append(pred)\\n                    except Exception as e:\\n                        # Log error but continue with other models/quantiles\\n                        # print(f\\"LGBM prediction failed for quantile {q}, location {current_loc}: {e}\\") # Debugging: uncomment for details\\n                        pass # Suppress print in final submission to avoid excessive output\\n            \\n            # Get predictions from XGBoost models for the current quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    try:\\n                        pred = xgb_model_q.predict(X_test_row_xgb)[0] # Use XGB-specific X\\n                        if np.isfinite(pred): # Only add finite predictions to ensemble\\n                            ensemble_preds_for_q.append(pred)\\n                    except Exception as e:\\n                        # Log error but continue with other models/quantiles\\n                        # print(f\\"XGBoost prediction failed for quantile {q}, location {current_loc}: {e}\\") # Debugging: uncomment for details\\n                        pass # Suppress print in final submission to avoid excessive output\\n            \\n            if ensemble_preds_for_q:\\n                # Average predictions from available ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # If all ensemble members for this quantile failed or are not present,\\n                # use a robust fallback (e.g., mean of transformed training target).\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        \\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle cases where population is 0 to avoid NaN or Inf results\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        # This is a crucial step for autoregressive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        \\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n        \\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n        \\n        # Append the new predicted value to the history for the current location\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across all predictions\\n    # This must be done on the final integer predictions\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform. This performed best previously.\\n      # Serves as a strong baseline.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 2000.0 \\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform.\\n      # Now with more robust XGBoost parameters and proper categorical handling.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 180, # Slightly reduced for more stability\\n            'learning_rate': 0.03,\\n            'max_depth': 3,\\n            'min_child_weight': 20, # Increased for more robustness\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0, # Increased regularization\\n            'lambda': 2.0, # Increased regularization\\n            'alpha': 0.2,   # Increased regularization\\n            'enable_categorical': True \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2000.0 \\n    },\\n    { # Config 3: Ensemble of LGBM and XGBoost, using 'log1p' transform for robustness.\\n      # With robust XGBoost parameters and proper categorical handling.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 160, # Slightly reduced\\n            'learning_rate': 0.03,\\n            'max_depth': 3,\\n            'min_child_weight': 25, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0, \\n            'lambda': 2.0, \\n            'alpha': 0.2, \\n            'enable_categorical': True \\n        },\\n        'target_transform': 'log1p', \\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 2000.0 \\n    },\\n    { # Config 4: Ensemble with 2 members for each model type (LGBM & XGBoost).\\n      # Uses 'fourth_root' transform, with slightly more conservative general regularization than default.\\n      # With robust XGBoost parameters and proper categorical handling.\\n        'lgbm_params': {\\n            'n_estimators': 150, \\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25, \\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.75,\\n            'subsample': 0.75,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 120,    # Slightly reduced\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 20, \\n            'subsample': 0.75,\\n            'colsample_bytree': 0.75,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0,          \\n            'lambda': 2.0,          \\n            'alpha': 0.2,\\n            'enable_categorical': True \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2, # Two members\\n        'n_xgb_ensemble_members': 2,  # Two members\\n        'max_admissions_per_million': 2000.0\\n    }\\n]",
  "new_index": "1116",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses LightGBM for quantile regression. It incorporates time-series features\\n    (including lagged target variables such as y_t-1, y_t-4, etc., as specified by\\n    \`lag_weeks\` in the config), a population-normalized and transformed target variable,\\n    and location information. The approach uses an iterative prediction strategy for the\\n    test set to correctly calculate lagged and rolling features for future steps,\\n    using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on robustness and simplicity by:\\n    - Exclusively using LightGBM as the model, removing the more complex ensemble of different model types.\\n    - Properly handling 'location' and 'horizon' as categorical features using \`pd.CategoricalDtype\`.\\n    - Robustly handling missing data introduced by lagging/rolling through \`ffill\`/\`bfill\` and intelligent fallbacks.\\n    - Improving numerical stability through aggressive clipping of transformed target and predictions to reasonable bounds.\\n    - Incorporating additional date-based features for seasonality and trend.\\n    - Applying regularization parameters to LightGBM to prevent overfitting and improve stability.\\n    - Explicitly handling cases where the history for a location is very short during prediction,\\n      using sensible default values.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for LightGBM and Feature Engineering ---\\n    # Default parameters for LightGBM (tuned for robustness and stability)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 25,  # Increased for more robustness\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.75, # Feature subsampling\\n        'subsample': 0.75,        # Data subsampling\\n        'reg_alpha': 0.15,        # L1 regularization\\n        'reg_lambda': 0.15        # L2 regularization\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}.copy() # Ensure copy for safety\\n\\n    # Feature engineering parameters\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    # Hardcode to 1 LightGBM model per quantile for simplicity, as requested (\\"simpler model\\").\\n    # This avoids issues with complex ensemble averaging that previously led to \`inf\` scores.\\n    n_lgbm_ensemble_members = 1\\n\\n    # Define a maximum plausible value for admissions per million to clip extreme values.\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 2000.0)\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Handle zero population by using 1.0 to avoid division by zero, preventing NaNs/Infs.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extremely large values\\n    # from skewing the transformation and training.\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration.\\n    # The \`+ 1.0\` in \`fourth_root\` and \`sqrt\` transforms, and \`log1p\` inherently handle zero values.\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            return np.expm1(x)\\n        def forward_transform(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # Fallback to no transformation if type is unknown/None.\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Calculate the maximum possible transformed value. This is used to clip transformed\\n    # target values in training and predictions, further enhancing stability.\\n    MAX_TRANSFORMED_VALUE = float(forward_transform(MAX_ADMISSIONS_PER_MILLION + 1.0))\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min() # Minimum date in the entire training dataset\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int) # ISO week number\\n        df['weekday'] = df[DATE_COL].dt.weekday # Monday=0, Sunday=6\\n\\n        # Add cyclical features for week of year to capture seasonality more effectively\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Add features representing time elapsed since the start of the dataset\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        return df\\n\\n    # Apply base feature engineering to both training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year', 'weekday',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n\\n    # Define categorical features that need special handling by LightGBM\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL, HORIZON_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n    # For historical observations in the training data, horizon is 0.\\n    train_features_df[HORIZON_COL] = 0\\n\\n    # Create lagged features of the transformed target variable\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Create lagged difference features\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # Shift by 1 first to avoid data leakage from current week's target\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Create rolling mean features (historical average of target)\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Create rolling standard deviation features (historical variability of target)\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Calculate a fallback value for imputation (mean of transformed training target).\\n    # This provides a reasonable default if history is insufficient or if a prediction fails.\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else forward_transform(1.0)\\n    mean_transformed_train_y_fallback = np.clip(mean_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # Handle missing data introduced by lagging/rolling in training features.\\n    # Impute within each location's time series using forward-fill then backward-fill.\\n    # Any remaining NaNs (e.g., if an entire series is NaN) are filled with a specific fallback.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            if X_train_model[col].isnull().any(): # Check again after ffill/bfill\\n                # For rolling standard deviations, 0.0 is appropriate when data is constant or insufficient.\\n                # For other features, use the robust mean_transformed_train_y_fallback.\\n                fill_value = 0.0 if 'rolling_std' in col else mean_transformed_train_y_fallback\\n                X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n    # Combine features and target to ensure consistent row dropping if any NaNs remain.\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows if the target itself is NaN (should not happen with a valid train_y)\\n    # or if, after imputation, any feature value is still NaN (highly unlikely but safeguards model training).\\n    train_combined.dropna(inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n\\n    # --- Prepare categorical features for LightGBM ---\\n    # Collect all possible categories for 'location' and 'horizon' from both train and test.\\n    # This ensures consistency during prediction, even if a category appears only in test.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_horizon_categories = pd.unique(pd.concat([X_train_model[HORIZON_COL], test_x_processed[HORIZON_COL]]))\\n\\n    # Convert categorical feature columns to Pandas CategoricalDtype for LightGBM.\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n    X_train_lgbm[HORIZON_COL] = X_train_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data. This is crucial for consistent prediction input.\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n\\n    # --- 4. Model Training (LightGBM models for each quantile) ---\\n    models = {q: [] for q in QUANTILES} # Each quantile will have a list of LGBM models\\n\\n    for q in QUANTILES:\\n        for i in range(n_lgbm_ensemble_members): # Loop for (n_lgbm_ensemble_members, currently 1)\\n            lgbm_model_params_i = lgbm_params.copy()\\n            lgbm_model_params_i['alpha'] = q # Set the quantile for this specific model\\n            # Vary random state for ensemble members to introduce diversity\\n            lgbm_model_params_i['random_state'] = lgbm_model_params_i['random_state'] + i\\n\\n            lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n            lgbm_model.fit(X_train_lgbm, y_train_model,\\n                           categorical_feature=categorical_feature_names)\\n            models[q].append(lgbm_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores the transformed target values for each location\\n    # from the training set, which will be extended with predictions.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x chronologically within each location for accurate iterative feature generation.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Iterate through each row in the test set to generate features and make predictions.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index']\\n\\n        # Retrieve the current location's historical transformed data.\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current test row using its base features.\\n        current_features_dict = {col: row.loc[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = current_loc\\n        current_features_dict[HORIZON_COL] = row.loc[HORIZON_COL]\\n\\n        # Generate time-series features for the current row based on \`current_loc_hist\`.\\n        # Fallback values are used if history is too short for a full calculation.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist: # If history exists but is shorter than lag, use the last known value.\\n                lag_value = current_loc_hist[-1]\\n            else: # If no history exists for this location, use the global training data mean.\\n                lag_value = mean_transformed_train_y_fallback\\n            current_features_dict[lag_col_name] = lag_value\\n\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Use a simpler diff if not enough data for full diff_period.\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else:\\n                diff_value = 0.0 # No sufficient history for a meaningful difference.\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist: # Use all available history for mean if shorter than window.\\n                rolling_mean_val = np.mean(current_loc_hist)\\n            else:\\n                rolling_mean_val = mean_transformed_train_y_fallback\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points to calculate std dev.\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points.\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Create a DataFrame from the current row's features.\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n        # Reindex to match the training feature columns exactly, filling any missing with 0.0.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Convert categorical columns to CategoricalDtype for LightGBM prediction.\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row_lgbm[HORIZON_COL] = X_test_row_lgbm[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            lgbm_preds_for_q = []\\n\\n            if q in models: # Ensure a model exists for this quantile.\\n                for lgbm_model_q in models[q]:\\n                    try:\\n                        # Predict using the prepared test row.\\n                        pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                        if np.isfinite(pred): # Only accept finite predictions.\\n                            lgbm_preds_for_q.append(pred)\\n                    except Exception as e:\\n                        # Gracefully handle prediction errors, fall back if necessary.\\n                        pass # Suppress detailed print for submission to avoid excessive output.\\n\\n            if lgbm_preds_for_q:\\n                # Average predictions from available LGBM ensemble members for this quantile (currently just one).\\n                row_predictions_transformed[q] = np.mean(lgbm_preds_for_q)\\n            else:\\n                # If all models for this quantile failed, use the robust fallback value.\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to stay within defined bounds to ensure numerical stability.\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Inverse transform predictions back to admissions per million.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clipping after inverse transformation.\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = row.loc[POPULATION_COL]\\n        # Convert admissions per million to total admissions by multiplying by population.\\n        # Handle zero population case to avoid division by zero errors or NaN results.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round predictions to the nearest integer and ensure they are non-negative.\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        # Store the predictions in the results DataFrame.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction for autoregressive forecasting.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        # Ensure the median prediction used for history update is also clipped.\\n        median_pred_transformed_raw = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across all predictions.\\n    # This is critical for Weighted Interval Score evaluation.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sort each row to ensure quantiles are non-decreasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Final check to ensure all predictions are non-negative integers.\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\n# The config_list now focuses on LightGBM only, exploring different feature sets and\\n# transformation types, with parameters tuned for robustness.\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with 'fourth_root' transform. Adjusted for robustness.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 25, # Increased from 20 for more robustness\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'max_admissions_per_million': 2000.0\\n    },\\n    { # Config 2: LGBM-only with 'log1p' transform and slightly different feature engineering.\\n      # Aims to test the 'log1p' transformation's robustness for comparison.\\n        'lgbm_params': {\\n            'n_estimators': 200, # Slightly fewer estimators\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 4, # Slightly shallower trees for generalization\\n            'min_child_samples': 30, # Further increased for more robustness\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.2, # More regularization\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'log1p',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # Broader range of lags\\n        'lag_diff_periods': [1, 3, 6], # Different diff periods\\n        'rolling_windows': [4, 12, 20], # Different rolling windows\\n        'rolling_std_windows': [6, 12],\\n        'max_admissions_per_million': 2500.0 # Slightly higher cap for target\\n    },\\n    { # Config 3: LGBM-only with 'fourth_root' transform, more aggressive regularization,\\n      # and slightly fewer estimators/leaves for a potentially faster, simpler model.\\n        'lgbm_params': {\\n            'n_estimators': 180, # Fewer estimators\\n            'learning_rate': 0.04, # Slightly higher learning rate\\n            'num_leaves': 20,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.2, # Increased regularization\\n            'reg_lambda': 0.2\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 26, 52],\\n        'lag_diff_periods': [1, 2, 4],\\n        'rolling_windows': [8, 16],\\n        'rolling_std_windows': [8],\\n        'max_admissions_per_million': 1800.0 # Slightly lower cap\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "645",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters (can be overridden by config)\\n    # Adjusted 'min_child_weight' and 'gamma' for more stability based on previous trial's 'inf' scores.\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 5, # Increased for stability (was 1)\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # Use faster histogram-based tree method\\n        'enable_categorical': True, # Enable native categorical feature support\\n        'gamma': 0.1, # Increased for stability (was 0.0)\\n        'lambda': 1.0,\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # New ensemble control parameters\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n    ensemble_aggregation_method = config.get('ensemble_aggregation_method', 'mean') # Can be 'mean' or 'median'\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Ensure population is not zero to prevent division by zero, set to a small positive value if it is.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x)) # Ensure non-negative before log1p\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(x, 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        # Added squared term for non-linear trend capture\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    # closed='left' ensures current value is not included in the window.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1 (std is 0 for single value)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # For any initial NaNs (where no prior values exist to ffill), fill with 0.0 (or a sensible default).\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where essential features/target are NaN *after* filling.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM and XGBoost ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM (XGBoost handles pandas CategoricalDtype natively with enable_categorical=True)\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES} # Store models for each quantile and model type\\n\\n    for q_idx, q in enumerate(QUANTILES): # Use q_idx for a unique random_state offset for each quantile\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set the quantile for this specific model\\n                # Vary seed for ensemble members to promote diversity\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + q_idx * n_lgbm_ensemble_members + i \\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names) # Pass categorical feature names\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Corrected: Use 'alpha' for quantile regression in XGBoost\\n                # Vary seed for ensemble members to promote diversity\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + q_idx * n_xgb_ensemble_members + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_model, y_train_model) # XGBoost sklearn API handles CategoricalDtype if enable_categorical=True\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = forward_transform(0.0) # Default for no history (transformed value of 0 admissions)\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Can at least compute a diff between last two available\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # Not enough history for any diff calculation\\n                diff_value = 0.0 # Default if no history or only one point\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Use what's available\\n            else:\\n                rolling_mean_val = forward_transform(0.0) # Default for no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling standard deviation features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for meaningful std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default if not enough history\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    ensemble_preds_for_q.append(lgbm_model_q.predict(X_test_row)[0])\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    ensemble_preds_for_q.append(xgb_model_q.predict(X_test_row)[0])\\n            \\n            if ensemble_preds_for_q:\\n                if ensemble_aggregation_method == 'median':\\n                    row_predictions_transformed[q] = np.median(ensemble_preds_for_q)\\n                else: # Default to mean\\n                    row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback in case no models were trained for a specific quantile/model type.\\n                # Use median from training data transformed target if no ensemble predictions possible.\\n                row_predictions_transformed[q] = y_train_model.median() if not y_train_model.empty else forward_transform(0.0)\\n\\n        # Extract predictions into an array and ensure monotonicity (sorted) before inverse transform.\\n        transformed_preds_array_raw = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        transformed_preds_array_sorted = np.sort(transformed_preds_array_raw)\\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array_sorted)\\n\\n        # Ensure admissions per million predictions are non-negative after inverse transformation.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle zero population explicitly: if population is zero, admissions must be zero.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million) # ensure non-negative\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Final monotonicity enforcement and non-negativity check after all predictions are made\\n    # (Important because rounding can sometimes slightly break monotonicity).\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: LGBM-only (previous best)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # Empty params as XGBoost is not used\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], # Only LGBM\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, # Explicitly no XGB members\\n        'ensemble_aggregation_method': 'mean', # For single model type, mean is acceptable\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, with median aggregation and conservative XGB parameters\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 5,\\n            'min_child_weight': 5, # Increased for stability\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.1, # Increased for stability\\n            'lambda': 1.0,\\n            'alpha': 0.1 # This will be overwritten by quantile 'q'\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], # Ensemble both\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'ensemble_aggregation_method': 'median', # Changed to median for robustness\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost parameters, using median agg\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 6,\\n            'min_child_weight': 5, # Increased for stability\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.1, # Increased for stability\\n            'lambda': 0.15,\\n            'alpha': 0.15 # This will be overwritten by quantile 'q'\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'ensemble_aggregation_method': 'median', # Changed to median for robustness\\n    }\\n]",
  "new_index": "651",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters (can be overridden by config)\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'eval_metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 5,\\n        'min_child_weight': 5, # Minimum sum of instance weight (hessian) needed in a child. Higher for stability.\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # Use faster histogram-based tree method\\n        'enable_categorical': True, # Enable native categorical feature support\\n        'gamma': 0.1, # Minimum loss reduction required to make a further partition. Higher for stability.\\n        'lambda': 1.0, # L2 regularization\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n    \\n    # New ensemble control parameters\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n    ensemble_aggregation_method = config.get('ensemble_aggregation_method', 'mean')\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # Ensure population is not zero to prevent division by zero, set to a small positive value if it is.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative\\n\\n    # Define transform and inverse transform functions based on config\\n    # Using +1.0 and -1.0 for power transforms to handle zeros robustly\\n    if target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): return np.expm1(x)\\n        def forward_transform(x): return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            # Ensure x is non-negative before squaring to avoid NaN with complex numbers from sqrt.\\n            # Then ensure final output is non-negative.\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    elif target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure x is non-negative before powering to avoid NaN with complex numbers from fractional power.\\n            # Then ensure final output is non-negative.\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        # Use .isocalendar().week for ISO week number, handling potential differences around year end.\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        # Added squared term for non-linear trend capture\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 \\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    # closed='left' ensures current value is not included in the window.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1 (std is 0 for single value)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # For any initial NaNs (where no prior values exist to ffill), fill with the transformed value of 0.\\n            X_train_model[col] = X_train_model[col].fillna(forward_transform(0.0))\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where essential features/target are NaN *after* filling.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM and XGBoost ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM (XGBoost handles pandas CategoricalDtype natively with enable_categorical=True)\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES} # Store models for each quantile and model type\\n\\n    for q_idx, q in enumerate(QUANTILES): # Use q_idx for a unique random_state offset for each quantile\\n        # Train LightGBM models\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set the quantile for this specific model\\n                # Vary seed for ensemble members to promote diversity\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + q_idx * n_lgbm_ensemble_members + i \\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names) # Pass categorical feature names\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Use 'alpha' for quantile regression in XGBoost\\n                # Vary seed for ensemble members to promote diversity\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + q_idx * n_xgb_ensemble_members + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_model, y_train_model) # XGBoost sklearn API handles CategoricalDtype if enable_categorical=True\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Fallback prediction value for problematic cases (e.g., NaN/Inf from models or no history)\\n    # Use the median of the transformed training target if available, otherwise 0 (transformed).\\n    fallback_pred_value_transformed = y_train_model.median() if not y_train_model.empty else forward_transform(0.0)\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        current_features_dict = {col: row[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = fallback_pred_value_transformed # Use robust fallback for no history\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Can at least compute a diff between last two available\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # Not enough history for any diff calculation\\n                diff_value = 0.0 # Default if no history or only one point (diff would be 0)\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Use what's available\\n            else:\\n                rolling_mean_val = fallback_pred_value_transformed # Use robust fallback for no history\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling standard deviation features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for meaningful std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default if not enough history or only one point (std is 0)\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    # Robustly handle potential NaN/inf from LGBM predictions\\n                    if np.isnan(pred) or np.isinf(pred):\\n                        ensemble_preds_for_q.append(fallback_pred_value_transformed)\\n                    else:\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    # Robustly handle potential NaN/inf from XGBoost predictions\\n                    if np.isnan(pred) or np.isinf(pred):\\n                        ensemble_preds_for_q.append(fallback_pred_value_transformed)\\n                    else:\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                if ensemble_aggregation_method == 'median':\\n                    row_predictions_transformed[q] = np.median(ensemble_preds_for_q)\\n                else: # Default to mean\\n                    row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback in case no models were trained for a specific quantile/model type.\\n                row_predictions_transformed[q] = fallback_pred_value_transformed\\n\\n        # Extract predictions into an array and ensure monotonicity (sorted) before inverse transform.\\n        transformed_preds_array_raw = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n        # Replace any remaining NaNs or Infs with the fallback value before sorting.\\n        transformed_preds_array_raw = np.nan_to_num(transformed_preds_array_raw, \\n                                                    nan=fallback_pred_value_transformed, \\n                                                    posinf=np.finfo(np.float32).max, \\n                                                    neginf=np.finfo(np.float32).min)\\n        transformed_preds_array_sorted = np.sort(transformed_preds_array_raw)\\n\\n        # Inverse transform predictions from transformed target scale.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array_sorted)\\n\\n        # Ensure admissions per million predictions are non-negative after inverse transformation.\\n        inv_preds_admissions_per_million = np.maximum(0.0, inv_preds_admissions_per_million)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row[POPULATION_COL]\\n        # Handle zero population explicitly: if population is zero, admissions must be zero.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts.\\n        final_preds_total_admissions = np.round(final_preds_total_admissions).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction (transformed back to feature scale).\\n        # Use the actual predicted median for updating history for consistency.\\n        median_pred_transformed_for_history = row_predictions_transformed[0.5]\\n        # Ensure the value added to history is numeric and finite\\n        if np.isnan(median_pred_transformed_for_history) or np.isinf(median_pred_transformed_for_history):\\n            median_pred_transformed_for_history = fallback_pred_value_transformed\\n        \\n        # Ensure it's non-negative before inverse transform and then forward transform\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_for_history)\\n        median_pred_admissions_per_million = max(0.0, median_pred_admissions_per_million)\\n        \\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Final monotonicity enforcement and non-negativity check after all predictions are made\\n    # (Important because rounding can sometimes slightly break monotonicity).\\n    predictions_array = predictions_df.values.astype(float)\\n    \\n    # Apply np.nan_to_num before sorting in case any values became NaN during final processing\\n    predictions_array = np.nan_to_num(predictions_array, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\\n\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: LGBM-only (previous best performer, retained as robust baseline)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # Explicitly no XGBoost parameters for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], # Only LGBM\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, # Explicitly no XGB members\\n        'ensemble_aggregation_method': 'mean',\\n    },\\n    { # Config 2: LGBM + Conservative XGBoost (with robust NaN/Inf handling)\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Slightly fewer estimators\\n            'learning_rate': 0.04, # Slightly higher learning rate to compensate\\n            'max_depth': 4, # Slightly shallower trees\\n            'min_child_weight': 10, # Increased for robustness\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.2, # Increased for robustness\\n            'lambda': 1.5, # Increased L2 regularization\\n            'alpha': 0.1 # This will be overwritten by quantile 'q'\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'ensemble_aggregation_method': 'median', # Median is more robust to outliers\\n    },\\n    { # Config 3: LGBM + Moderately Tuned XGBoost (with robust NaN/Inf handling)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200, # Increased from 150 (Config 2)\\n            'learning_rate': 0.03, # Slightly lower than Config 2\\n            'max_depth': 5, # Slightly deeper than Config 2\\n            'min_child_weight': 8, # Less conservative than Config 2\\n            'subsample': 0.8, # Increased\\n            'colsample_bytree': 0.8, # Increased\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.15, # Less conservative than Config 2\\n            'lambda': 1.0, # Decreased L2 regularization\\n            'alpha': 0.15 # This will be overwritten by quantile 'q'\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52],\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8],\\n        'rolling_windows': [4, 8, 12, 16, 26],\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'ensemble_aggregation_method': 'median',\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

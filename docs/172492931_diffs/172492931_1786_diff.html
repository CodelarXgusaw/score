<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "1747",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on enhancing numerical stability and robustness, particularly for XGBoost, by:\\n    - Adjusting XGBoost parameters (min_child_weight, regularization) to balance robustness and predictive power.\\n    - Setting a \`MAX_ADMISSIONS_PER_MILLION\` cap to constrain target values effectively.\\n    - Maintaining robust handling of missing time-series data using time-series specific methods (ffill/bfill) and fallbacks.\\n    - Ensuring monotonic increase and minimum spread between predicted quantiles as a final post-processing step.\\n    - Ensemble aggregation now uses \`np.median\` instead of \`np.mean\` for robustness against outlier predictions from individual models.\\n    - Added \`np.isfinite\` check before adding predictions to the history, preventing propagation of \`NaN\` or \`Inf\` values into future time-series features.\\n    - Improved: Robust population handling for \`test_x\` to ensure it's float and non-NaN/zero.\\n    - Improved: Clipped individual model predictions *before* ensembling to prevent extreme values from influencing the ensemble median.\\n    - Improved: More conservative default XGBoost parameters.\\n    - **New in this trial:** Changed history fallback for missing data from mean to median of transformed target.\\n    - **New in this trial:** Further increased \`min_child_weight\` and regularization for XGBoost in ensemble configs to improve stability, aiming to prevent \`inf\` scores.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability and performance\\n    # **SIGNIFICANTLY more conservative parameters for XGBoost** to avoid \`inf\` scores.\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 150, # Reduced estimators for safety and speed\\n        'learning_rate': 0.01, # Reduced learning rate for robustness\\n        'max_depth': 3, # Reduced depth\\n        'min_child_weight': 1000, # Significantly increased for robustness\\n        'subsample': 0.6, # Conservative subsampling\\n        'colsample_bytree': 0.6, # Conservative colsampling\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 1.0, # Increased gamma for more regularization\\n        'reg_lambda': 2.0, # Increased L2 regularization\\n        'reg_alpha': 1.0 # Increased L1 regularization\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    xgb_params.pop('eval_metric', None) # Safely remove if present, not used for quantile objective\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap to prevent extremely large or unstable target values.\\n    # Adjusted this default to be more conservative.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1)) # Must be at least 1 for integer counts\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Ensure population column is float and handle NaNs or zeros for train data\\n    # Use median for robustness against outliers if NaNs/extremes are present (unlikely for scaffold population)\\n    median_population_train = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train) or median_population_train == 0:\\n        median_population_train = 1.0 # Absolute fallback for median if it's NaN/Inf/0\\n\\n    df_train_full[POPULATION_COL] = df_train_full[POPULATION_COL].fillna(median_population_train).astype(float)\\n    # Replace any remaining zeros with 1.0 to avoid division by zero\\n    safe_population_train = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    \\n    # Calculate admissions per million\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population_train * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to power is non-negative\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            # Ensure input to expm1 is non-negative and output is non-negative\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            # Ensure input to log1p is non-negative\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            # Ensure input to power is non-negative, and output is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            # Ensure input to sqrt is non-negative\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min() # Global min date to make 'weeks_since_start' consistent\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add horizon feature directly, as it's specific to test_x but can be used in train_x (as 0)\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data if not explicitly provided\\n        \\n        # Ensure population is float for all base features and handle potential NaNs (from test scaffold)\\n        # using the median from the *full* training data for consistency.\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_population_train).astype(float)\\n        # Replace any remaining zeros with 1.0 to avoid division by zero\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, 1.0, df[POPULATION_COL])\\n        \\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Ensure HORIZON_COL is present in the base features list for model training\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Pre-calculate fallback for missing history. This will be median of the *transformed* target.\\n    # It should be based on the full training data AFTER transformation.\\n    # Handle cases where transformed_target_col might be empty or all NaNs\\n    # CHANGED: Use median for fallback for robustness\\n    median_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(median_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        # Fallback to a small positive value if median is not finite or data is empty\\n        median_transformed_train_y_fallback = forward_transform(1.0)\\n    # Clip the fallback value to be within the valid transformed range\\n    median_transformed_train_y_fallback = np.clip(median_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        # lag_1 is effectively target_t-1 - target_t-(1+diff_period)\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data introduced by lagging/rolling in training features\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Use ffill/bfill within groups\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if a location has no history for lags or only NaNs in its history)\\n            # Diffs and stds should fallback to 0.0 if not enough data\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else median_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n\\n    # Handle categorical features for LightGBM and XGBoost.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    # Ensure all_location_categories is a list for CategoricalDtype \`categories\` argument\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Prepare X_train for XGBoost (integer encoding)\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories_list)}\\n    # Assign a unique integer for locations not seen in training data for XGBoost\\n    unknown_location_int = len(all_location_categories_list)\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n                # XGBoost's 'enable_categorical' is for native categorical support,\\n                # but we're doing manual integer encoding for broad compatibility.\\n                # Remove if present, as it won't be used with integer encoding.\\n                xgb_model_params_i.pop('enable_categorical', None)\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with training data's transformed target values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row[LOCATION_COL]\\n\\n        # Ensure current population from test_x is float and handle potential NaNs or zeros.\\n        # Fallback to training median if problem, then to a safe default (1.0).\\n        current_population_val = float(row[POPULATION_COL])\\n        if not np.isfinite(current_population_val) or current_population_val == 0:\\n            current_features_dict[POPULATION_COL] = median_population_train # Use pre-calculated robust median\\n        else:\\n            current_features_dict[POPULATION_COL] = current_population_val\\n        \\n        # Double check population is not zero or NaN after all checks\\n        if not np.isfinite(current_features_dict[POPULATION_COL]) or current_features_dict[POPULATION_COL] == 0:\\n            current_features_dict[POPULATION_COL] = 1.0 # Absolute safety fallback\\n\\n\\n        # Generate time-series features for the current test row using available history\\n        if not current_loc_hist:\\n            # If no history, use fallbacks for all time-series features\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = median_transformed_train_y_fallback # Use median fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0 # Diffs fallback to zero\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = median_transformed_train_y_fallback # Use median fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0 # Stds fallback to zero\\n        else:\\n            # Generate features based on available history\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n                else:\\n                    lag_value = median_transformed_train_y_fallback # Fallback if history is too short for lag\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                # Need at least diff_period + 1 points for a diff of that period (t and t-diff_period)\\n                if len(current_loc_hist) >= diff_period + 1:\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-(diff_period + 1)]\\n                elif len(current_loc_hist) >= 2: # Fallback to 1-period diff if longer diff not possible\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n                else:\\n                    diff_value = 0.0 # No sufficient history for diff\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                # Use min(window, len(current_loc_hist)) to get actual window size if history is shorter\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_mean_val = median_transformed_train_y_fallback\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1: # Need at least 2 points for std dev\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0 # Std dev is 0 for 0 or 1 data points\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        # Fill any new columns (e.g., if a feature wasn't in train_x_model_cols) with 0.0\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        # Prepare X_test_row for XGBoost (integer encoding)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LightGBM models for the current quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    try:\\n                        pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                        # Clip individual predictions to prevent extreme values before ensembling\\n                        if np.isfinite(pred):\\n                            pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                            ensemble_preds_for_q.append(pred_clipped)\\n                    except Exception as e:\\n                        # print(f\\"LGBM prediction failed for q={q}, loc={current_loc}, err: {e}\\") # For debugging\\n                        pass # Continue if a model prediction fails\\n\\n            # Get predictions from XGBoost models for the current quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    try:\\n                        pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                        # Clip individual predictions to prevent extreme values before ensembling\\n                        if np.isfinite(pred):\\n                            pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                            ensemble_preds_for_q.append(pred_clipped)\\n                    except Exception as e:\\n                        # print(f\\"XGBoost prediction failed for q={q}, loc={current_loc}, err: {e}\\") # For debugging\\n                        pass # Continue if a model prediction fails\\n\\n            if ensemble_preds_for_q:\\n                # Use median for robustness against outliers from individual models\\n                row_predictions_transformed[q] = np.median(ensemble_preds_for_q)\\n            else:\\n                # Fallback if all ensemble members for this quantile failed or are not present.\\n                # Use a safe default: median of transformed training target\\n                row_predictions_transformed[q] = median_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        # Final clip of admissions per million to ensure values are within defined limits\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL] # Use the cleaned population value\\n        \\n        # Defensive check against zero population (should be handled by earlier fallbacks)\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        median_pred_transformed_raw = row_predictions_transformed[0.5] # Get the 0.5 quantile prediction\\n\\n        # Clip median transformed prediction before inverse and forward transform for history update\\n        median_pred_transformed_clipped = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_clipped)\\n\\n        # Clip median admissions per million before adding to history\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        # Ensure the value added to history is also clipped to MAX_TRANSFORMED_VALUE\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        # Append the new predicted value to the history for the current location,\\n        # ensuring it's finite and within bounds. This prevents bad values from propagating.\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            # Fallback for history update if the predicted value is not finite.\\n            location_history_data.setdefault(current_loc, []).append(median_transformed_train_y_fallback)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negativity\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # 2. Ensure monotonicity and minimal spread between adjacent quantiles\\n    for i in range(predictions_array.shape[0]):\\n        # Ensure non-decreasing order first by sorting\\n        predictions_array[i, :] = np.sort(predictions_array[i, :])\\n\\n        for j in range(1, len(QUANTILES)):\\n            # Ensure the current quantile is at least the previous one plus MIN_PRED_SPREAD\\n            # Use max to enforce the minimum spread\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # Final conversion to DataFrame and ensure non-negative integers\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform. This config previously performed best.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 2: Ensemble of LGBM (2 members) and XGBoost (2 members), fourth_root transform.\\n      # VERY CONSERVATIVE XGBoost regularization to avoid INF scores and promote stability.\\n        'lgbm_params': {\\n            'n_estimators': 200, # Reduced for faster execution with ensemble\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.01, # Reduced learning rate\\n            'max_depth': 3, # Reduced depth\\n            'min_child_weight': 1000, # Significantly increased for robustness\\n            'subsample': 0.6, # Conservative subsampling\\n            'colsample_bytree': 0.6, # Conservative colsampling\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0, # Increased gamma for more regularization\\n            'reg_lambda': 2.0, # Increased L2 regularization\\n            'reg_alpha': 1.0 # Increased L1 regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2, # Reduced ensemble size\\n        'n_xgb_ensemble_members': 2, # Reduced ensemble size\\n        'max_admissions_per_million': 5000.0, # Consistent with best performing config\\n        'min_pred_spread': 1\\n    },\\n    { # Config 3: Ensemble of LGBM (2 members) and XGBoost (2 members), log1p transform.\\n      # VERY CONSERVATIVE XGBoost regularization to avoid INF scores and promote stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.025,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150,\\n            'learning_rate': 0.01,\\n            'max_depth': 3,\\n            'min_child_weight': 1000,\\n            'subsample': 0.6,\\n            'colsample_bytree': 0.6,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.0,\\n            'reg_lambda': 2.0,\\n            'reg_alpha': 1.0\\n        },\\n        'target_transform': 'log1p', # Uses log1p transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0, # Consistent with best performing config\\n        'min_pred_spread': 1\\n    },\\n    { # Config 4: Ensemble of LGBM (1 member) and XGBoost (1 member), fourth_root transform.\\n      # Minimal ensemble, even more conservative XGBoost parameters to ensure stability in edge cases.\\n        'lgbm_params': {\\n            'n_estimators': 200, \\n            'learning_rate': 0.03,\\n            'num_leaves': 20,\\n            'max_depth': 4,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 120, # Further reduced estimators\\n            'learning_rate': 0.01,\\n            'max_depth': 3,\\n            'min_child_weight': 1200, # Even higher min_child_weight\\n            'subsample': 0.7, # Slightly higher subsample\\n            'colsample_bytree': 0.7, # Slightly higher colsample_bytree\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 1.2, # Higher gamma\\n            'reg_lambda': 2.5, # Higher reg_lambda\\n            'reg_alpha': 1.2 # Higher reg_alpha\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0, # Consistent\\n        'min_pred_spread': 1\\n    }\\n]",
  "new_index": "1786",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features (including lagged target variables such as y_t-1, y_t-4, etc.,\\n    as specified by \`lag_weeks\` in the config), a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version focuses on enhancing numerical stability and robustness, particularly for XGBoost, by:\\n    - **Crucially reducing XGBoost's \`min_child_weight\`** to a more typical value, allowing the model to learn more effectively without becoming overly constrained and unstable.\\n    - Adjusting other XGBoost regularization parameters (\`gamma\`, \`reg_lambda\`, \`reg_alpha\`) to be less aggressive, promoting better learning.\\n    - Setting a \`MAX_ADMISSIONS_PER_MILLION\` cap to constrain target values effectively.\\n    - Maintaining robust handling of missing time-series data using time-series specific methods (ffill/bfill) and fallbacks.\\n    - Ensuring monotonic increase and minimum spread between predicted quantiles as a final post-processing step.\\n    - Ensemble aggregation now uses \`np.median\` instead of \`np.mean\` for robustness against outlier predictions from individual models.\\n    - Added \`np.isfinite\` check before adding predictions to the history, preventing propagation of \`NaN\` or \`Inf\` values into future time-series features.\\n    - Improved: Robust population handling for \`test_x\` to ensure it's float and non-NaN/zero.\\n    - Improved: Clipped individual model predictions *before* ensembling to prevent extreme values from influencing the ensemble median.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features (DataFrame, same features as \`train_x\`, but for future time periods).\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default parameters for LightGBM\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1,\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    # Override defaults with config-specific LGBM parameters\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default parameters for XGBoost - adjusted for better stability and performance\\n    # min_child_weight significantly reduced from previous trial (1000/1200) to a more standard value\\n    default_xgb_params = {\\n        'objective': 'reg:quantile',\\n        'n_estimators': 150,\\n        'learning_rate': 0.01,\\n        'max_depth': 4, # Slightly increased max_depth from 3\\n        'min_child_weight': 50, # *** CRITICAL CHANGE: Reduced from very high values (1000+) ***\\n        'subsample': 0.7, # Increased from 0.6\\n        'colsample_bytree': 0.7, # Increased from 0.6\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist',\\n        'gamma': 0.5, # Reduced from 1.0/1.2\\n        'reg_lambda': 1.0, # Reduced from 2.0/2.5\\n        'reg_alpha': 0.5 # Reduced from 1.0/1.2\\n    }\\n    # Override defaults with config-specific XGBoost parameters\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n    xgb_params.pop('eval_metric', None) # Safely remove if present, not used for quantile objective\\n\\n    # Feature engineering parameters.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 4, 8, 16, 26, 52])\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8])\\n    ROLLING_WINDOWS = config.get('rolling_windows', [8, 16, 26])\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8])\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root')\\n\\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb'])\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Max admissions per million cap to prevent extremely large or unstable target values.\\n    MAX_ADMISSIONS_PER_MILLION = float(config.get('max_admissions_per_million', 6000.0))\\n\\n    # Minimal difference between adjacent quantile predictions to avoid zero-width intervals\\n    MIN_PRED_SPREAD = float(config.get('min_pred_spread', 1)) # Must be at least 1 for integer counts\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Ensure population column is float and handle NaNs or zeros for train data\\n    median_population_train = df_train_full[POPULATION_COL].median()\\n    if not np.isfinite(median_population_train) or median_population_train == 0:\\n        median_population_train = 1.0 # Absolute fallback for median if it's NaN/Inf/0\\n\\n    df_train_full[POPULATION_COL] = df_train_full[POPULATION_COL].fillna(median_population_train).astype(float)\\n    # Replace any remaining zeros with 1.0 to avoid division by zero\\n    df_train_full[POPULATION_COL] = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    \\n    # Calculate admissions per million\\n    admissions_per_million = df_train_full[TARGET_COL] / df_train_full[POPULATION_COL] * 1_000_000\\n\\n    # Clip admissions per million before transformation to prevent extreme values\\n    admissions_per_million = np.clip(admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n    # Define transform and inverse transform functions based on configuration\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x):\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x):\\n            return np.expm1(np.maximum(0.0, x))\\n        def forward_transform(x):\\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x):\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x):\\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # No transformation\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # Determine maximum valid transformed value based on the clipping of admissions_per_million\\n    MAX_TRANSFORMED_VALUE = forward_transform(MAX_ADMISSIONS_PER_MILLION)\\n    # Clip the transformed target in the training data to ensure it's within a reasonable range\\n    df_train_full[TRANSFORMED_TARGET_COL] = np.clip(df_train_full[TRANSFORMED_TARGET_COL], 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    # --- 2. Function to add common date-based features ---\\n    min_date_global = df_train_full[DATE_COL].min() # Global min date to make 'weeks_since_start' consistent\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year\\n        df['month'] = df[DATE_COL].dt.month\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int)\\n\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2\\n\\n        # Add horizon feature directly, as it's specific to test_x but can be used in train_x (as 0)\\n        if HORIZON_COL not in df.columns:\\n             df[HORIZON_COL] = 0 # Default for training data if not explicitly provided\\n        \\n        # Ensure population is float for all base features and handle potential NaNs (from test scaffold)\\n        df[POPULATION_COL] = df[POPULATION_COL].fillna(median_population_train).astype(float)\\n        # Replace any remaining zeros with 1.0 to avoid division by zero\\n        df[POPULATION_COL] = np.where(df[POPULATION_COL] == 0, 1.0, df[POPULATION_COL])\\n        \\n        return df\\n\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Ensure HORIZON_COL is present in the base features list for model training\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq', HORIZON_COL]\\n\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL]\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Pre-calculate fallback for missing history. This will be median of the *transformed* target.\\n    median_transformed_train_y_fallback = df_train_full[TRANSFORMED_TARGET_COL].median()\\n    if not np.isfinite(median_transformed_train_y_fallback) or df_train_full[TRANSFORMED_TARGET_COL].empty:\\n        median_transformed_train_y_fallback = forward_transform(1.0) # Fallback to a small positive value\\n    median_transformed_train_y_fallback = np.clip(median_transformed_train_y_fallback, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            )\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model = train_features_df[BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features].copy()\\n\\n    # Handle missing data introduced by lagging/rolling in training features using time-series specific methods\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            # Use ffill/bfill within groups\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill().bfill())\\n\\n            # Fill any remaining NaNs (e.g., if a location has no history for lags)\\n            fill_value = 0.0 if 'std' in col or 'diff' in col else median_transformed_train_y_fallback\\n            X_train_model[col] = X_train_model[col].fillna(fill_value)\\n\\n\\n    # Drop rows where the target itself is NaN (shouldn't happen with valid train_y from harness)\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    train_combined.dropna(subset=[TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # Handle cases where training data becomes empty after processing (e.g., very early folds)\\n    if X_train_model.empty or y_train_model.empty:\\n        print(\\"Warning: Training data is empty after preprocessing. Returning fallback predictions (all zeros).\\")\\n        predictions_df = pd.DataFrame(index=test_x.index, columns=[f'quantile_{q}' for q in QUANTILES])\\n        for q_col in [f'quantile_{q}' for q in QUANTILES]:\\n            predictions_df[q_col] = 0\\n        return predictions_df\\n\\n\\n    # Handle categorical features for LightGBM and XGBoost.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    all_location_categories_list = all_location_categories.tolist()\\n\\n    # Prepare X_train for LightGBM (categorical Dtype)\\n    X_train_lgbm = X_train_model.copy()\\n    X_train_lgbm[LOCATION_COL] = X_train_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n    # Prepare X_train for XGBoost (integer encoding)\\n    location_to_int = {loc: i for i, loc in enumerate(all_location_categories_list)}\\n    unknown_location_int = len(all_location_categories_list)\\n    X_train_xgb = X_train_model.copy()\\n    X_train_xgb[LOCATION_COL] = X_train_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES}\\n\\n    for q in QUANTILES:\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                lgbm_model.fit(X_train_lgbm, y_train_model,\\n                               categorical_feature=categorical_feature_names)\\n                models[q]['lgbm'].append(lgbm_model)\\n\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['quantile_alpha'] = q\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n                xgb_model_params_i.pop('enable_categorical', None) # Ensure this is not used with manual int encoding\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_xgb, y_train_model)\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # \`location_history_data\` stores transformed target values for each location to generate lags.\\n    # Initialize with training data's transformed target values\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    original_test_x_index = test_x.index\\n\\n    # Sort test_x to ensure chronological processing within each location for iterative predictions\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row[LOCATION_COL]\\n        original_idx = row['original_index']\\n\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        current_features_dict = {col: row[col] for col in BASE_FEATURES}\\n        current_features_dict[LOCATION_COL] = row[LOCATION_COL]\\n\\n        # Ensure current population from test_x is float and handle potential NaNs or zeros.\\n        current_population_val = float(row[POPULATION_COL])\\n        if not np.isfinite(current_population_val) or current_population_val == 0:\\n            current_features_dict[POPULATION_COL] = median_population_train # Use pre-calculated robust median\\n        else:\\n            current_features_dict[POPULATION_COL] = current_population_val\\n        \\n        # Double check population is not zero or NaN after all checks\\n        if not np.isfinite(current_features_dict[POPULATION_COL]) or current_features_dict[POPULATION_COL] == 0:\\n            current_features_dict[POPULATION_COL] = 1.0 # Absolute safety fallback\\n\\n\\n        # Generate time-series features for the current test row using available history\\n        if not current_loc_hist:\\n            for lag in LAG_WEEKS:\\n                current_features_dict[f'lag_{lag}_wk'] = median_transformed_train_y_fallback\\n            for diff_period in LAG_DIFF_PERIODS:\\n                current_features_dict[f'diff_lag_1_period_{diff_period}_wk'] = 0.0\\n            for window in ROLLING_WINDOWS:\\n                current_features_dict[f'rolling_mean_{window}_wk'] = median_transformed_train_y_fallback\\n            for window in ROLLING_STD_WINDOWS:\\n                current_features_dict[f'rolling_std_{window}_wk'] = 0.0\\n        else:\\n            for lag in LAG_WEEKS:\\n                lag_col_name = f'lag_{lag}_wk'\\n                if len(current_loc_hist) >= lag:\\n                    lag_value = current_loc_hist[len(current_loc_hist) - lag]\\n                else:\\n                    lag_value = median_transformed_train_y_fallback\\n                current_features_dict[lag_col_name] = lag_value\\n\\n            for diff_period in LAG_DIFF_PERIODS:\\n                diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n                if len(current_loc_hist) >= diff_period + 1:\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-(diff_period + 1)]\\n                elif len(current_loc_hist) >= 2:\\n                    diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n                else:\\n                    diff_value = 0.0\\n                current_features_dict[diff_col_name] = diff_value\\n\\n            for window in ROLLING_WINDOWS:\\n                rolling_col_name = f'rolling_mean_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 0:\\n                    rolling_mean_val = np.mean(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_mean_val = median_transformed_train_y_fallback\\n                current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n            for window in ROLLING_STD_WINDOWS:\\n                rolling_std_col_name = f'rolling_std_{window}_wk'\\n                actual_window_size = min(window, len(current_loc_hist))\\n                if actual_window_size > 1:\\n                    rolling_std_val = np.std(current_loc_hist[-actual_window_size:])\\n                else:\\n                    rolling_std_val = 0.0\\n                current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n\\n        # Create a DataFrame for the current row's features\\n        X_test_row_base = pd.DataFrame([current_features_dict])\\n\\n        # Reindex to ensure all columns from training set are present and in order.\\n        X_test_row_base = X_test_row_base.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Prepare X_test_row for LGBM (categorical Dtype)\\n        X_test_row_lgbm = X_test_row_base.copy()\\n        X_test_row_lgbm[LOCATION_COL] = X_test_row_lgbm[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories_list))\\n\\n        # Prepare X_test_row for XGBoost (integer encoding)\\n        X_test_row_xgb = X_test_row_base.copy()\\n        X_test_row_xgb[LOCATION_COL] = X_test_row_xgb[LOCATION_COL].map(location_to_int).fillna(unknown_location_int).astype(int)\\n\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    try:\\n                        pred = lgbm_model_q.predict(X_test_row_lgbm)[0]\\n                        if np.isfinite(pred):\\n                            pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                            ensemble_preds_for_q.append(pred_clipped)\\n                    except Exception:\\n                        pass # Continue if a model prediction fails\\n\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    try:\\n                        pred = xgb_model_q.predict(X_test_row_xgb)[0]\\n                        if np.isfinite(pred):\\n                            pred_clipped = np.clip(pred, 0.0, MAX_TRANSFORMED_VALUE)\\n                            ensemble_preds_for_q.append(pred_clipped)\\n                    except Exception:\\n                        pass # Continue if a model prediction fails\\n\\n            if ensemble_preds_for_q:\\n                row_predictions_transformed[q] = np.median(ensemble_preds_for_q)\\n            else:\\n                row_predictions_transformed[q] = median_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES])\\n\\n        # Clip transformed predictions to prevent extreme values before inverse transformation\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        population_val = current_features_dict[POPULATION_COL]\\n        \\n        if population_val == 0: # Defensive check\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer and ensure non-negative\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update history for the next iteration using the median prediction\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n\\n        median_pred_transformed_clipped = np.clip(median_pred_transformed_raw, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_clipped)\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        value_to_add_to_history = np.clip(value_to_add_to_history, 0.0, MAX_TRANSFORMED_VALUE)\\n\\n        if np.isfinite(value_to_add_to_history):\\n            location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n        else:\\n            location_history_data.setdefault(current_loc, []).append(median_transformed_train_y_fallback)\\n\\n\\n    # --- 6. Post-processing to ensure monotonicity and minimum spread ---\\n    predictions_array = predictions_df.values.astype(float)\\n\\n    # 1. Ensure non-negativity\\n    predictions_array = np.maximum(0, predictions_array)\\n\\n    # 2. Ensure monotonicity and minimal spread between adjacent quantiles\\n    for i in range(predictions_array.shape[0]):\\n        predictions_array[i, :] = np.sort(predictions_array[i, :]) # Ensure non-decreasing order\\n\\n        for j in range(1, len(QUANTILES)):\\n            predictions_array[i, j] = max(predictions_array[i, j], predictions_array[i, j-1] + MIN_PRED_SPREAD)\\n\\n    # Final conversion to DataFrame and ensure non-negative integers\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n    predictions_df = predictions_df.astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform.\\n        'lgbm_params': {\\n            'n_estimators': 250, 'learning_rate': 0.03, 'num_leaves': 26, 'max_depth': 5,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 2: Ensemble of LGBM (2 members) and XGBoost (2 members), fourth_root transform.\\n      # XGBoost parameters adjusted for stability.\\n        'lgbm_params': {\\n            'n_estimators': 200, 'learning_rate': 0.025, 'num_leaves': 25, 'max_depth': 5,\\n            'min_child_samples': 25, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': { # ** Adjusted parameters for stability **\\n            'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 4,\\n            'min_child_weight': 50, # Reduced\\n            'subsample': 0.7, 'colsample_bytree': 0.7, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.5, 'reg_lambda': 1.0, 'reg_alpha': 0.5 # Relaxed regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 3: Ensemble of LGBM (2 members) and XGBoost (2 members), log1p transform.\\n      # XGBoost parameters adjusted for stability.\\n        'lgbm_params': {\\n            'n_estimators': 200, 'learning_rate': 0.025, 'num_leaves': 25, 'max_depth': 5,\\n            'min_child_samples': 25, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': { # ** Adjusted parameters for stability **\\n            'n_estimators': 150, 'learning_rate': 0.01, 'max_depth': 4,\\n            'min_child_weight': 50, # Reduced\\n            'subsample': 0.7, 'colsample_bytree': 0.7, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.5, 'reg_lambda': 1.0, 'reg_alpha': 0.5 # Relaxed regularization\\n        },\\n        'target_transform': 'log1p', # Uses log1p transformation\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2,\\n        'n_xgb_ensemble_members': 2,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    },\\n    { # Config 4: Ensemble of LGBM (1 member) and XGBoost (1 member), fourth_root transform.\\n      # Minimal ensemble, with adjusted XGBoost parameters for stability.\\n        'lgbm_params': {\\n            'n_estimators': 200, 'learning_rate': 0.03, 'num_leaves': 20, 'max_depth': 4,\\n            'min_child_samples': 20, 'random_state': 42, 'n_jobs': -1, 'verbose': -1,\\n            'colsample_bytree': 0.8, 'subsample': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1\\n        },\\n        'xgb_params': { # ** Adjusted parameters for stability **\\n            'n_estimators': 120, 'learning_rate': 0.01, 'max_depth': 4,\\n            'min_child_weight': 50, # Reduced\\n            'subsample': 0.7, 'colsample_bytree': 0.7, 'random_state': 42, 'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'gamma': 0.5, 'reg_lambda': 1.0, 'reg_alpha': 0.5 # Relaxed regularization\\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 5000.0,\\n        'min_pred_spread': 1\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": "727",
  "old_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    The primary changes from the previous trial focus on numerical stability and\\n    robustness by consistently using the 'fourth_root' target transformation,\\n    which has shown to prevent 'inf' scores in previous runs, and adjusting\\n    the clipping values to be more appropriate.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters adjusted to be robust for stability\\n    default_xgb_params = {\\n        'objective': 'reg:quantile', # Use quantile regression objective\\n        'eval_metric': 'quantile', # Evaluation metric\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3, # Conservative depth to prevent overfitting\\n        'min_child_weight': 10, # Conservative min child weight\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # Use faster histogram-based tree method\\n        'enable_categorical': True, # Enable native categorical feature support\\n        'gamma': 0.5, # Controls minimum loss reduction required to make a further partition\\n        'lambda': 1.0, # L2 regularization\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    # Feature engineering parameters. These define which time-series features are generated.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52]) # Lagged target values (most important for time series)\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8]) # Rate of change features\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16]) # Smoothed trends\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8]) # Volatility measures\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root') # Default to fourth_root for stability\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb']) # Ensemble both LGBM and XGBoost\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Maximum allowed admissions per million after inverse transform. Prevents extreme outliers.\\n    # Set to a value that provides a reasonable upper bound based on observed data.\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 1000.0) # Adjusted from 250 to 1000\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # This normalization helps the model learn patterns independent of state population size.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative before transform\\n\\n    # Define transform and inverse transform functions based on config\\n    # The 'fourth_root' transformation proved more numerically stable and effective in previous trials.\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative and result is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input is non-negative before applying the transform\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p': # Retained for flexibility, but less stable for this problem\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt': # Another power transform option\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year # Captures long-term trends/phases of the pandemic\\n        df['month'] = df[DATE_COL].dt.month # Seasonal feature\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int) # Fine-grained seasonality\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly (e.g., flu season, holidays).\\n        # These are important for recurring patterns that are not strictly linear.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Captures non-linear trend over time\\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define base features used for all predictions\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Location is treated as a categorical variable\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # These are typically the most important features for time series forecasting, capturing autocorrelation.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    # Captures the rate of change in admissions, indicating rising or falling trends.\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    # Provides a smoothed trend of recent admissions, making the model robust to short-term noise.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    # Captures the variability or volatility of recent admissions, useful for uncertainty estimation.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1 (std is 0 for single value)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This helps the model learn how forecast accuracy or patterns might change with prediction horizon.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # This handles missing values introduced by lagging/rolling, especially at the start of series.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # For any initial NaNs (where no prior values exist to ffill), fill with 0.0.\\n            # This is a safe fallback for new locations or very early data points.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where essential features/target are NaN *after* filling, ensuring clean training data.\\n    # This removes early rows for each location where lags couldn't be computed.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM and XGBoost ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES} # Store models for each quantile and model type\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models for the given quantile (alpha parameter)\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set the quantile for this specific model\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Vary seed for ensemble members\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                \\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names) # Pass categorical feature names\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models for the given quantile (alpha parameter)\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Use 'alpha' for quantile regression in XGBoost\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                xgb_model.fit(X_train_model, y_train_model) # XGBoost sklearn API handles CategoricalDtype\\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This allows test predictions to use recent actuals from the training period.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    # Sorting ensures that we process data in chronological order per location.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Pre-calculate mean of transformed training target for robust fallback in case of no ensemble predictions\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    # This is crucial for generating features that depend on previously predicted values (recursive forecasting).\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        # Use .loc for explicit label-based indexing.\\n        current_features_dict = {col: row.loc[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        # These features mimic the real-world scenario where past data is available.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = 0.0 # Default if no history at all for this location\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Can at least compute a diff between last two available\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # Not enough history for any diff calculation\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Use what's available\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for meaningful std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default if not enough history\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LGBM models for this quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions for averaging\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            # Get predictions from XGBoost models for this quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions for averaging\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                # Average predictions from all finite ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback: if all ensemble members for this quantile produced non-finite predictions\\n                # or if no models of that type were configured.\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) # Ensure order\\n\\n        # Ensure transformed predictions are non-negative before inverse transformation.\\n        transformed_preds_array = np.maximum(0.0, transformed_preds_array)\\n\\n        # Inverse transform predictions from transformed target scale back to admissions per million.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n\\n        # Apply clipping to prevent excessively large predictions, using the configured maximum.\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle zero population explicitly: if population is zero, admissions must be zero.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts and ensure non-negative.\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction.\\n        # This recursive step is critical for multi-step forecasting; the median (0.5 quantile)\\n        # is used as the best single-point estimate to inform future feature generation.\\n        median_pred_transformed_raw = row_predictions_transformed[0.5]\\n        \\n        # Inverse transform the median prediction to admissions per million, clip, then re-transform\\n        # to ensure it's on the same scale as the feature values used for history.\\n        median_pred_admissions_per_million = inverse_transform(median_pred_transformed_raw)\\n        median_pred_admissions_per_million = np.clip(median_pred_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Re-transform this median value back to the *feature* scale (e.g., fourth_root scale)\\n        # for use in generating future lag/rolling features.\\n        value_to_add_to_history = forward_transform(median_pred_admissions_per_million)\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # This step corrects any non-monotonic predictions that might arise from independent quantile models.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    # Convert back to DataFrame with original columns and index.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform (replicates previous best result)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 10000.0 # Original working value\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform, adjusted clip.\\n      # This aims to combine the robustness of fourth_root with the diversity of an ensemble.\\n      # Parameters are relatively conservative to ensure stability.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root', # Crucial change from previous trial's 'inf' configs\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 1000.0 # More reasonable cap than 250\\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost, fourth_root, and expanded features.\\n      # Explores if more complex models and features improve scores with the robust transform.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 4,\\n            'min_child_weight': 15,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.8,\\n            'lambda': 0.15,\\n            'alpha': 0.15 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # More lags\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8], # More diff periods\\n        'rolling_windows': [4, 8, 12, 16, 26], # More rolling windows\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 1000.0 \\n    },\\n    { # Config 4: Ensemble with 2 members for each model type, slightly reduced estimators, fourth_root.\\n      # Aims for higher ensemble diversity to potentially improve calibration and robustness.\\n        'lgbm_params': {\\n            'n_estimators': 150, # Reduced estimators to balance with more ensemble members\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2, # Two LGBM members per quantile\\n        'n_xgb_ensemble_members': 2,   # Two XGB members per quantile\\n        'max_admissions_per_million': 1000.0\\n    }\\n]",
  "new_index": "849",
  "new_code": "# YOUR CODE\\nimport numpy as np\\nimport pandas as pd\\nfrom lightgbm import LGBMRegressor\\nimport xgboost as xgb\\nfrom typing import Any\\n\\ndef fit_and_predict_fn(\\n    train_x: pd.DataFrame,\\n    train_y: pd.Series,\\n    test_x: pd.DataFrame,\\n    config: dict[str, Any]\\n) -> pd.DataFrame:\\n    \\"\\"\\"Make probabilistic predictions for test_x by modeling train_x to train_y.\\n\\n    The model uses an ensemble of LightGBM and XGBoost models for quantile regression.\\n    It incorporates time-series features, a population-normalized and transformed\\n    target variable, and location information. The approach uses an iterative prediction\\n    strategy for the test set to correctly calculate lagged and rolling features for\\n    future steps, using median predictions to recursively inform future feature generation.\\n\\n    This version introduces a crucial clip on the transformed predictions *before*\\n    inverse transformation to prevent numerical overflow leading to 'inf' scores.\\n\\n    Args:\\n        train_x (pd.DataFrame): Training features.\\n        train_y (pd.Series): Training target values (Total COVID-19 Admissions).\\n        test_x (pd.DataFrame): Test features for future time periods.\\n        config (dict[str, Any]): Configuration parameters for the model.\\n\\n    Returns:\\n        pd.DataFrame: DataFrame with quantile predictions for each row in test_x.\\n                      Columns are named 'quantile_0.XX'.\\n    \\"\\"\\"\\n    QUANTILES = [\\n        0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\\n        0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99\\n    ]\\n    TARGET_COL = 'Total COVID-19 Admissions'\\n    DATE_COL = 'target_end_date'\\n    LOCATION_COL = 'location'\\n    POPULATION_COL = 'population'\\n    HORIZON_COL = 'horizon'\\n\\n    TRANSFORMED_TARGET_COL = 'transformed_admissions_per_million'\\n\\n    # --- Configuration for Models and Feature Engineering ---\\n    # Default LightGBM parameters (can be overridden by config)\\n    default_lgbm_params = {\\n        'objective': 'quantile',\\n        'metric': 'quantile',\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'num_leaves': 25,\\n        'max_depth': 5,\\n        'min_child_samples': 20,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'verbose': -1, # Suppress verbose output during training\\n        'colsample_bytree': 0.8,\\n        'subsample': 0.8,\\n        'reg_alpha': 0.1,\\n        'reg_lambda': 0.1\\n    }\\n    lgbm_params = {**default_lgbm_params, **config.get('lgbm_params', {})}\\n\\n    # Default XGBoost parameters adjusted to be robust for stability\\n    default_xgb_params = {\\n        'objective': 'reg:quantile', # Use quantile regression objective\\n        'eval_metric': 'quantile', # Evaluation metric\\n        'n_estimators': 200,\\n        'learning_rate': 0.03,\\n        'max_depth': 3, # Conservative depth to prevent overfitting\\n        'min_child_weight': 10, # Conservative min child weight\\n        'subsample': 0.8,\\n        'colsample_bytree': 0.8,\\n        'random_state': 42,\\n        'n_jobs': -1,\\n        'tree_method': 'hist', # Use faster histogram-based tree method\\n        'enable_categorical': True, # Enable native categorical feature support\\n        'gamma': 0.5, # Controls minimum loss reduction required to make a further partition\\n        'lambda': 1.0, # L2 regularization\\n        'alpha': 0.1 # This 'alpha' will be overwritten by the quantile 'q'\\n    }\\n    xgb_params = {**default_xgb_params, **config.get('xgb_params', {})}\\n\\n    # Feature engineering parameters. These define which time-series features are generated.\\n    LAG_WEEKS = config.get('lag_weeks', [1, 2, 3, 4, 8, 26, 52]) # Lagged target values (most important for time series)\\n    LAG_DIFF_PERIODS = config.get('lag_diff_periods', [1, 2, 4, 8]) # Rate of change features\\n    ROLLING_WINDOWS = config.get('rolling_windows', [4, 8, 16]) # Smoothed trends\\n    ROLLING_STD_WINDOWS = config.get('rolling_std_windows', [4, 8]) # Volatility measures\\n\\n    target_transform_type = config.get('target_transform', 'fourth_root') # Default to fourth_root for stability\\n    \\n    ensemble_model_types = config.get('ensemble_model_types', ['lgbm', 'xgb']) # Ensemble both LGBM and XGBoost\\n    n_lgbm_ensemble_members = config.get('n_lgbm_ensemble_members', 1)\\n    n_xgb_ensemble_members = config.get('n_xgb_ensemble_members', 1)\\n\\n    # Maximum allowed admissions per million after inverse transform. Prevents extreme outliers.\\n    MAX_ADMISSIONS_PER_MILLION = config.get('max_admissions_per_million', 10000.0) \\n\\n    # --- Derived clipping value for transformed predictions ---\\n    # This is crucial for numerical stability before inverse transformation.\\n    # Calculate a maximum value for the transformed target based on MAX_ADMISSIONS_PER_MILLION.\\n    # Adding a small buffer (10%) and then a hard cap to prevent highly improbable values.\\n    # For example, if MAX_ADMISSIONS_PER_MILLION=10000, then (10001)^0.25 = ~10.0. Buffer makes it ~11.0.\\n    # Capping at 50 ensures that even with a large population, total admissions don't exceed int64 max.\\n    MAX_TRANSFORMED_PREDICTION_CAP = np.power(MAX_ADMISSIONS_PER_MILLION + 1.0, 0.25) * 1.1 \\n    MAX_TRANSFORMED_PREDICTION_CAP = min(MAX_TRANSFORMED_PREDICTION_CAP, 50.0)\\n\\n\\n    # --- 1. Combine train_x and train_y, and prepare for transformations ---\\n    df_train_full = train_x.copy()\\n    df_train_full[TARGET_COL] = train_y\\n    df_train_full[DATE_COL] = pd.to_datetime(df_train_full[DATE_COL])\\n    df_train_full = df_train_full.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Calculate transformed target: Admissions per million people, then apply chosen transformation.\\n    # This normalization helps the model learn patterns independent of state population size.\\n    safe_population = np.where(df_train_full[POPULATION_COL] == 0, 1.0, df_train_full[POPULATION_COL])\\n    admissions_per_million = df_train_full[TARGET_COL] / safe_population * 1_000_000\\n    admissions_per_million = np.maximum(0.0, admissions_per_million) # Ensure non-negative before transform\\n\\n    # Define transform and inverse transform functions based on config\\n    # The 'fourth_root' transformation proved more numerically stable and effective in previous trials.\\n    if target_transform_type == 'fourth_root':\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.power(admissions_per_million + 1.0, 0.25)\\n        def inverse_transform(x): \\n            # Ensure input to power is non-negative and result is non-negative\\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 4) - 1.0)\\n        def forward_transform(x): \\n            # Ensure input is non-negative before applying the transform\\n            return np.power(np.maximum(0.0, x) + 1.0, 0.25)\\n    elif target_transform_type == 'log1p': # Retained for flexibility, but less stable for this problem\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.log1p(admissions_per_million)\\n        def inverse_transform(x): \\n            return np.expm1(x)\\n        def forward_transform(x): \\n            return np.log1p(np.maximum(0.0, x))\\n    elif target_transform_type == 'sqrt': # Another power transform option\\n        df_train_full[TRANSFORMED_TARGET_COL] = np.sqrt(admissions_per_million + 1.0)\\n        def inverse_transform(x): \\n            return np.maximum(0.0, np.power(np.maximum(0.0, x), 2) - 1.0)\\n        def forward_transform(x): \\n            return np.sqrt(np.maximum(0.0, x) + 1.0)\\n    else: # Fallback to raw (per million) if transform type is unknown/invalid\\n        df_train_full[TRANSFORMED_TARGET_COL] = admissions_per_million\\n        def inverse_transform(x): return x\\n        def forward_transform(x): return x\\n\\n    # --- 2. Function to add common date-based features ---\\n    # Determine the global minimum date from the training set. This anchors 'weeks_since_start'.\\n    min_date_global = df_train_full[DATE_COL].min()\\n\\n    def add_base_features(df_input: pd.DataFrame, min_date: pd.Timestamp) -> pd.DataFrame:\\n        df = df_input.copy()\\n        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\\n\\n        df['year'] = df[DATE_COL].dt.year # Captures long-term trends/phases of the pandemic\\n        df['month'] = df[DATE_COL].dt.month # Seasonal feature\\n        df['week_of_year'] = df[DATE_COL].dt.isocalendar().week.astype(int) # Fine-grained seasonality\\n\\n        # Add cyclical features for week of year to capture seasonality smoothly (e.g., flu season, holidays).\\n        # These are important for recurring patterns that are not strictly linear.\\n        df['sin_week_of_year'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\\n        df['cos_week_of_year'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\\n\\n        # Weeks since start of the entire dataset, to capture overall trend.\\n        df['weeks_since_start'] = ((df[DATE_COL] - min_date).dt.days / 7).astype(int)\\n        df['weeks_since_start_sq'] = df['weeks_since_start']**2 # Captures non-linear trend over time\\n\\n        return df\\n\\n    # Apply feature extraction to training and test dataframes\\n    df_train_full = add_base_features(df_train_full, min_date_global)\\n    test_x_processed = add_base_features(test_x.copy(), min_date_global)\\n\\n    # Define base features used for all predictions\\n    BASE_FEATURES = [POPULATION_COL, 'year', 'month', 'week_of_year',\\n                     'sin_week_of_year', 'cos_week_of_year', 'weeks_since_start',\\n                     'weeks_since_start_sq']\\n    CATEGORICAL_FEATURES_LIST = [LOCATION_COL] # Location is treated as a categorical variable\\n\\n    # --- 3. Generate time-series dependent features for training data ---\\n    train_features_df = df_train_full.copy()\\n\\n    # Generate lagged transformed target features for each location group.\\n    # These are typically the most important features for time series forecasting, capturing autocorrelation.\\n    for lag in LAG_WEEKS:\\n        train_features_df[f'lag_{lag}_wk'] = train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].shift(lag)\\n\\n    # Generate lagged differences of transformed target features: (value_t-1 - value_t-1-k)\\n    # Captures the rate of change in admissions, indicating rising or falling trends.\\n    for diff_period in LAG_DIFF_PERIODS:\\n        train_features_df[f'diff_lag_1_period_{diff_period}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].diff(periods=diff_period).shift(1)\\n\\n    # Generate rolling mean features, shifted by 1 to avoid data leakage.\\n    # Provides a smoothed trend of recent admissions, making the model robust to short-term noise.\\n    for window in ROLLING_WINDOWS:\\n        train_features_df[f'rolling_mean_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').mean()\\n            )\\n\\n    # Generate rolling standard deviation features, shifted by 1 to avoid data leakage.\\n    # Captures the variability or volatility of recent admissions, useful for uncertainty estimation.\\n    for window in ROLLING_STD_WINDOWS:\\n        train_features_df[f'rolling_std_{window}_wk'] = \\\\\\n            train_features_df.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].transform(\\n                lambda x: x.rolling(window=window, min_periods=1, closed='left').std()\\n            ).fillna(0) # Fill NaN for std where window is 1 (std is 0 for single value)\\n\\n    y_train_model = train_features_df[TRANSFORMED_TARGET_COL]\\n\\n    # Compile the list of all target-derived feature columns\\n    train_specific_features = [f'lag_{lag}_wk' for lag in LAG_WEEKS] + \\\\\\n                              [f'diff_lag_1_period_{p}_wk' for p in LAG_DIFF_PERIODS] + \\\\\\n                              [f'rolling_mean_{window}_wk' for window in ROLLING_WINDOWS] + \\\\\\n                              [f'rolling_std_{window}_wk' for window in ROLLING_STD_WINDOWS]\\n\\n    X_train_model_cols_pre_horizon = BASE_FEATURES + CATEGORICAL_FEATURES_LIST + train_specific_features\\n    X_train_model = train_features_df[X_train_model_cols_pre_horizon].copy()\\n\\n    # Add the 'horizon' feature to the training data. For historical data, horizon is 0.\\n    # This helps the model learn how forecast accuracy or patterns might change with prediction horizon.\\n    if HORIZON_COL not in X_train_model.columns:\\n        X_train_model[HORIZON_COL] = 0\\n\\n    # --- Time-series specific missing data handling for training features ---\\n    # Apply forward fill then fill remaining initial NaNs within each location group.\\n    # This handles missing values introduced by lagging/rolling, especially at the start of series.\\n    for col in train_specific_features:\\n        if X_train_model[col].isnull().any():\\n            X_train_model[col] = X_train_model.groupby(LOCATION_COL)[col].transform(lambda x: x.ffill())\\n            # For any initial NaNs (where no prior values exist to ffill), fill with 0.0.\\n            # This is a safe fallback for new locations or very early data points.\\n            X_train_model[col] = X_train_model[col].fillna(0.0)\\n\\n    train_combined = X_train_model.copy()\\n    train_combined[TRANSFORMED_TARGET_COL] = y_train_model\\n    # Drop rows where essential features/target are NaN *after* filling, ensuring clean training data.\\n    # This removes early rows for each location where lags couldn't be computed.\\n    train_combined.dropna(subset=train_specific_features + [TRANSFORMED_TARGET_COL], inplace=True)\\n\\n    X_train_model = train_combined.drop(columns=[TRANSFORMED_TARGET_COL])\\n    y_train_model = train_combined[TRANSFORMED_TARGET_COL]\\n\\n    # --- Handle categorical features for LightGBM and XGBoost ---\\n    # Get all unique locations from both train and test to ensure consistent categories.\\n    all_location_categories = pd.unique(pd.concat([X_train_model[LOCATION_COL], test_x_processed[LOCATION_COL]]))\\n    X_train_model[LOCATION_COL] = X_train_model[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n\\n    # Process 'horizon' as a categorical feature\\n    train_horizon_categories = X_train_model[HORIZON_COL].unique().tolist()\\n    test_horizon_categories_vals = test_x_processed[HORIZON_COL].unique().tolist()\\n    all_horizon_categories = sorted(list(set(train_horizon_categories + test_horizon_categories_vals)))\\n    X_train_model[HORIZON_COL] = X_train_model[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n    # Store the final column order from training data to ensure consistency during prediction\\n    X_train_model_cols = X_train_model.columns.tolist()\\n\\n    # Identify categorical feature column names for LightGBM\\n    categorical_feature_names = [col for col in CATEGORICAL_FEATURES_LIST + [HORIZON_COL] if col in X_train_model_cols]\\n\\n    # --- 4. Model Training (Ensemble of LightGBM and XGBoost models) ---\\n    models = {q: {} for q in QUANTILES} # Store models for each quantile and model type\\n\\n    for q in QUANTILES:\\n        # Train LightGBM models for the given quantile (alpha parameter)\\n        if 'lgbm' in ensemble_model_types and n_lgbm_ensemble_members > 0:\\n            models[q]['lgbm'] = []\\n            for i in range(n_lgbm_ensemble_members):\\n                lgbm_model_params_i = lgbm_params.copy()\\n                lgbm_model_params_i['alpha'] = q # Set the quantile for this specific model\\n                lgbm_model_params_i['random_state'] = lgbm_params['random_state'] + i # Vary seed for ensemble members\\n\\n                lgbm_model = LGBMRegressor(**lgbm_model_params_i)\\n                \\n                lgbm_model.fit(X_train_model, y_train_model,\\n                               categorical_feature=categorical_feature_names) # Pass categorical feature names\\n                models[q]['lgbm'].append(lgbm_model)\\n        \\n        # Train XGBoost models for the given quantile (alpha parameter)\\n        if 'xgb' in ensemble_model_types and n_xgb_ensemble_members > 0:\\n            models[q]['xgb'] = []\\n            for i in range(n_xgb_ensemble_members):\\n                xgb_model_params_i = xgb_params.copy()\\n                xgb_model_params_i['alpha'] = q # Use 'alpha' for quantile regression in XGBoost\\n                xgb_model_params_i['random_state'] = xgb_params['random_state'] + i\\n\\n                xgb_model = xgb.XGBRegressor(**xgb_model_params_i)\\n                # XGBoost sklearn API handles CategoricalDtype for columns specified as such\\n                xgb_model.fit(X_train_model, y_train_model) \\n                models[q]['xgb'].append(xgb_model)\\n\\n    # --- 5. Iterative Feature Generation and Prediction for Test Data ---\\n    # Initialize history for each location using full training data's transformed target values.\\n    # This allows test predictions to use recent actuals from the training period.\\n    location_history_data = df_train_full.groupby(LOCATION_COL)[TRANSFORMED_TARGET_COL].apply(list).to_dict()\\n\\n    # Store original test_x index for mapping back predictions.\\n    original_test_x_index = test_x.index\\n\\n    # Prepare test data for sequential processing, keeping original index and sorting.\\n    # Sorting ensures that we process data in chronological order per location.\\n    test_x_processed['original_index'] = test_x_processed.index\\n    test_x_processed[DATE_COL] = pd.to_datetime(test_x_processed[DATE_COL])\\n    test_x_processed = test_x_processed.sort_values(by=[LOCATION_COL, DATE_COL]).reset_index(drop=True)\\n\\n    # Initialize prediction DataFrame with the original test_x index and quantile columns.\\n    predictions_df = pd.DataFrame(index=original_test_x_index, columns=[f'quantile_{q}' for q in QUANTILES])\\n\\n    # Pre-calculate mean of transformed training target for robust fallback in case of no ensemble predictions\\n    mean_transformed_train_y_fallback = y_train_model.mean() if not y_train_model.empty else 0.0\\n\\n    # Loop through each row of the sorted test_x_processed to predict sequentially.\\n    # This is crucial for generating features that depend on previously predicted values (recursive forecasting).\\n    for idx, row in test_x_processed.iterrows():\\n        current_loc = row.loc[LOCATION_COL]\\n        original_idx = row.loc['original_index'] # Get the original index for placing predictions\\n\\n        # Retrieve current location history (list of transformed admissions).\\n        current_loc_hist = location_history_data.get(current_loc, [])\\n\\n        # Build feature dictionary for the current row using base and categorical features.\\n        # Use .loc for explicit label-based indexing.\\n        current_features_dict = {col: row.loc[col] for col in (BASE_FEATURES + CATEGORICAL_FEATURES_LIST + [HORIZON_COL]) if col in row}\\n\\n        # Generate dynamic lag features using current_loc_hist.\\n        # These features mimic the real-world scenario where past data is available.\\n        for lag in LAG_WEEKS:\\n            lag_col_name = f'lag_{lag}_wk'\\n            if len(current_loc_hist) >= lag:\\n                lag_value = current_loc_hist[-lag]\\n            elif current_loc_hist:\\n                lag_value = current_loc_hist[-1] # Fallback to most recent if not enough history\\n            else:\\n                lag_value = 0.0 # Default if no history at all for this location\\n            current_features_dict[lag_col_name] = lag_value\\n        \\n        # Generate dynamic lagged difference features: (value_t-1 - value_t-1-k)\\n        for diff_period in LAG_DIFF_PERIODS:\\n            diff_col_name = f'diff_lag_1_period_{diff_period}_wk'\\n            if len(current_loc_hist) >= 1 + diff_period:\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-(1 + diff_period)]\\n            elif len(current_loc_hist) >= 2: # Can at least compute a diff between last two available\\n                diff_value = current_loc_hist[-1] - current_loc_hist[-2]\\n            else: # Not enough history for any diff calculation\\n                diff_value = 0.0\\n            current_features_dict[diff_col_name] = diff_value\\n\\n        # Generate dynamic rolling mean features using current_loc_hist.\\n        for window in ROLLING_WINDOWS:\\n            rolling_col_name = f'rolling_mean_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_mean_val = np.mean(current_loc_hist[-window:])\\n            elif current_loc_hist:\\n                rolling_mean_val = np.mean(current_loc_hist) # Use what's available\\n            else:\\n                rolling_mean_val = 0.0\\n            current_features_dict[rolling_col_name] = rolling_mean_val\\n\\n        # Generate dynamic rolling std features using current_loc_hist.\\n        for window in ROLLING_STD_WINDOWS:\\n            rolling_std_col_name = f'rolling_std_{window}_wk'\\n            if len(current_loc_hist) >= window:\\n                rolling_std_val = np.std(current_loc_hist[-window:])\\n            elif len(current_loc_hist) > 1: # Need at least 2 points for meaningful std\\n                rolling_std_val = np.std(current_loc_hist)\\n            else:\\n                rolling_std_val = 0.0 # Default if not enough history\\n            current_features_dict[rolling_std_col_name] = rolling_std_val\\n\\n        # Convert to DataFrame row for prediction.\\n        X_test_row = pd.DataFrame([current_features_dict])\\n\\n        # Ensure X_test_row has the same columns and order as X_train_model, filling missing with 0.0.\\n        X_test_row = X_test_row.reindex(columns=X_train_model_cols, fill_value=0.0)\\n\\n        # Re-cast categorical features with appropriate types for prediction.\\n        X_test_row[LOCATION_COL] = X_test_row[LOCATION_COL].astype(pd.CategoricalDtype(categories=all_location_categories))\\n        X_test_row[HORIZON_COL] = X_test_row[HORIZON_COL].astype(pd.CategoricalDtype(categories=all_horizon_categories))\\n\\n        # Make predictions for all quantiles for this single row using the ensemble.\\n        row_predictions_transformed = {}\\n        for q in QUANTILES:\\n            ensemble_preds_for_q = []\\n\\n            # Get predictions from LGBM models for this quantile\\n            if 'lgbm' in ensemble_model_types and q in models and 'lgbm' in models[q]:\\n                for lgbm_model_q in models[q]['lgbm']:\\n                    pred = lgbm_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions for averaging\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            # Get predictions from XGBoost models for this quantile\\n            if 'xgb' in ensemble_model_types and q in models and 'xgb' in models[q]:\\n                for xgb_model_q in models[q]['xgb']:\\n                    pred = xgb_model_q.predict(X_test_row)[0]\\n                    if np.isfinite(pred): # Only add finite predictions for averaging\\n                        ensemble_preds_for_q.append(pred)\\n            \\n            if ensemble_preds_for_q:\\n                # Average predictions from all finite ensemble members for this quantile\\n                row_predictions_transformed[q] = np.mean(ensemble_preds_for_q)\\n            else:\\n                # Fallback: if all ensemble members for this quantile produced non-finite predictions\\n                # or if no models of that type were configured.\\n                row_predictions_transformed[q] = mean_transformed_train_y_fallback\\n\\n        transformed_preds_array = np.array([row_predictions_transformed[q] for q in QUANTILES]) # Ensure order\\n\\n        # --- IMPORTANT: Clip transformed predictions BEFORE inverse transformation ---\\n        # This prevents extremely large transformed values from leading to \`inf\` after inverse_transform.\\n        transformed_preds_array = np.clip(transformed_preds_array, 0.0, MAX_TRANSFORMED_PREDICTION_CAP)\\n\\n        # Inverse transform predictions from transformed target scale back to admissions per million.\\n        inv_preds_admissions_per_million = inverse_transform(transformed_preds_array)\\n\\n        # Apply clipping to prevent excessively large predictions in admissions per million.\\n        # This clip is now primarily for setting a high-level ceiling, as values should already be bounded.\\n        inv_preds_admissions_per_million = np.clip(inv_preds_admissions_per_million, 0.0, MAX_ADMISSIONS_PER_MILLION)\\n\\n        # Convert from admissions per million back to total admissions.\\n        population_val = row.loc[POPULATION_COL]\\n        # Handle zero population explicitly: if population is zero, admissions must be zero.\\n        if population_val == 0:\\n            final_preds_total_admissions = np.zeros_like(inv_preds_admissions_per_million)\\n        else:\\n            final_preds_total_admissions = inv_preds_admissions_per_million * population_val / 1_000_000\\n\\n        # Round to nearest integer as admissions are discrete counts and ensure non-negative.\\n        final_preds_total_admissions = np.round(np.maximum(0, final_preds_total_admissions)).astype(int)\\n\\n        # Store predictions in the final DataFrame using original index.\\n        for i, q in enumerate(QUANTILES):\\n            predictions_df.loc[original_idx, f'quantile_{q}'] = final_preds_total_admissions[i]\\n\\n        # Update the history for the current location with the median prediction.\\n        # This recursive step is critical for multi-step forecasting; the median (0.5 quantile)\\n        # is used as the best single-point estimate to inform future feature generation.\\n        # Use the *clipped* transformed median prediction for history to maintain stability.\\n        median_pred_transformed_clipped = transformed_preds_array[QUANTILES.index(0.5)]\\n        value_to_add_to_history = median_pred_transformed_clipped\\n        \\n        location_history_data.setdefault(current_loc, []).append(value_to_add_to_history)\\n\\n    # Ensure monotonicity of quantiles across each row (important for valid quantile forecasts).\\n    # This step corrects any non-monotonic predictions that might arise from independent quantile models.\\n    predictions_array = predictions_df.values.astype(float)\\n    predictions_array.sort(axis=1) # Sorts each row in-place to ensure quantiles are monotonically increasing\\n    # Convert back to DataFrame with original columns and index.\\n    predictions_df = pd.DataFrame(predictions_array, columns=predictions_df.columns, index=predictions_df.index)\\n\\n    # Ensure all predictions are non-negative integers as required for counts\\n    predictions_df = predictions_df.apply(lambda x: np.maximum(0, x)).astype(int)\\n\\n    return predictions_df\\n\\n# YOUR config_list\\nconfig_list = [\\n    { # Config 1: Baseline LGBM-only with fourth_root transform (replicates previous best result)\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.03,\\n            'num_leaves': 26,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {}, # No XGBoost for this config\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [8, 16, 26],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 0, \\n        'max_admissions_per_million': 10000.0 # Original working value\\n    },\\n    { # Config 2: Ensemble of LGBM and XGBoost, fourth_root transform, adjusted clip.\\n      # This aims to combine the robustness of fourth_root with the diversity of an ensemble.\\n      # Parameters are relatively conservative to ensure stability.\\n      # MAX_ADMISSIONS_PER_MILLION increased to 10000.0 like Config 1.\\n      # MAX_TRANSFORMED_PREDICTION_CAP is dynamically calculated and capped.\\n        'lgbm_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5, \\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 200,\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10,\\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root', \\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'], \\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 3: Ensemble with slightly more aggressive LGBM and XGBoost, fourth_root, and expanded features.\\n      # Explores if more complex models and features improve scores with the robust transform.\\n      # MAX_ADMISSIONS_PER_MILLION increased to 10000.0.\\n      # MAX_TRANSFORMED_PREDICTION_CAP is dynamically calculated and capped.\\n        'lgbm_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'num_leaves': 30,\\n            'max_depth': 6,\\n            'min_child_samples': 25,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.7,\\n            'subsample': 0.7,\\n            'reg_alpha': 0.15,\\n            'reg_lambda': 0.15\\n        },\\n        'xgb_params': {\\n            'n_estimators': 250,\\n            'learning_rate': 0.025,\\n            'max_depth': 4,\\n            'min_child_weight': 15,\\n            'subsample': 0.7,\\n            'colsample_bytree': 0.7,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.8,\\n            'lambda': 0.15,\\n            'alpha': 0.15 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 3, 4, 8, 12, 26, 52], # More lags\\n        'lag_diff_periods': [1, 2, 3, 4, 6, 8], # More diff periods\\n        'rolling_windows': [4, 8, 12, 16, 26], # More rolling windows\\n        'rolling_std_windows': [4, 8, 12],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 1,\\n        'n_xgb_ensemble_members': 1,\\n        'max_admissions_per_million': 10000.0 \\n    },\\n    { # Config 4: Ensemble with 2 members for each model type, slightly reduced estimators, fourth_root.\\n      # Aims for higher ensemble diversity to potentially improve calibration and robustness.\\n      # MAX_ADMISSIONS_PER_MILLION increased to 10000.0.\\n      # MAX_TRANSFORMED_PREDICTION_CAP is dynamically calculated and capped.\\n        'lgbm_params': {\\n            'n_estimators': 150, # Reduced estimators to balance with more ensemble members\\n            'learning_rate': 0.03,\\n            'num_leaves': 25,\\n            'max_depth': 5,\\n            'min_child_samples': 20,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'verbose': -1,\\n            'colsample_bytree': 0.8,\\n            'subsample': 0.8,\\n            'reg_alpha': 0.1,\\n            'reg_lambda': 0.1\\n        },\\n        'xgb_params': {\\n            'n_estimators': 150, # Reduced estimators\\n            'learning_rate': 0.03,\\n            'max_depth': 3, \\n            'min_child_weight': 10, \\n            'subsample': 0.8,\\n            'colsample_bytree': 0.8,\\n            'random_state': 42,\\n            'n_jobs': -1,\\n            'tree_method': 'hist',\\n            'enable_categorical': True,\\n            'gamma': 0.5,\\n            'lambda': 1.0,\\n            'alpha': 0.1 \\n        },\\n        'target_transform': 'fourth_root',\\n        'lag_weeks': [1, 2, 4, 8, 16, 26, 52],\\n        'lag_diff_periods': [1, 2, 4, 8],\\n        'rolling_windows': [4, 8, 16],\\n        'rolling_std_windows': [4, 8],\\n        'ensemble_model_types': ['lgbm', 'xgb'],\\n        'n_lgbm_ensemble_members': 2, # Two LGBM members per quantile\\n        'n_xgb_ensemble_members': 2,   # Two XGB members per quantile\\n        'max_admissions_per_million': 10000.0\\n    }\\n]"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

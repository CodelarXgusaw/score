<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 514.0,
  "old_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\n# The config_list can be adjusted to explore more hyperparameter combinations.\\n# The current config_list is designed to offer a diverse set of strategies, including\\n# simple baselines, additive/multiplicative models, different trend behaviors,\\n# primary/secondary seasonality, and residual corrections. This diversity aims\\n# to generalize well across the various GiftEval datasets within the \`MAX_CONFIGS\` limit.\\n# The selection of 8 configurations provides necessary coverage for various time series\\n# patterns, from simple to complex, without redundant combinations.\\n\\n# Try to reduce the number of configs, prune unnecessary combinations, and\\n# up the diversity of the remaining configs. Do not overfit to validation data.\\nconfig_list = [\\n    # Config 0 (IMPROVED BASELINE): More Robust Naive. For very short or erratic series.\\n    # Changed from 'last_value' to 'median_last_k_window' with k=7 for better stability.\\n    {'name': 'median_k_window_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 7}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Improved: Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    Correctly distinguishes between global and local trend fitting for \`fitted_trend_on_full_train\`.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Full history for trend fitting\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        # For constant trend, fitted value is constant across full train_len\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment)) # 0 to len(y_fit_segment)-1\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min_segment = x_fit_segment_local.min()\\n        x_max_segment = x_fit_segment_local.max()\\n        x_range_segment = x_max_segment - x_min_segment\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range_segment == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min_segment) / x_range_segment\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale\\n            x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n            x_forecast_normalized = (x_forecast_raw - x_min_segment) / x_range_segment\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: Logic corrected for global vs local trend\\n            if trend_window_multiplier is None: # Global trend, fit on full history\\n                # Use full training indices, normalized by full range\\n                x_full_train_for_fit = np.arange(train_len)\\n                x_min_full_train = x_full_train_for_fit.min()\\n                x_max_full_train = x_full_train_for_fit.max()\\n                x_range_full_train = x_max_full_train - x_min_full_train\\n                \\n                # Handle single-point case for global trend (should already be handled by y_fit_segment < 1 or range == 0 above)\\n                if x_range_full_train == 0:\\n                    fitted_trend_on_full_train = np.full(train_len, np.median(y_fit_segment))\\n                else:\\n                    x_full_train_normalized = (x_full_train_for_fit - x_min_full_train) / x_range_full_train\\n                    fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n            else: # Local trend, fit on window, apply fitted values only to that window\\n                # fitted_trend_on_full_train is already initialized to zeros\\n                fitted_trend_on_full_train[fit_start_idx:] = trend_poly(x_fit_segment_local_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    Correctly fills \`fitted_rc_on_train\` for consistency with boosting-like approach.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float) # Initialize to zeros\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        # Only apply fitted values to the window used for calculation\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value * (residual_correction_damping_factor ** np.arange(rc_fit_window))[::-1]\\n    else:\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Calculate transformed fallback on the original scale, then apply transformation if needed\\n        final_fallback_val = initial_base_level_fallback_val # Use the value derived from original_targets\\n        if transform_log:\\n            # Need to re-apply transformation logic safely for 0.0 or negative values\\n            final_fallback_val = np.expm1(np.log1p(np.maximum(0, final_fallback_val)))\\n        \\n        final_fallback_pred = np.full(prediction_length, final_fallback_val)\\n        \\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Re-evaluate fallback on processed (potentially log) scale for this path.\\n        if transform_log:\\n            fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level':\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 537,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 3 # Incremented version to reflect latest review and validation\\n\\n# The config_list has been reduced to 5 configurations, focusing on diversity and\\n# generalization. Redundant or overly similar configurations have been pruned.\\n# The selection now includes: a robust naive baseline, a general additive model,\\n# two distinct multiplicative models (one for linear growth, one for complex\\n# high-frequency seasonality), and a comprehensive additive model for multiple seasonalities.\\nconfig_list = [\\n    # Config 0: Robust Naive Baseline - Median of last k-window.\\n    # Essential for very short or erratic series, providing a stable simple forecast.\\n    {'name': 'base_median_naive', 'components': [{'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 7}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Additive - Local Linear Trend + Primary Seasonal + Median Residual Correction.\\n    # Good general-purpose additive model for series with local trend and clear primary seasonality.\\n    # Base level is implicitly handled by the trend component starting from a suitable intercept.\\n    {'name': 'additive_local_trend_primary_rc', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 2: Multiplicative - Global Linear Trend + Primary Seasonal (Log-transformed).\\n    # Effective for data with exponential growth/decay and strong primary seasonality.\\n    {'name': 'multiplicative_global_trend_primary_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative - Constant Trend + Secondary Seasonal + Residual Correction (Log-transformed).\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively.\\n    {'name': 'multiplicative_const_trend_secondary_rc_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4: Comprehensive Additive - Global Linear Trend + Full Secondary Seasonal + Residual Correction.\\n    # Robust for complex datasets with multiple seasonalities and a general trend in an additive manner.\\n    {'name': 'comprehensive_additive_seasonal_rc', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Improved: Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    Correctly distinguishes between global and local trend fitting for \`fitted_trend_on_full_train\`.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Full history for trend fitting\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        # For constant trend, fitted value is constant across full train_len\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment)) # 0 to len(y_fit_segment)-1\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min_segment = x_fit_segment_local.min()\\n        x_max_segment = x_fit_segment_local.max()\\n        x_range_segment = x_max_segment - x_min_segment\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range_segment == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min_segment) / x_range_segment\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale\\n            x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n            x_forecast_normalized = (x_forecast_raw - x_min_segment) / x_range_segment\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: Logic corrected for global vs local trend\\n            if trend_window_multiplier is None: # Global trend, fit on full history\\n                # Use full training indices, normalized by full range\\n                x_full_train_for_fit = np.arange(train_len)\\n                x_min_full_train = x_full_train_for_fit.min()\\n                x_max_full_train = x_full_train_for_fit.max()\\n                x_range_full_train = x_max_full_train - x_min_full_train\\n                \\n                # Handle single-point case for global trend (should already be handled by y_fit_segment < 1 or range == 0 above)\\n                if x_range_full_train == 0:\\n                    fitted_trend_on_full_train = np.full(train_len, np.median(y_fit_segment))\\n                else:\\n                    x_full_train_normalized = (x_full_train_for_fit - x_min_full_train) / x_range_full_train\\n                    fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n            else: # Local trend, fit on window, apply fitted values only to that window\\n                # fitted_trend_on_full_train is already initialized to zeros\\n                fitted_trend_on_full_train[fit_start_idx:] = trend_poly(x_fit_segment_local_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    Correctly fills \`fitted_rc_on_train\` for consistency with boosting-like approach.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float) # Initialize to zeros\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        # Only apply fitted values to the window used for calculation\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value * (residual_correction_damping_factor ** np.arange(rc_fit_window))[::-1]\\n    else:\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Calculate transformed fallback on the original scale, then apply transformation if needed\\n        final_fallback_val = initial_base_level_fallback_val # Use the value derived from original_targets\\n        if transform_log:\\n            # Need to re-apply transformation logic safely for 0.0 or negative values\\n            final_fallback_val = np.expm1(np.log1p(np.maximum(0, final_fallback_val)))\\n        \\n        final_fallback_pred = np.full(prediction_length, final_fallback_val)\\n        \\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Re-evaluate fallback on processed (potentially log) scale for this path.\\n        if transform_log:\\n            fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Ensure a base_level config always exists as the first component applied\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        # Default base_level if not explicitly provided in config\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components based on a fixed, logical order ---\\n    # This ensures a consistent decomposition hierarchy regardless of config component order.\\n    FIXED_COMPONENT_ORDER = ['trend', 'seasonal_primary', 'seasonal_secondary', 'residual_correction']\\n\\n    for comp_type in FIXED_COMPONENT_ORDER:\\n        # Find the specific config for this component type\\n        component_config = next((c for c in config.get('components', []) if c.get('type') == comp_type), None)\\n        \\n        if component_config: # Only apply if this component type is specified in the current config\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[comp_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

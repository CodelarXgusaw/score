<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 799.0,
  "old_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# The solution aims to improve accuracy and generalize effectively across diverse GiftEval datasets\\n# by refining the fit_and_predict_fn and its configurations.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 7 # Incremented version to reflect numerical stability improvements\\n\\n# The config_list offers a diverse set of forecasting strategies, balancing between\\n# robustness, adaptability, and the ability to capture complex patterns (trend, seasonality, residuals).\\n# The goal is to maximize performance across GiftEval datasets within the \`MAX_CONFIGS\` limit of 8,\\n# by ensuring each configuration is distinct and powerful.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Simple and ultra-robust, suitable for highly erratic or very short series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal. Effective for exponential growth/decay with strong primary seasonality.\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2: Additive: More Reactive Linear Trend + Primary Seasonal + Median RC. For moderately fast-changing patterns with clear primary seasonality.\\n    {'name': 'additive_more_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC. Ideal for high-frequency data where complex daily/weekly patterns scale.\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4: Additive: Adaptive Stable Base + Long Primary Seasonal Only. For very stable, strong seasonal patterns with minimal trend/noise.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # Adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal. Robust for datasets with consistent, global linear changes.\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC. For log-transformed series with a stable level, but seasonality/residuals are key.\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC. Robust for complex datasets with multiple seasonalities and a general trend.\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.85} # Slightly stronger damping\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Trend is projected over the entire training history for consistent residual calculation.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize full array\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history (None multiplier)\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n    # X-coordinates for fitting are the actual absolute indices within the training data\\n    x_fit_segment_raw = np.arange(fit_start_idx, train_len)\\n\\n    if len(y_fit_segment) < 1 or len(x_fit_segment_raw) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        \\n        # For a constant trend, apply it uniformly over the training history.\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        # Use np.isclose for floating-point robustness\\n        if np.all(np.isclose(y_fit_segment, y_fit_segment[0], atol=1e-9)):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree) # Ensure degree is non-negative, at least 0\\n\\n        # Fallback to constant median if not enough points for polynomial degree\\n        # This handles cases where len(y_fit_segment) <= effective_degree\\n        if len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        # --- Numerical Stability Improvement: Normalize X-coordinates for polyfit ---\\n        # Scale x_fit_segment_raw to a 0-1 range for numerical stability.\\n        min_x_fit = x_fit_segment_raw[0]\\n        max_x_fit = x_fit_segment_raw[-1]\\n        range_x_fit = max_x_fit - min_x_fit\\n\\n        if range_x_fit > 0:\\n            scaled_x_fit_segment = (x_fit_segment_raw - min_x_fit) / range_x_fit\\n        else: # All x values are the same, effectively a constant line\\n            scaled_x_fit_segment = np.zeros_like(x_fit_segment_raw, dtype=float)\\n\\n        # Fit polynomial using the scaled x-coordinates.\\n        poly_coeffs = np.polyfit(scaled_x_fit_segment, y_fit_segment, effective_degree)\\n        trend_poly = np.poly1d(poly_coeffs)\\n\\n        # Forecast: x-values for prediction need to be absolute indices extending from train_len\\n        x_forecast_raw = np.arange(train_len, train_len + prediction_length)\\n        if range_x_fit > 0:\\n            scaled_x_forecast = (x_forecast_raw - min_x_fit) / range_x_fit\\n        else:\\n            scaled_x_forecast = np.zeros_like(x_forecast_raw, dtype=float)\\n        trend_forecast_component = trend_poly(scaled_x_forecast)\\n\\n        # Fitted: This is the critical correction for residual calculation.\\n        # If a window was used (local fit), apply the fitted trend only to that window.\\n        # Values before fit_start_idx remain 0, consistent with a local trend component\\n        # not impacting earlier residuals in this sequential decomposition step.\\n        if window_size < train_len:\\n            # Use scaled x-values for the fitted segment\\n            fitted_trend_on_full_train[fit_start_idx:] = trend_poly(scaled_x_fit_segment)\\n        else:\\n            # Full history fit, apply to all points\\n            x_full_train_raw = np.arange(train_len)\\n            if range_x_fit > 0:\\n                scaled_x_full_train = (x_full_train_raw - min_x_fit) / range_x_fit\\n            else:\\n                scaled_x_full_train = np.zeros_like(x_full_train_raw, dtype=float)\\n            fitted_trend_on_full_train = trend_poly(scaled_x_full_train)\\n\\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    # Replacing inf/-inf with 0.0, or NaNs from operations that might occur.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1] if train_len > 0 else 0.0\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length if season_length > 0 else 7) # Ensure minimum k_window_size\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This ensures that np.nanmedian can operate on full columns, even if the last cycle is incomplete.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        # Only compute seasonal pattern if at least one full cycle is available\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality): # Requires at least 2 full years of data\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality): # Requires at least 2 full years of data\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0: # Should not happen if train_len > 0 and rc_fit_window min 1\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean') # Default to mean if not specified\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # Fitted: The amount of correction that would have been applied to the training window.\\n    # Apply the constant correction value to the window from which it was derived.\\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    # Forecast: Apply damping factor for forecast, decaying with prediction horizon.\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        # If no decay, apply constant value to forecast (potentially damped once, if damping factor is < 1.0)\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else: # If all original targets are NaN, default to 0.0\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    # This fills most NaNs based on adjacent valid values.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite. If ffill/bfill left NaNs (e.g., all NaNs input),\\n    # or introduced +/-inf, this ensures they are replaced by the robust fallback.\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine the fallback value on the processed (potentially log) scale\\n        fallback_val_on_processed_scale = initial_base_level_fallback_val\\n        if transform_log:\\n            # For log transform, ensure non-negative before log1p\\n            fallback_val_on_processed_scale = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n\\n        # Create predictions on the processed scale\\n        predictions_on_processed_scale = np.full(prediction_length, fallback_val_on_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was applied\\n        if transform_log:\\n            predictions = np.expm1(predictions_on_processed_scale)\\n        else:\\n            predictions = predictions_on_processed_scale\\n\\n        # Apply non-negativity constraint\\n        if config.get('non_negative', False):\\n            predictions = np.maximum(0, predictions)\\n            \\n        return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # If the last \`effective_season_length_for_naive\` values are empty (e.g., train_len < season_length),\\n        # use the initial base level (on the processed scale) for naive fallback.\\n        if transform_log:\\n            fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Ensure a base_level component is always present, even if not explicitly in config\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite at this point.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level': # Base level already handled\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train # Subtract fitted component from residuals for next step\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 823,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# The solution aims to improve accuracy and generalize effectively across diverse GiftEval datasets\\n# by refining the fit_and_predict_fn and its configurations.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 8 # Incremented version to reflect numerical stability improvements and logical fixes\\n\\n# The config_list offers a diverse set of forecasting strategies, balancing between\\n# robustness, adaptability, and the ability to capture complex patterns (trend, seasonality, residuals).\\n# The goal is to maximize performance across GiftEval datasets within the \`MAX_CONFIGS\` limit of 8,\\n# by ensuring each configuration is distinct and powerful.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Simple and ultra-robust, suitable for highly erratic or very short series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal. Effective for exponential growth/decay with strong primary seasonality.\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2: Additive: More Reactive Linear Trend + Primary Seasonal + Median RC. For moderately fast-changing patterns with clear primary seasonality.\\n    {'name': 'additive_more_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC. Ideal for high-frequency data where complex daily/weekly patterns scale.\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4: Additive: Adaptive Stable Base + Long Primary Seasonal Only. For very stable, strong seasonal patterns with minimal trend/noise.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # Adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal. Robust for datasets with consistent, global linear changes.\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC. For log-transformed series with a stable level, but seasonality/residuals are key.\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC. Robust for complex datasets with multiple seasonalities and a general trend.\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.85} # Slightly stronger damping\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Trend is projected over the entire training history for consistent residual calculation.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize full array\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history (None multiplier)\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n    # X-coordinates for fitting are the actual absolute indices within the training data\\n    x_fit_segment_raw = np.arange(fit_start_idx, train_len)\\n\\n    if len(y_fit_segment) < 1 or len(x_fit_segment_raw) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        \\n        # For a constant trend, apply it uniformly over the training history.\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        # Use np.isclose for floating-point robustness\\n        if np.all(np.isclose(y_fit_segment, y_fit_segment[0], atol=1e-9)):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree) # Ensure degree is non-negative, at least 0\\n\\n        # Fallback to constant median if not enough points for polynomial degree\\n        # This handles cases where len(y_fit_segment) <= effective_degree\\n        if len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        # --- Numerical Stability Improvement: Normalize X-coordinates for polyfit ---\\n        # Scale x_fit_segment_raw to a 0-1 range for numerical stability.\\n        min_x_fit = x_fit_segment_raw[0]\\n        max_x_fit = x_fit_segment_raw[-1]\\n        range_x_fit = max_x_fit - min_x_fit\\n\\n        if range_x_fit > 0:\\n            scaled_x_fit_segment = (x_fit_segment_raw - min_x_fit) / range_x_fit\\n        else: # All x values are the same, effectively a constant line\\n            scaled_x_fit_segment = np.zeros_like(x_fit_segment_raw, dtype=float)\\n\\n        # Fit polynomial using the scaled x-coordinates.\\n        poly_coeffs = np.polyfit(scaled_x_fit_segment, y_fit_segment, effective_degree)\\n        trend_poly = np.poly1d(poly_coeffs)\\n\\n        # Forecast: x-values for prediction need to be absolute indices extending from train_len\\n        x_forecast_raw = np.arange(train_len, train_len + prediction_length)\\n        if range_x_fit > 0:\\n            scaled_x_forecast = (x_forecast_raw - min_x_fit) / range_x_fit\\n        else:\\n            scaled_x_forecast = np.zeros_like(x_forecast_raw, dtype=float)\\n        trend_forecast_component = trend_poly(scaled_x_forecast)\\n\\n        # Fitted: This is the critical correction for residual calculation.\\n        # If a window was used (local fit), apply the fitted trend only to that window.\\n        # Values before fit_start_idx remain 0, consistent with a local trend component\\n        # not impacting earlier residuals in this sequential decomposition step.\\n        if window_size < train_len:\\n            # Use scaled x-values for the fitted segment\\n            fitted_trend_on_full_train[fit_start_idx:] = trend_poly(scaled_x_fit_segment)\\n        else:\\n            # Full history fit, apply to all points\\n            x_full_train_raw = np.arange(train_len)\\n            if range_x_fit > 0:\\n                scaled_x_full_train = (x_full_train_raw - min_x_fit) / range_x_fit\\n            else:\\n                scaled_x_full_train = np.zeros_like(x_full_train_raw, dtype=float)\\n            fitted_trend_on_full_train = trend_poly(scaled_x_full_train)\\n\\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    # Replacing inf/-inf with 0.0, or NaNs from operations that might occur.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1] if train_len > 0 else 0.0\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length if season_length > 0 else 7) # Ensure minimum k_window_size\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This ensures that np.nanmedian can operate on full columns, even if the last cycle is incomplete.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        # Only compute seasonal pattern if at least one full cycle is available\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            segment_data = historical_data[mask]\\n            if len(segment_data) > 0: # Only compute if there's data for this attribute value\\n                median_val = np.nanmedian(segment_data) # Use np.nanmedian to handle NaNs\\n                if np.isfinite(median_val): # Only update if the computed median is finite\\n                    mapping_array[attr_val] = median_val\\n        \\n        # Ensure mapping array is finite (should be if fallback_value is finite and median_val checked)\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality): # Requires at least 2 full years of data\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality): # Requires at least 2 full years of data\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0: # Should not happen if train_len > 0 and rc_fit_window min 1\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean') # Default to mean if not specified\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # Fitted: The amount of correction that would have been applied to the training window.\\n    # Apply the constant correction value to the window from which it was derived.\\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    # Forecast: Apply damping factor for forecast, decaying with prediction horizon.\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        # If no decay, apply constant value to forecast (potentially damped once, if damping factor is < 1.0)\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else: # If all original targets are NaN, default to 0.0\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    # This fills most NaNs based on adjacent valid values.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite. If ffill/bfill left NaNs (e.g., all NaNs input),\\n    # or introduced +/-inf, this ensures they are replaced by the robust fallback.\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine the fallback value on the processed (potentially log) scale\\n        fallback_val_on_processed_scale = initial_base_level_fallback_val\\n        if transform_log:\\n            # For log transform, ensure non-negative before log1p\\n            fallback_val_on_processed_scale = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n\\n        # Create predictions on the processed scale\\n        predictions_on_processed_scale = np.full(prediction_length, fallback_val_on_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was applied\\n        if transform_log:\\n            predictions = np.expm1(predictions_on_processed_scale)\\n        else:\\n            predictions = predictions_on_processed_scale\\n\\n        # Non-negativity for this path is now handled by the final global check\\n        return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # If the last \`effective_season_length_for_naive\` values are empty (e.g., train_len < season_length),\\n        # use the initial base level (on the processed scale) for naive fallback.\\n        if transform_log:\\n            fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    # Non-negativity for fallback is now handled by the final global check\\n    # seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions) # REMOVED\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Ensure a base_level component is always present, even if not explicitly in config\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite at this point.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level': # Base level already handled\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train # Subtract fitted component from residuals for next step\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

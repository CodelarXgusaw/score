<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 645.0,
  "old_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\nconfig_list = [\\n    # Config 0 (IMPROVED): Additive: Adaptive Median Base + Light Residual Correction.\\n    # More generally robust than last_value, adapting to recent median with a light residual dampening.\\n    {'name': 'additive_adaptive_median_rc_0', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 10}, # Very reactive base\\n        {'type': 'residual_correction', 'rc_window_size': 5, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.9} # Light, reactive correction\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros for clarity\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    # else: window_size remains train_len (full history)\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n        fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Constant trend applies globally to residuals\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Constant fallback applies globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        # Use relative indices for the segment for fitting, starting from 0.\\n        x_fit_segment_local = np.arange(len(y_fit_segment), dtype=float) # This goes from 0 to window_size - 1\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min_normalized_ref = x_fit_segment_local.min() # This will be 0\\n        x_max_normalized_ref = x_fit_segment_local.max() # This will be len(y_fit_segment) - 1\\n        x_range_normalized_ref = x_max_normalized_ref - x_min_normalized_ref\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range_normalized_ref == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Fallback to constant globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min_normalized_ref) / x_range_normalized_ref\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale.\\n            # These are indices relative to the start of the *segment used for fitting*.\\n            x_forecast_relative_to_segment_start = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length, dtype=float)\\n            x_forecast_normalized = (x_forecast_relative_to_segment_start - x_min_normalized_ref) / x_range_normalized_ref\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: Apply the fitted trend across the ENTIRE training history.\\n            # x-values for full training data, relative to the *start of the segment used for fitting*.\\n            x_full_train_relative_to_segment_start = np.arange(train_len, dtype=float) - fit_start_idx\\n            x_full_train_normalized = (x_full_train_relative_to_segment_start - x_min_normalized_ref) / x_range_normalized_ref\\n            fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Fallback to constant globally\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length, dtype=float), np.zeros(train_len, dtype=float)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val, dtype=float), np.full(train_len, base_val, dtype=float)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length, dtype=float), np.zeros(train_len, dtype=float)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# Helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\ndef _calculate_seasonal_median_optimized(attribute_values_train: np.ndarray, historical_data_train: np.ndarray,\\n                                         attribute_values_predict: np.ndarray, num_possible_values: int,\\n                                         fallback_value: float) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates median seasonal pattern for integer-indexed attributes using sorting for efficiency.\\n    \`historical_data_train\` is assumed to be finite.\\n    \\"\\"\\"\\n    seasonal_medians = np.full(num_possible_values, fallback_value, dtype=float)\\n\\n    if historical_data_train.size == 0:\\n        return np.full(len(attribute_values_predict), fallback_value, dtype=float), np.full(len(attribute_values_train), fallback_value, dtype=float)\\n    \\n    # Create pairs of (attribute, data) and sort by attribute\\n    paired_data = np.column_stack((attribute_values_train, historical_data_train))\\n    \\n    # Sort by the first column (attribute_values_train). Using 'mergesort' for stability.\\n    sorted_indices = paired_data[:, 0].argsort(kind='mergesort')\\n    sorted_paired_data = paired_data[sorted_indices]\\n\\n    # Find the unique attribute values and their start indices in the sorted array\\n    # np.diff tells us where the value changes. Add a start index (0) to mark the first group.\\n    unique_attrs_sorted = sorted_paired_data[:, 0]\\n    unique_indices = np.concatenate(([0], np.where(np.diff(unique_attrs_sorted) != 0)[0] + 1))\\n    \\n    # Iterate over unique attributes and calculate their medians\\n    for i, start_idx in enumerate(unique_indices):\\n        attr_val = int(sorted_paired_data[start_idx, 0])\\n        end_idx = unique_indices[i+1] if i + 1 < len(unique_indices) else len(sorted_paired_data)\\n        \\n        segment_data = sorted_paired_data[start_idx:end_idx, 1]\\n        \\n        # historical_data_train is already finite, so segment_data is finite.\\n        if len(segment_data) > 0:\\n            seasonal_medians[attr_val] = np.median(segment_data)\\n        # Else, it remains fallback_value\\n\\n    seasonal_medians = np.nan_to_num(seasonal_medians, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n    \\n    fitted_comp = seasonal_medians[attribute_values_train]\\n    forecast_comp = seasonal_medians[attribute_values_predict]\\n    \\n    return forecast_comp, fitted_comp\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This allows np.nanmedian to ignore empty slots in partial cycles.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN (e.g., if all padded data).\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    # Using .values for direct NumPy array access\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median_optimized(\\n            train_composite_dh, historical_data, pred_composite_dh, num_dh_values, fallback_value)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofweek, historical_data, pred_dayofweek, 7, fallback_value) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_hour, historical_data, pred_hour, 24, fallback_value) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_month - 1, historical_data, pred_month - 1, 12, fallback_value) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofyear - 1, historical_data, pred_dayofyear - 1, 366, fallback_value) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float) # Change: Initialize as all zeros\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # For forecast, apply decay from the first prediction step onwards\\n    rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length, dtype=float))\\n\\n    # fitted_rc_on_train remains zeros, as this component is primarily for forecast correction.\\n    # It does not historically explain residuals for subsequent components in this revised logic.\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Handle Edge Case: Empty or Very Short Processed Input (all NaNs)\\n    if train_len == 0:\\n        # Determine fallback value on the processed scale (original or log)\\n        if transform_log:\\n            # On log scale, 0.0 implies original scale 0 (expm1(0) = 0).\\n            fallback_val_processed_scale = 0.0\\n        else:\\n            fallback_val_processed_scale = initial_base_level_fallback_val\\n        \\n        # Create predictions on processed scale\\n        final_fallback_pred_processed_scale = np.full(prediction_length, fallback_val_processed_scale, dtype=float)\\n        \\n        # Transform back to original scale if log transformation was used\\n        if transform_log:\\n            final_fallback_pred = np.expm1(final_fallback_pred_processed_scale)\\n        else:\\n            final_fallback_pred = final_fallback_pred_processed_scale\\n        \\n        # Apply non-negativity constraint\\n        if non_negative:\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        \\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience to fill internal NaNs.\\n    # This creates a copy and handles NaNs in a standard way.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # If all values are still NaN after ffill/bfill (meaning input_targets was entirely NaN),\\n    # replace with the determined fallback value to prevent NaN propagation.\\n    if np.all(np.isnan(filled_targets_np)):\\n        filled_targets_np = np.full_like(filled_targets_np, initial_base_level_fallback_val, dtype=float)\\n\\n    # --- 3. Apply transformation and ensure processed_targets_np is finite ---\\n    if transform_log:\\n        # Clamp values to a small positive epsilon before log1p to avoid log1p(0) or log1p(negative).\\n        # Also, ensure result is finite by replacing posinf with a large but finite value on log scale.\\n        processed_targets_np = np.log1p(np.maximum(1e-9, filled_targets_np))\\n        # Handle inf values explicitly: replace posinf with a large float, neginf with 0.0 (already covered by max(1e-9, .))\\n        # A large but finite value on the log scale, e.g., representing expm1(100)\\n        max_log_val_finite = 100.0 # This translates to a very large number (e.g., ~2.68e43) on the original scale\\n        processed_targets_np = np.nan_to_num(processed_targets_np, nan=0.0, posinf=max_log_val_finite, neginf=0.0)\\n    else:\\n        # For non-log transformed data, just ensure it's finite using the original scale fallback.\\n        processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 4. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    \\n    # Use the last \`effective_season_length_for_naive\` values from processed_targets_np\\n    # If training data is shorter than season_length, use whatever available.\\n    if train_len >= effective_season_length_for_naive:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n    else:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np # Use all available data\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0: \\n        # Fallback for truly empty or invalid processed data (e.g., if processed_targets_np became empty somehow)\\n        base_seasonal_pattern_naive_processed_scale = np.array([0.0], dtype=float) # 0.0 on processed scale (log or original)\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    if non_negative:\\n        seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 5. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Extract base_level config first (it's often foundational)\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        # Default base_level if not explicitly defined in config components\\n        # Using a more adaptive default base_level_method: median_last_k_window\\n        default_k_window_size = max(7, season_length) if season_length > 0 else 7\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': default_k_window_size}\\n\\n\\n    # processed_targets_np is guaranteed finite and on the correct scale.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 6. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level':\\n            # Base level already handled. Skip to avoid re-processing.\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 7. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 8. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 9. Apply Non-Negativity Constraint if configured ---\\n    if non_negative:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 666,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\nconfig_list = [\\n    # Config 0 (IMPROVED): Additive: Adaptive Median Base + Light Residual Correction.\\n    # More generally robust than last_value, adapting to recent median with a light residual dampening.\\n    {'name': 'additive_adaptive_median_rc_0', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 10}, # Very reactive base\\n        {'type': 'residual_correction', 'rc_window_size': 5, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.9} # Light, reactive correction\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros for clarity\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    # else: window_size remains train_len (full history)\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n        fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Constant trend applies globally to residuals\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Constant fallback applies globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        # Use relative indices for the segment for fitting, starting from 0.\\n        x_fit_segment_local = np.arange(len(y_fit_segment), dtype=float) # This goes from 0 to window_size - 1\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min_normalized_ref = x_fit_segment_local.min() # This will be 0\\n        x_max_normalized_ref = x_fit_segment_local.max() # This will be len(y_fit_segment) - 1\\n        x_range_normalized_ref = x_max_normalized_ref - x_min_normalized_ref\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range_normalized_ref == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Fallback to constant globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min_normalized_ref) / x_range_normalized_ref\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale.\\n            # These are indices relative to the start of the *segment used for fitting*.\\n            x_forecast_relative_to_segment_start = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length, dtype=float)\\n            x_forecast_normalized = (x_forecast_relative_to_segment_start - x_min_normalized_ref) / x_range_normalized_ref\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: Apply the fitted trend across the ENTIRE training history.\\n            # x-values for full training data, relative to the *start of the segment used for fitting*.\\n            x_full_train_relative_to_segment_start = np.arange(train_len, dtype=float) - fit_start_idx\\n            x_full_train_normalized = (x_full_train_relative_to_segment_start - x_min_normalized_ref) / x_range_normalized_ref\\n            fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val, dtype=float)\\n            fitted_trend_on_full_train = np.full(train_len, median_val, dtype=float) # Fallback to constant globally\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length, dtype=float), np.zeros(train_len, dtype=float)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val, dtype=float), np.full(train_len, base_val, dtype=float)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length, dtype=float), np.zeros(train_len, dtype=float)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# Helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\ndef _calculate_seasonal_median_optimized(attribute_values_train: np.ndarray, historical_data_train: np.ndarray,\\n                                         attribute_values_predict: np.ndarray, num_possible_values: int,\\n                                         fallback_value: float) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates median seasonal pattern for integer-indexed attributes using sorting for efficiency.\\n    \`historical_data_train\` is assumed to be finite.\\n    \\"\\"\\"\\n    seasonal_medians = np.full(num_possible_values, fallback_value, dtype=float)\\n\\n    if historical_data_train.size == 0:\\n        return np.full(len(attribute_values_predict), fallback_value, dtype=float), np.full(len(attribute_values_train), fallback_value, dtype=float)\\n    \\n    # Create pairs of (attribute, data) and sort by attribute\\n    paired_data = np.column_stack((attribute_values_train, historical_data_train))\\n    \\n    # Sort by the first column (attribute_values_train). Using 'mergesort' for stability.\\n    sorted_indices = paired_data[:, 0].argsort(kind='mergesort')\\n    sorted_paired_data = paired_data[sorted_indices]\\n\\n    # Find the unique attribute values and their start indices in the sorted array\\n    # np.diff tells us where the value changes. Add a start index (0) to mark the first group.\\n    unique_attrs_sorted = sorted_paired_data[:, 0]\\n    unique_indices = np.concatenate(([0], np.where(np.diff(unique_attrs_sorted) != 0)[0] + 1))\\n    \\n    # Iterate over unique attributes and calculate their medians\\n    for i, start_idx in enumerate(unique_indices):\\n        attr_val = int(sorted_paired_data[start_idx, 0])\\n        end_idx = unique_indices[i+1] if i + 1 < len(unique_indices) else len(sorted_paired_data)\\n        \\n        segment_data = sorted_paired_data[start_idx:end_idx, 1]\\n        \\n        # historical_data_train is already finite, so segment_data is finite.\\n        if len(segment_data) > 0:\\n            seasonal_medians[attr_val] = np.median(segment_data)\\n        # Else, it remains fallback_value\\n\\n    seasonal_medians = np.nan_to_num(seasonal_medians, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n    \\n    fitted_comp = seasonal_medians[attribute_values_train]\\n    forecast_comp = seasonal_medians[attribute_values_predict]\\n    \\n    return forecast_comp, fitted_comp\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This allows np.nanmedian to ignore empty slots in partial cycles.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN (e.g., if all padded data).\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    # Using .values for direct NumPy array access\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median_optimized(\\n            train_composite_dh, historical_data, pred_composite_dh, num_dh_values, fallback_value)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofweek, historical_data, pred_dayofweek, 7, fallback_value) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_hour, historical_data, pred_hour, 24, fallback_value) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_month - 1, historical_data, pred_month - 1, 12, fallback_value) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofyear - 1, historical_data, pred_dayofyear - 1, 366, fallback_value) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    \\n    if train_len == 0:\\n        return rc_forecast_component, np.zeros(train_len, dtype=float)\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, np.zeros(train_len, dtype=float)\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # For forecast, apply decay from the first prediction step onwards\\n    rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length, dtype=float))\\n\\n    # The fitted component should reflect the historical contribution of this correction.\\n    # It should reduce the historical residuals for subsequent components to model.\\n    fitted_rc_on_train = np.full(train_len, residual_correction_value, dtype=float) \\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Handle Edge Case: Empty or Very Short Processed Input (all NaNs)\\n    if train_len == 0:\\n        # Determine fallback value on the processed scale (original or log)\\n        if transform_log:\\n            # On log scale, 0.0 implies original scale 0 (expm1(0) = 0).\\n            fallback_val_processed_scale = 0.0\\n        else:\\n            fallback_val_processed_scale = initial_base_level_fallback_val\\n        \\n        # Create predictions on processed scale\\n        final_fallback_pred_processed_scale = np.full(prediction_length, fallback_val_processed_scale, dtype=float)\\n        \\n        # Transform back to original scale if log transformation was used\\n        if transform_log:\\n            final_fallback_pred = np.expm1(final_fallback_pred_processed_scale)\\n        else:\\n            final_fallback_pred = final_fallback_pred_processed_scale\\n        \\n        # Apply non-negativity constraint\\n        if non_negative:\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        \\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience to fill internal NaNs.\\n    # This creates a copy and handles NaNs in a standard way.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # If all values are still NaN after ffill/bfill (meaning input_targets was entirely NaN),\\n    # replace with the determined fallback value to prevent NaN propagation.\\n    if np.all(np.isnan(filled_targets_np)):\\n        filled_targets_np = np.full_like(filled_targets_np, initial_base_level_fallback_val, dtype=float)\\n\\n    # --- 3. Apply transformation and ensure processed_targets_np is finite ---\\n    if transform_log:\\n        # Clamp values to a small positive epsilon before log1p to avoid log1p(0) or log1p(negative).\\n        # Also, ensure result is finite by replacing posinf with a large but finite value on log scale.\\n        processed_targets_np = np.log1p(np.maximum(1e-9, filled_targets_np))\\n        # Handle inf values explicitly: replace posinf with a large float, neginf with 0.0 (already covered by max(1e-9, .))\\n        # A large but finite value on the log scale, e.g., representing expm1(100)\\n        max_log_val_finite = 100.0 # This translates to a very large number (e.g., ~2.68e43) on the original scale\\n        processed_targets_np = np.nan_to_num(processed_targets_np, nan=0.0, posinf=max_log_val_finite, neginf=0.0)\\n    else:\\n        # For non-log transformed data, just ensure it's finite using the original scale fallback.\\n        processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 4. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    \\n    # Use the last \`effective_season_length_for_naive\` values from processed_targets_np\\n    # If training data is shorter than season_length, use whatever available.\\n    if train_len >= effective_season_length_for_naive:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n    else:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np # Use all available data\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0: \\n        # Fallback for truly empty or invalid processed data (e.g., if processed_targets_np became empty somehow)\\n        base_seasonal_pattern_naive_processed_scale = np.array([0.0], dtype=float) # 0.0 on processed scale (log or original)\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    if non_negative:\\n        seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 5. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Extract base_level config first (it's often foundational)\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        # Default base_level if not explicitly defined in config components\\n        # Using a more adaptive default base_level_method: median_last_k_window\\n        default_k_window_size = max(7, season_length) if season_length > 0 else 7\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': default_k_window_size}\\n\\n\\n    # processed_targets_np is guaranteed finite and on the correct scale.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 6. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level':\\n            # Base level already handled. Skip to avoid re-processing.\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 7. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 8. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 9. Apply Non-Negativity Constraint if configured ---\\n    if non_negative:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

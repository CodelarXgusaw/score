<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 551.0,
  "old_code": "# TODO: Implement the \`fit_and_predict_fn\` to return a Pandas Series with a\\n#  DateTimeIndex and forecast predictions. Provide \`config_list\` to help your\\n#  solution to generalize effectively across diverse GiftEval datasets. Optimize\\n#  the seed solution to run faster and more efficiently while also incorperating\\n#  more complex modeling SOTA methods that generalize to different GiftEval\\n#  datasets. Your goal is to improve the score with fewer configs and a more\\n#  powerful \`fit_and_predict_fn\` solution.\\n\\nfrom typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\n# The config_list can be adjusted to explore more hyperparameter combinations.\\n# The current config_list is designed to offer a diverse set of strategies, including\\n# simple baselines, additive/multiplicative models, different trend behaviors,\\n# primary/secondary seasonality, and residual corrections. This diversity aims\\n# to generalize well across the various GiftEval datasets within the \`MAX_CONFIGS\` limit.\\n# The selection of 8 configurations provides necessary coverage for various time series\\n# patterns, from simple to complex, without redundant combinations.\\n\\n# Try to reduce the number of configs, prune unnecessary combinations, and\\n# up the diversity of the remaining configs. Do not overfit to validation data.\\nconfig_list = [\\n    # Config 0 (IMPROVED): Additive: Adaptive Median Base + Light Residual Correction.\\n    # More generally robust than last_value, adapting to recent median with a light residual dampening.\\n    {'name': 'additive_adaptive_median_rc_0', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 10}, # Very reactive base\\n        {'type': 'residual_correction', 'rc_window_size': 5, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.9} # Light, reactive correction\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Improved: Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros for clarity\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history when multiplier is None\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # Constant trend applies globally to residuals\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Constant fallback applies globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min = x_fit_segment_local.min()\\n        x_max = x_fit_segment_local.max()\\n        x_range = x_max - x_min\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Fallback to constant globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min) / x_range\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale\\n            x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n            x_forecast_normalized = (x_forecast_raw - x_min) / x_range\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: This is the improved part for local vs. global trend fitting for residuals.\\n            # Initialize to zeros.\\n            fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n            if trend_window_multiplier is not None:\\n                # If a window was used for fitting, apply the fitted trend only to that segment\\n                fitted_trend_on_full_train[fit_start_idx:] = trend_poly(x_fit_segment_local_normalized)\\n            else:\\n                # If full history was used (trend_window_multiplier is None), apply to entire training history\\n                x_full_train_global_indices = np.arange(train_len)\\n                x_full_train_normalized = (x_full_train_global_indices - x_min) / x_range\\n                fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Fallback to constant globally\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value * (residual_correction_damping_factor ** np.arange(rc_fit_window))[::-1]\\n    else:\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience to fill internal NaNs.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine fallback value on the processed scale (original or log)\\n        if transform_log:\\n            # On log scale, 0.0 implies original scale 0 (expm1(0) = 0).\\n            fallback_val_processed_scale = 0.0\\n        else:\\n            fallback_val_processed_scale = initial_base_level_fallback_val\\n        \\n        # Create predictions on processed scale\\n        final_fallback_pred_processed_scale = np.full(prediction_length, fallback_val_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was used\\n        if transform_log:\\n            final_fallback_pred = np.expm1(final_fallback_pred_processed_scale)\\n        else:\\n            final_fallback_pred = final_fallback_pred_processed_scale\\n        \\n        # Apply non-negativity constraint\\n        if non_negative:\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        \\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n    # --- 3. Apply transformation and ensure processed_targets_np is finite ---\\n    if transform_log:\\n        # Clamp values to a small positive epsilon before log1p to avoid log1p(0) or log1p(negative) leading to -inf.\\n        processed_targets_np = np.log1p(np.maximum(1e-9, filled_targets_np))\\n        # Ensure log-transformed data is finite. Replace NaNs/Infs (which can arise from e.g., original NaNs that were filled with 0, then log1p(0)) with 0.0 on log scale.\\n        processed_targets_np = np.nan_to_num(processed_targets_np, nan=0.0, posinf=0.0, neginf=0.0)\\n    else:\\n        # For non-log transformed data, just ensure it's finite using the original scale fallback.\\n        processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 4. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    \\n    # Use the last \`effective_season_length_for_naive\` values from processed_targets_np\\n    # If training data is shorter than season_length, use whatever available.\\n    if train_len >= effective_season_length_for_naive:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n    else:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np # Use all available data\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0: # Should not happen due to train_len == 0 check\\n        # Fallback for truly empty or invalid processed data (e.g., if processed_targets_np became empty somehow)\\n        base_seasonal_pattern_naive_processed_scale = np.array([0.0]) # 0.0 on processed scale (log or original)\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    if non_negative:\\n        seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 5. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite and on the correct scale.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 6. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level':\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 7. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 8. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 9. Apply Non-Negativity Constraint if configured ---\\n    if non_negative:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 569,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\nconfig_list = [\\n    # Config 0 (IMPROVED): Additive: Adaptive Median Base + Light Residual Correction.\\n    # More generally robust than last_value, adapting to recent median with a light residual dampening.\\n    {'name': 'additive_adaptive_median_rc_0', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 10}, # Very reactive base\\n        {'type': 'residual_correction', 'rc_window_size': 5, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.9} # Light, reactive correction\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Uses x-coordinate normalization for polyfit for numerical stability.\\n    Ensures \`historical_residuals\` is finite before operations within this function.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros for clarity\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else:\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history when multiplier is None\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # Constant trend applies globally to residuals\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Constant fallback applies globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Numerical stability: Normalize x-coordinates to [0, 1] range for polyfit\\n        x_min = x_fit_segment_local.min()\\n        x_max = x_fit_segment_local.max()\\n        x_range = x_max - x_min\\n\\n        # Fallback to constant median if x_range is zero (e.g., single point) or not enough points for degree\\n        if x_range == 0 or len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Fallback to constant globally\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        x_fit_segment_local_normalized = (x_fit_segment_local - x_min) / x_range\\n\\n        try:\\n            poly_coeffs = np.polyfit(x_fit_segment_local_normalized, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be transformed to the same normalized scale\\n            x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n            x_forecast_normalized = (x_forecast_raw - x_min) / x_range\\n            trend_forecast_component = trend_poly(x_forecast_normalized)\\n\\n            # Fitted: This is the improved part for local vs. global trend fitting for residuals.\\n            fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n            if trend_window_multiplier is not None:\\n                # If a window was used for fitting, apply the fitted trend only to that segment\\n                fitted_trend_on_full_train[fit_start_idx:] = trend_poly(x_fit_segment_local_normalized)\\n            else:\\n                # If full history was used (trend_window_multiplier is None), apply to entire training history\\n                x_full_train_global_indices = np.arange(train_len)\\n                x_full_train_normalized = (x_full_train_global_indices - x_min) / x_range\\n                fitted_trend_on_full_train = trend_poly(x_full_train_normalized)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Fallback to constant globally\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# Helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\ndef _calculate_seasonal_median_optimized(attribute_values_train: np.ndarray, historical_data_train: np.ndarray,\\n                                         attribute_values_predict: np.ndarray, num_possible_values: int,\\n                                         fallback_value: float) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates median seasonal pattern for integer-indexed attributes using sorting for efficiency.\\n    \`historical_data_train\` is assumed to be finite.\\n    \\"\\"\\"\\n    seasonal_medians = np.full(num_possible_values, fallback_value, dtype=float)\\n\\n    if historical_data_train.size == 0:\\n        return np.full(len(attribute_values_predict), fallback_value), np.full(len(attribute_values_train), fallback_value)\\n    \\n    # Create pairs of (attribute, data) and sort by attribute\\n    paired_data = np.column_stack((attribute_values_train, historical_data_train))\\n    \\n    # Sort by the first column (attribute_values_train). Using 'mergesort' for stability.\\n    sorted_indices = paired_data[:, 0].argsort(kind='mergesort')\\n    sorted_paired_data = paired_data[sorted_indices]\\n\\n    # Find the unique attribute values and their start indices in the sorted array\\n    # np.diff tells us where the value changes. Add a start index (0) to mark the first group.\\n    unique_attrs_sorted = sorted_paired_data[:, 0]\\n    unique_indices = np.concatenate(([0], np.where(np.diff(unique_attrs_sorted) != 0)[0] + 1))\\n    \\n    # Iterate over unique attributes and calculate their medians\\n    for i, start_idx in enumerate(unique_indices):\\n        attr_val = int(sorted_paired_data[start_idx, 0])\\n        end_idx = unique_indices[i+1] if i + 1 < len(unique_indices) else len(sorted_paired_data)\\n        \\n        segment_data = sorted_paired_data[start_idx:end_idx, 1]\\n        \\n        # historical_data_train is already finite, so segment_data is finite.\\n        if len(segment_data) > 0:\\n            seasonal_medians[attr_val] = np.median(segment_data)\\n        # Else, it remains fallback_value\\n\\n    seasonal_medians = np.nan_to_num(seasonal_medians, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n    \\n    fitted_comp = seasonal_medians[attribute_values_train]\\n    forecast_comp = seasonal_medians[attribute_values_predict]\\n    \\n    return forecast_comp, fitted_comp\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This allows np.nanmedian to ignore empty slots in partial cycles.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN (e.g., if all padded data).\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    # Using .values for direct NumPy array access\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median_optimized(\\n            train_composite_dh, historical_data, pred_composite_dh, num_dh_values, fallback_value)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofweek, historical_data, pred_dayofweek, 7, fallback_value) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_hour, historical_data, pred_hour, 24, fallback_value) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_month - 1, historical_data, pred_month - 1, 12, fallback_value) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median_optimized(\\n            train_dayofyear - 1, historical_data, pred_dayofyear - 1, 366, fallback_value) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0: # Should be caught by max(1, ...) above, but as a safeguard.\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        # Apply decay to fitted component for historical residuals, more recent gets less decay\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value * (residual_correction_damping_factor ** np.arange(rc_fit_window))[::-1]\\n    else:\\n        # If no decay, apply constant value to the window used for fitting\\n        fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    if residual_correction_decay_enabled:\\n        # For forecast, apply decay from the first prediction step onwards\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        # If no decay, apply constant value to the forecast horizon\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience to fill internal NaNs.\\n    # This creates a copy and handles NaNs in a standard way.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine fallback value on the processed scale (original or log)\\n        if transform_log:\\n            # On log scale, 0.0 implies original scale 0 (expm1(0) = 0).\\n            fallback_val_processed_scale = 0.0\\n        else:\\n            fallback_val_processed_scale = initial_base_level_fallback_val\\n        \\n        # Create predictions on processed scale\\n        final_fallback_pred_processed_scale = np.full(prediction_length, fallback_val_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was used\\n        if transform_log:\\n            final_fallback_pred = np.expm1(final_fallback_pred_processed_scale)\\n        else:\\n            final_fallback_pred = final_fallback_pred_processed_scale\\n        \\n        # Apply non-negativity constraint\\n        if non_negative:\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        \\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n    # --- 3. Apply transformation and ensure processed_targets_np is finite ---\\n    if transform_log:\\n        # Clamp values to a small positive epsilon before log1p to avoid log1p(0) or log1p(negative) leading to -inf.\\n        processed_targets_np = np.log1p(np.maximum(1e-9, filled_targets_np))\\n        # Ensure log-transformed data is finite. Replace NaNs/Infs with 0.0 on log scale.\\n        processed_targets_np = np.nan_to_num(processed_targets_np, nan=0.0, posinf=0.0, neginf=0.0)\\n    else:\\n        # For non-log transformed data, just ensure it's finite using the original scale fallback.\\n        processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 4. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    \\n    # Use the last \`effective_season_length_for_naive\` values from processed_targets_np\\n    # If training data is shorter than season_length, use whatever available.\\n    if train_len >= effective_season_length_for_naive:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n    else:\\n        base_seasonal_pattern_naive_processed_scale = processed_targets_np # Use all available data\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0: \\n        # Fallback for truly empty or invalid processed data (e.g., if processed_targets_np became empty somehow)\\n        base_seasonal_pattern_naive_processed_scale = np.array([0.0]) # 0.0 on processed scale (log or original)\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    if non_negative:\\n        seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 5. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Extract base_level config first (it's often foundational)\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        # Default base_level if not explicitly defined in config components\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite and on the correct scale.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 6. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level':\\n            # Base level already handled. Skip to avoid re-processing.\\n            continue\\n\\n        if component_type in component_functions:\\n            # Subsequent components train on current_residuals (which are guaranteed finite).\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # This input is guaranteed finite\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 7. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 8. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 9. Apply Non-Negativity Constraint if configured ---\\n    if non_negative:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

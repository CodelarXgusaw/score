<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 885.0,
  "old_code": "# The solution aims to improve accuracy and generalize effectively across diverse GiftEval datasets\\n# by refining the fit_and_predict_fn and its configurations.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 9 # Incremented version to reflect latest review and validation\\n\\n# The config_list offers a diverse set of forecasting strategies, balancing between\\n# robustness, adaptability, and the ability to capture complex patterns (trend, seasonality, residuals).\\n# The goal is to maximize performance across GiftEval datasets within the \`MAX_CONFIGS\` limit of 8,\\n# by ensuring each configuration is distinct and powerful.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Simple and ultra-robust, suitable for highly erratic or very short series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal. Effective for exponential growth/decay with strong primary seasonality.\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2: Additive: More Reactive Linear Trend + Primary Seasonal + Median RC. For moderately fast-changing patterns with clear primary seasonality.\\n    {'name': 'additive_more_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.99} # Increased damping factor from 0.98 to 0.99 for more persistence\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC. Ideal for high-frequency data where complex daily/weekly patterns scale.\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.99} # Reduced damping factor from 1.0 to 0.99 for slight decay\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4: Additive: Adaptive Stable Base + Long Primary Seasonal Only. For very stable, strong seasonal patterns with minimal trend/noise.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # Adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal. Robust for datasets with consistent, global linear changes.\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC. For log-transformed series with a stable level, but seasonality/residuals are key.\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98} # Increased damping factor from 0.95 to 0.98 for more persistence\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC. Robust for complex datasets with multiple seasonalities and a general trend.\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90} # Increased damping factor from 0.85 to 0.90 for less aggressive decay\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int, # Used for trend window multiplier, can be 1 for low freq\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Trend is projected over the entire training history for consistent residual calculation.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0: # Use provided season_length for dynamic windowing if available\\n            window_size_base = int(multiplier * season_length)\\n        else: # Fallback if season_length is 0 or less\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n    # X-coordinates for fitting are the actual absolute indices within the training data\\n    x_fit_segment_raw = np.arange(fit_start_idx, train_len)\\n\\n    if len(y_fit_segment) < 1 or len(x_fit_segment_raw) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        # Use np.isclose for floating-point robustness\\n        if np.all(np.isclose(y_fit_segment, y_fit_segment[0], atol=1e-9)):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree) # Ensure degree is non-negative\\n\\n        # Fallback to constant median if not enough points for polynomial degree\\n        if len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        try:\\n            # Center x-values around their mean to improve numerical stability of polyfit\\n            x_mean = np.mean(x_fit_segment_raw)\\n            x_fit_segment_centered = x_fit_segment_raw - x_mean\\n\\n            # Fit polynomial using the centered x-coordinates\\n            poly_coeffs = np.polyfit(x_fit_segment_centered, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be absolute indices extending from train_len, then centered\\n            x_forecast_raw = np.arange(train_len, train_len + prediction_length)\\n            x_forecast_centered = x_forecast_raw - x_mean\\n            trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n            # Fitted: For the entire training history, evaluate the trend polynomial for each point\\n            # using their absolute indices, then centered. This ensures consistent detrending.\\n            x_full_train_raw = np.arange(train_len)\\n            x_full_train_centered = x_full_train_raw - x_mean\\n            fitted_trend_on_full_train = trend_poly(x_full_train_centered)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \`season_length\` is used for 'median_last_season' and default 'k_window_size'.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7 # Use season_length for default\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\n    \`season_length\` is passed through to \`_calculate_trend_and_fitted\` for windowing.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length, # Pass original season_length for trend window context\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \`season_length\` here is the *effective* one determined by fit_and_predict_fn.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length) # Ensure season_length is at least 1\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This ensures that np.nanmedian can operate on full columns, even if the last cycle is incomplete.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        # Only compute seasonal pattern if at least one full cycle is available\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component\\n    if do_month_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component\\n    if do_doy_config and \\\\\\n       is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # Fitted: The amount of correction that would have been applied to the training window.\\n    # For simplicity and consistency with the \\"average residual\\" idea, apply the constant\\n    # correction value to the window from which it was derived. This represents what was \\"removed\\"\\n    # from these residuals.\\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    # Forecast: Apply damping factor for forecast, decaying with prediction horizon.\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        # If no decay, apply constant value without any damping factor\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative_constraint = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine the fallback value on the processed scale\\n        fallback_val_on_processed_scale = initial_base_level_fallback_val\\n        if transform_log:\\n            # If log-transform and non-negative, the lowest log-value for a non-negative original is log1p(0) = 0.\\n            if non_negative_constraint:\\n                fallback_val_on_processed_scale = 0.0\\n            else:\\n                # If not non-negative, can use log1p directly, but ensure initial_base_level_fallback_val is not too small\\n                fallback_val_on_processed_scale = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n\\n        # Create predictions on the processed scale\\n        predictions_on_processed_scale = np.full(prediction_length, fallback_val_on_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was applied\\n        if transform_log:\\n            predictions = np.expm1(predictions_on_processed_scale)\\n        else:\\n            predictions = predictions_on_processed_scale\\n\\n        # Apply non-negativity constraint\\n        if non_negative_constraint:\\n            predictions = np.maximum(0, predictions)\\n            \\n        return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Re-evaluate fallback on processed (potentially log) scale for this path.\\n        if transform_log:\\n            if non_negative_constraint:\\n                fallback_val_for_naive = 0.0\\n            else:\\n                fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length, # Pass original season_length to base_level\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level': # Base level already handled\\n            continue\\n\\n        # Determine the effective season length for this specific component\\n        effective_comp_season_length = season_length # Start with the provided season_length from GluonTS\\n\\n        # Special handling for seasonal_primary to infer a more relevant season_length if GluonTS defaults to 1\\n        if component_type == 'seasonal_primary' and effective_comp_season_length <= 1:\\n            freq_str = input_targets.index.freqstr if input_targets.index.freqstr is not None else ''\\n            \\n            # Calculate data span in years for yearly seasonality checks, consistent with secondary seasonality\\n            data_span_days = (input_targets.index.max() - input_targets.index.min()).days if train_len > 1 else 0\\n            min_years_for_yearly_seasonality = 2.0\\n            \\n            # Prioritize weekly seasonality for daily data if sufficient history\\n            if 'D' in freq_str and train_len >= 2 * 7: # At least two full weeks of data\\n                effective_comp_season_length = 7\\n            # Prioritize yearly seasonality for monthly data if sufficient history\\n            elif 'M' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 12\\n            # Prioritize yearly seasonality for weekly data if sufficient history\\n            elif 'W' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 52\\n            # Add yearly seasonality for daily if not already picked up by weekly and sufficient history\\n            elif 'D' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 365\\n            \\n            # If after all checks, effective_comp_season_length is still <= 1,\\n            # then no meaningful primary season was inferred, and the component will return zeros.\\n\\n\\n        if component_type in component_functions:\\n            func_args = {\\n                'historical_data': current_residuals, # This input is guaranteed finite\\n                'prediction_length': prediction_length,\\n                'train_len': train_len,\\n                'component_config': component_config,\\n            }\\n            if component_type == 'seasonal_primary': # Pass the derived value\\n                func_args['season_length'] = effective_comp_season_length\\n            elif component_type == 'seasonal_secondary':\\n                func_args['input_targets_index'] = input_targets.index\\n                func_args['prediction_index'] = prediction_index\\n            elif component_type == 'trend': # Trend uses original season_length for windowing\\n                func_args['season_length'] = season_length\\n            # Base_level and residual_correction do not need \`season_length\` from here,\\n            # as base_level already received it during initialization and residual_correction doesn't use it.\\n            \\n            component_forecast, component_fitted_on_train = component_functions[component_type](**func_args)\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        # Ensure non-negativity on the log-transformed scale if the original output should be non-negative.\\n        # np.expm1(x) is >= 0 if and only if x >= 0.\\n        if non_negative_constraint:\\n            predictions_on_processed_scale = np.maximum(0, predictions_on_processed_scale)\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    # This acts as a final safeguard, even if non_negative_constraint was applied on log scale.\\n    if non_negative_constraint:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 910,
  "new_code": "# TODO: Implement the \`fit_and_predict_fn\` to return a Pandas Series with a\\n#  DateTimeIndex and forecast predictions. Provide \`config_list\` to help your\\n#  solution to generalize effectively across diverse GiftEval datasets. Optimize\\n#  the seed solution to run faster and more efficiently while also incorperating\\n#  more complex modeling SOTA methods that generalize to different GiftEval\\n#  datasets. Your goal is to improve the score with fewer configs and a more\\n#  powerful \`fit_and_predict_fn\` solution.\\n\\n# The solution aims to improve accuracy and generalize effectively across diverse GiftEval datasets\\n# by refining the fit_and_predict_fn and its configurations.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 10 # Incremented version to reflect latest review and validation\\n\\n# The config_list offers a diverse set of forecasting strategies, balancing between\\n# robustness, adaptability, and the ability to capture complex patterns (trend, seasonality, residuals).\\n# The goal is to maximize performance across GiftEval datasets within the \`MAX_CONFIGS\` limit of 8,\\n# by ensuring each configuration is distinct and powerful.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Simple and ultra-robust, suitable for highly erratic or very short series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Constant Trend + Primary Seasonal. Effective for exponential growth/decay with strong primary seasonality.\\n    # Modified from linear trend to constant trend to improve stability for log-transformed data.\\n    {'name': 'multiplicative_constant_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None}, # Changed to constant, full history\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2: Additive: More Reactive Linear Trend + Primary Seasonal + Median RC. For moderately fast-changing patterns with clear primary seasonality.\\n    {'name': 'additive_more_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.99} # Increased damping factor from 0.98 to 0.99 for more persistence\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC. Ideal for high-frequency data where complex daily/weekly patterns scale.\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.99} # Reduced damping factor from 1.0 to 0.99 for slight decay\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4: Additive: Adaptive Stable Base + Long Primary Seasonal Only. For very stable, strong seasonal patterns with minimal trend/noise.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # Adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal. Robust for datasets with consistent, global linear changes.\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC. For log-transformed series with a stable level, but seasonality/residuals are key.\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98} # Increased damping factor from 0.95 to 0.98 for more persistence\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC. Robust for complex datasets with multiple seasonalities and a general trend.\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30}, # More reactive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90} # Increased damping factor from 0.85 to 0.90 for less aggressive decay\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int, # Used for trend window multiplier, can be 1 for low freq\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Trend is projected over the entire training history for consistent residual calculation.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        # If season_length is very small or zero, use a default sensible base (e.g., 30 for monthly-like, 7 for weekly-like)\\n        # This prevents window_size_base from being too small (e.g. multiplier * 1) when season_length is 1.\\n        if season_length is None or season_length <= 1:\\n            base_for_multiplier = 30 # A general number of points to represent a reasonable window\\n            window_size_base = int(multiplier * base_for_multiplier)\\n        else:\\n            window_size_base = int(multiplier * season_length)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else:\\n        window_size = train_len # Use full history\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n    # X-coordinates for fitting are the actual absolute indices within the training data\\n    x_fit_segment_raw = np.arange(fit_start_idx, train_len)\\n\\n    if len(y_fit_segment) < 1 or len(x_fit_segment_raw) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        # Use np.isclose for floating-point robustness\\n        if np.all(np.isclose(y_fit_segment, y_fit_segment[0], atol=1e-9)):\\n            median_val = y_fit_segment[0]\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree) # Ensure degree is non-negative\\n\\n        # Fallback to constant median if not enough points for polynomial degree\\n        if len(y_fit_segment) <= effective_degree:\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train\\n\\n        try:\\n            # Center x-values around their mean to improve numerical stability of polyfit\\n            x_mean = np.mean(x_fit_segment_raw)\\n            x_fit_segment_centered = x_fit_segment_raw - x_mean\\n\\n            # Fit polynomial using the centered x-coordinates\\n            poly_coeffs = np.polyfit(x_fit_segment_centered, y_fit_segment, effective_degree)\\n            trend_poly = np.poly1d(poly_coeffs)\\n\\n            # Forecast: x-values for prediction need to be absolute indices extending from train_len, then centered\\n            x_forecast_raw = np.arange(train_len, train_len + prediction_length)\\n            x_forecast_centered = x_forecast_raw - x_mean\\n            trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n            # Fitted: For the entire training history, evaluate the trend polynomial for each point\\n            # using their absolute indices, then centered. This ensures consistent detrending.\\n            x_full_train_raw = np.arange(train_len)\\n            x_full_train_centered = x_full_train_raw - x_mean\\n            fitted_trend_on_full_train = trend_poly(x_full_train_centered)\\n\\n        except np.linalg.LinAlgError:\\n            # Fallback to constant median if polyfit fails (e.g., singular matrix)\\n            median_val = np.median(y_fit_segment)\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val) # Applies uniformly\\n    \\n    # Ensure outputs are finite before returning as a final safeguard against extreme values from polyfit.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` is expected to be finite (e.g., \`processed_targets_np\` or its log-transformed version).\\n    \`season_length\` is used for 'median_last_season' and default 'k_window_size'.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7 # Use season_length for default\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n        k_window_size_val = max(1, min(k_window_size_val, train_len)) # Ensure valid window size\\n        base_val = np.median(historical_data[-k_window_size_val:])\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted. historical_data is guaranteed finite.\\n    \`season_length\` is passed through to \`_calculate_trend_and_fitted\` for windowing.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This input is guaranteed finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length, # Pass original season_length for trend window context\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is expected to be finite.\\n    \`season_length\` here is the *effective* one determined by fit_and_predict_fn.\\n    \\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length) # Ensure season_length is at least 1\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles. Padding at the beginning.\\n        # This ensures that np.nanmedian can operate on full columns, even if the last cycle is incomplete.\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        # Only compute seasonal pattern if at least one full cycle is available\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0) # Use np.nanmedian to handle NaNs from padding\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is expected to be finite.\\n    Optimized to use pure NumPy for median calculations, avoiding Pandas groupby overhead.\\n    Includes logic for DayOfWeek-HourOfDay interaction.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0 # Sensible fallback for additive components\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    # is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str) # No longer needed for yearly seasonality restriction\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Get datetime attributes for training and prediction indices\\n    train_dayofweek = input_targets_index.dayofweek.values\\n    pred_dayofweek = prediction_index.dayofweek.values\\n    train_hour = input_targets_index.hour.values\\n    pred_hour = prediction_index.hour.values\\n    train_month = input_targets_index.month.values\\n    pred_month = prediction_index.month.values\\n    train_dayofyear = input_targets_index.dayofyear.values\\n    pred_dayofyear = prediction_index.dayofyear.values\\n\\n    # Helper to calculate median for a given attribute\\n    def _calculate_seasonal_median(attribute_values_train: np.ndarray, attribute_values_predict: np.ndarray, num_possible_values: int) -> Tuple[np.ndarray, np.ndarray]:\\n        mapping_array = np.full(num_possible_values, fallback_value, dtype=float)\\n        \\n        # Identify unique attribute values present in training data\\n        unique_attrs_train = np.unique(attribute_values_train)\\n\\n        for attr_val in unique_attrs_train:\\n            mask = (attribute_values_train == attr_val)\\n            # Ensure there's data for this attribute value and it's finite\\n            valid_residuals = historical_data[mask]\\n            if len(valid_residuals) > 0 and np.any(np.isfinite(valid_residuals)):\\n                mapping_array[attr_val] = np.median(valid_residuals)\\n        \\n        # Ensure mapping array is finite\\n        mapping_array = np.nan_to_num(mapping_array, nan=fallback_value, posinf=fallback_value, neginf=fallback_value)\\n        \\n        # Apply mapping to training and prediction sets\\n        fitted_comp = mapping_array[attribute_values_train]\\n        forecast_comp = mapping_array[attribute_values_predict]\\n        \\n        return forecast_comp, fitted_comp\\n\\n    # Determine which components are requested by the config\\n    do_dow_config = component_config.get('use_dayofweek', False)\\n    do_hod_config = component_config.get('use_hourofday', False)\\n    do_month_config = component_config.get('use_month_of_year', False)\\n    do_doy_config = component_config.get('use_day_of_year', False)\\n\\n    # Handle DayOfWeek-HourOfDay interaction if both are true and frequency is suitable\\n    if do_dow_config and do_hod_config and is_hourly_or_subhourly:\\n        # Create composite feature (0-167 for DayOfWeek*24 + Hour)\\n        train_composite_dh = train_dayofweek * 24 + train_hour\\n        pred_composite_dh = pred_dayofweek * 24 + pred_hour\\n        num_dh_values = 7 * 24 # Total possible combinations\\n\\n        forecast_comp_dh, fitted_comp_dh = _calculate_seasonal_median(train_composite_dh, pred_composite_dh, num_dh_values)\\n        secondary_seasonal_forecast_component += forecast_comp_dh\\n        fitted_secondary_seasonal_on_train += fitted_comp_dh\\n\\n        # Disable individual DOW and HOD so they are not added again\\n        do_dow_config = False\\n        do_hod_config = False\\n\\n    # Add individual Day of Week component if not covered by interaction\\n    if do_dow_config and (is_hourly_or_subhourly or _dataset_has_daily_or_coarser_freq(freq_str)): # Re-add daily/coarser for DOW\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofweek, pred_dayofweek, 7) # 0-6 for dayofweek\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add individual Hour of Day component if not covered by interaction\\n    if do_hod_config and is_hourly_or_subhourly:\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_hour, pred_hour, 24) # 0-23 for hour\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Month of Year component (removed is_daily_or_coarser restriction)\\n    if do_month_config and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_month - 1, pred_month - 1, 12) # Months 1-12, map to 0-11\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    # Add Day of Year component (removed is_daily_or_coarser restriction)\\n    if do_doy_config and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        forecast_comp, fitted_comp = _calculate_seasonal_median(train_dayofyear - 1, pred_dayofyear - 1, 366) # Dayofyear 1-366, map to 0-365\\n        secondary_seasonal_forecast_component += forecast_comp\\n        fitted_secondary_seasonal_on_train += fitted_comp\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any]\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates and applies a residual correction component.\\n    \`historical_data\` is expected to be finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    # Fitted: The amount of correction that would have been applied to the training window.\\n    # For simplicity and consistency with the \\"average residual\\" idea, apply the constant\\n    # correction value to the window from which it was derived. This represents what was \\"removed\\"\\n    # from these residuals.\\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    # Forecast: Apply damping factor for forecast, decaying with prediction horizon.\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        # If no decay, apply constant value without any damping factor\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n    non_negative_constraint = config.get('non_negative', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Determine a robust fallback value from the original, finite data points.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Determine the fallback value on the processed scale\\n        fallback_val_on_processed_scale = initial_base_level_fallback_val\\n        if transform_log:\\n            # If log-transform and non-negative, the lowest log-value for a non-negative original is log1p(0) = 0.\\n            if non_negative_constraint:\\n                fallback_val_on_processed_scale = 0.0\\n            else:\\n                # If not non-negative, can use log1p directly, but ensure initial_base_level_fallback_val is not too small\\n                fallback_val_on_processed_scale = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n\\n        # Create predictions on the processed scale\\n        predictions_on_processed_scale = np.full(prediction_length, fallback_val_on_processed_scale)\\n        \\n        # Transform back to original scale if log transformation was applied\\n        if transform_log:\\n            predictions = np.expm1(predictions_on_processed_scale)\\n        else:\\n            predictions = predictions_on_processed_scale\\n\\n        # Apply non-negativity constraint\\n        if non_negative_constraint:\\n            predictions = np.maximum(0, predictions)\\n            \\n        return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Re-evaluate fallback on processed (potentially log) scale for this path.\\n        if transform_log:\\n            if non_negative_constraint:\\n                fallback_val_for_naive = 0.0\\n            else:\\n                fallback_val_for_naive = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n        else:\\n            fallback_val_for_naive = initial_base_level_fallback_val\\n        base_seasonal_pattern_naive_processed_scale = np.array([fallback_val_for_naive])\\n\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config is None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length, # Pass original season_length to base_level\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite as processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        if component_type == 'base_level': # Base level already handled\\n            continue\\n\\n        # Determine the effective season length for this specific component\\n        effective_comp_season_length = season_length # Start with the provided season_length from GluonTS\\n\\n        # Special handling for seasonal_primary to infer a more relevant season_length if GluonTS defaults to 1\\n        if component_type == 'seasonal_primary' and effective_comp_season_length <= 1:\\n            freq_str = input_targets.index.freqstr if input_targets.index.freqstr is not None else ''\\n            \\n            # Calculate data span in years for yearly seasonality checks, consistent with secondary seasonality\\n            data_span_days = (input_targets.index.max() - input_targets.index.min()).days if train_len > 1 else 0\\n            min_years_for_yearly_seasonality = 2.0\\n            \\n            # Prioritize weekly seasonality for daily data if sufficient history\\n            if 'D' in freq_str and train_len >= 2 * 7: # At least two full weeks of data\\n                effective_comp_season_length = 7\\n            # Prioritize yearly seasonality for monthly data if sufficient history\\n            elif 'M' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 12\\n            # Prioritize yearly seasonality for weekly data if sufficient history\\n            elif 'W' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 52\\n            # Add yearly seasonality for daily if not already picked up by weekly and sufficient history\\n            elif 'D' in freq_str and data_span_days >= (365 * min_years_for_yearly_seasonality):\\n                effective_comp_season_length = 365\\n            \\n            # If after all checks, effective_comp_season_length is still <= 1,\\n            # then no meaningful primary season was inferred, and the component will return zeros.\\n\\n\\n        if component_type in component_functions:\\n            func_args = {\\n                'historical_data': current_residuals, # This input is guaranteed finite\\n                'prediction_length': prediction_length,\\n                'train_len': train_len,\\n                'component_config': component_config,\\n            }\\n            if component_type == 'seasonal_primary': # Pass the derived value\\n                func_args['season_length'] = effective_comp_season_length\\n            elif component_type == 'seasonal_secondary':\\n                func_args['input_targets_index'] = input_targets.index\\n                func_args['prediction_index'] = prediction_index\\n            elif component_type == 'trend': # Trend uses original season_length for windowing\\n                func_args['season_length'] = season_length\\n            # Base_level and residual_correction do not need \`season_length\` from here,\\n            # as base_level already received it during initialization and residual_correction doesn't use it.\\n            \\n            component_forecast, component_fitted_on_train = component_functions[component_type](**func_args)\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        # Ensure non-negativity on the log-transformed scale if the original output should be non-negative.\\n        # np.expm1(x) is >= 0 if and only if x >= 0.\\n        if non_negative_constraint:\\n            predictions_on_processed_scale = np.maximum(0, predictions_on_processed_scale)\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # Fall back to seasonal naive if predictions contain any NaNs or Infs\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    # This acts as a final safeguard, even if non_negative_constraint was applied on log scale.\\n    if non_negative_constraint:\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

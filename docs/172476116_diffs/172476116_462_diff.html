<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 443.0,
  "old_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\n# The config_list can be adjusted to explore more hyperparameter combinations.\\n# The current config_list is designed to offer a diverse set of strategies, including\\n# simple baselines, additive/multiplicative models, different trend behaviors,\\n# primary/secondary seasonality, and residual corrections. This diversity aims\\n# to generalize well across the various GiftEval datasets within the \`MAX_CONFIGS\` limit.\\n# The selection of 8 configurations provides necessary coverage for various time series\\n# patterns, from simple to complex, without redundant combinations.\\n\\n# Try to reduce the number of configs, prune unnecessary combinations, and\\n# up the diversity of the remaining configs. Do not overfit to validation data.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Ultra-robust for any series. Good for very short or highly erratic series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (IMPROVED BASE LEVEL): Additive: More Adaptive Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    # Changed 'median_all_history' to 'median_last_k_window' with a large k_window_size (200) for better adaptability while maintaining stability.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray, # Guaranteed finite by fit_and_predict_fn\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to median if trend cannot be reliably estimated.\\n    Centers x-coordinates for polyfit for numerical stability.\\n    Ensures fitted_trend_on_full_train is fully populated when windowing is active.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else: # season_length is 0, no dominant primary seasonality, interpret multiplier as absolute points\\n            window_size_base = int(multiplier)\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else: # trend_window_multiplier is None, use full history\\n        window_size = train_len\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if len(y_fit_segment) > 0 and np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0] # Use the constant value\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train # Early exit for constant case\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        if len(y_fit_segment) > effective_degree:\\n            try:\\n                x_mean = np.mean(x_fit_segment_local)\\n                x_fit_segment_local_centered = x_fit_segment_local - x_mean\\n\\n                poly_coeffs = np.polyfit(x_fit_segment_local_centered, y_fit_segment, effective_degree)\\n                trend_poly = np.poly1d(poly_coeffs)\\n\\n                x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n                x_forecast_centered = x_forecast_raw - x_mean\\n                trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n                x_full_train_global_indices = np.arange(train_len)\\n                x_full_train_centered_for_trend_poly = (x_full_train_global_indices - fit_start_idx) - x_mean\\n                fitted_trend_on_full_train = trend_poly(x_full_train_centered_for_trend_poly)\\n\\n            except np.linalg.LinAlgError:\\n                # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n                median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n                trend_forecast_component = np.full(prediction_length, median_val)\\n                fitted_trend_on_full_train = np.full(train_len, median_val)\\n        else: # Not enough points for desired degree, fall back to median\\n            median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n\\n    # All outputs are guaranteed finite by the above logic or fallbacks.\\n    return trend_forecast_component, fitted_trend_on_full_train\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` here is expected to be the raw \`processed_targets_np\` (or log-transformed version),\\n    and is guaranteed to be finite by the main \`fit_and_predict_fn\`.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback for calculation\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n\\n        k_window_size_val = max(1, min(k_window_size_val, train_len))\\n\\n        if k_window_size_val > 0:\\n            base_val = np.median(historical_data[-k_window_size_val:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0)\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    actual_use_dayofweek_seasonal = component_config.get('use_dayofweek', False) and (is_hourly_or_subhourly or is_daily_or_coarser)\\n    actual_use_hourofday_seasonal = component_config.get('use_hourofday', False) and is_hourly_or_subhourly\\n\\n    min_years_for_yearly_seasonality = 2.0\\n    actual_use_month_of_year_seasonal = component_config.get('use_month_of_year', False) and \\\\\\n                                        is_daily_or_coarser and \\\\\\n                                        data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    actual_use_day_of_year_seasonal = component_config.get('use_day_of_year', False) and \\\\\\n                                      is_daily_or_coarser and \\\\\\n                                      data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    residuals_series_for_groupby = pd.Series(historical_data, index=input_targets_index)\\n\\n    if actual_use_dayofweek_seasonal:\\n        day_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofweek).median()\\n        pred_dayofweek = prediction_index.dayofweek.values\\n        train_dayofweek = input_targets_index.dayofweek.values\\n\\n        projected_day_forecast = day_to_median_map_series.reindex(pred_dayofweek, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_day_forecast\\n        fitted_secondary_seasonal_on_train += day_to_median_map_series.reindex(train_dayofweek, fill_value=fallback_value).values\\n\\n    if actual_use_hourofday_seasonal:\\n        hour_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.hour).median()\\n        pred_hour = prediction_index.hour.values\\n        train_hour = input_targets_index.hour.values\\n\\n        projected_hour_forecast = hour_to_median_map_series.reindex(pred_hour, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_hour_forecast\\n        fitted_secondary_seasonal_on_train += hour_to_median_map_series.reindex(train_hour, fill_value=fallback_value).values\\n\\n    if actual_use_month_of_year_seasonal:\\n        month_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.month).median()\\n        pred_month = prediction_index.month.values\\n        train_month = input_targets_index.month.values\\n\\n        projected_month_forecast = month_to_median_map_series.reindex(pred_month, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_month_forecast\\n        fitted_secondary_seasonal_on_train += month_to_median_map_series.reindex(train_month, fill_value=fallback_value).values\\n\\n    if actual_use_day_of_year_seasonal:\\n        dayofyear_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofyear).median()\\n        pred_dayofyear = prediction_index.dayofyear.values\\n        train_dayofyear = input_targets_index.dayofyear.values\\n\\n        projected_dayofyear_forecast = dayofyear_to_median_map_series.reindex(pred_dayofyear, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_dayofyear_forecast\\n        fitted_secondary_seasonal_on_train += dayofyear_to_median_map_series.reindex(train_dayofyear, fill_value=fallback_value).values\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates and applies a residual correction component.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:]\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # This step is crucial to guarantee that \`processed_targets_np\` is always finite.\\n    processed_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n        initial_base_level_fallback_val_transformed = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n    else:\\n        initial_base_level_fallback_val_transformed = initial_base_level_fallback_val\\n\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        if transform_log:\\n            final_fallback_pred = np.expm1(np.full(prediction_length, initial_base_level_fallback_val_transformed))\\n        else:\\n            final_fallback_pred = np.full(prediction_length, initial_base_level_fallback_val_transformed)\\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        base_seasonal_pattern_naive_processed_scale = np.array([initial_base_level_fallback_val_transformed])\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config == None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite because processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) in a fixed order ---\\n    component_configs_map = {c['type']: c for c in config.get('components', [])}\\n\\n    fixed_component_order = ['trend', 'seasonal_primary', 'seasonal_secondary', 'residual_correction']\\n\\n    for component_type in fixed_component_order:\\n        component_config = component_configs_map.get(component_type)\\n        if component_config:\\n            # historical_data (current_residuals) is guaranteed finite.\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals,\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 462,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 3 # Incremented version to reflect latest review and validation\\n\\n# The config_list has been reduced to 6 configurations, providing a more concise yet diverse set of strategies.\\n# This reduction aims to simplify the search space while maintaining strong generalization capabilities.\\n# Each configuration is designed to cover distinct time series patterns, from simple baselines to\\n# complex additive/multiplicative models with various trend and seasonality considerations.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Ultra-robust for any series. Good for very short or highly erratic series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1 (Improved Config 2 from original): Additive: Adaptive Base + Smoother Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a robust reactive trend and residual correction.\\n    {'name': 'additive_adaptive_trend_seasonal_rc_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 2 (Adapted from original Config 1): Multiplicative: Log-Linear Trend + Primary Seasonal.\\n    # Effective for data with exponential growth/decay and strong primary seasonality.\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 3 (Original Config 3): Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data where DOW/HOD seasonality is key, often scales multiplicatively.\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (Original Config 4): Additive: Very Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 200}, # More adaptive stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5 (Combined from original Config 5 & 7): Comprehensive Additive: Global Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend over the full history.\\n    {'name': 'comprehensive_additive_global_trend_full_secondary_rc_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray, # Guaranteed finite by fit_and_predict_fn\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: Any # Can be float or None\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to median if trend cannot be reliably estimated.\\n    Centers x-coordinates for polyfit for numerical stability.\\n    Ensures fitted_trend_on_full_train is fully populated when windowing is active.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros\\n\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            window_size_base = int(multiplier * season_length)\\n        else: # season_length is 0, no dominant primary seasonality, interpret multiplier as absolute points\\n            window_size_base = int(multiplier * 7) # Default to 7 if season_length is 0, scaled by multiplier\\n        \\n        window_size = max(1, min(window_size_base, train_len))\\n    else: # trend_window_multiplier is None, use full history\\n        window_size = train_len\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        fitted_trend_on_full_train = np.full(train_len, median_val)\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        if len(y_fit_segment) > 0 and np.all(y_fit_segment == y_fit_segment[0]):\\n            median_val = y_fit_segment[0] # Use the constant value\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train # Early exit for constant case\\n\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        if len(y_fit_segment) > effective_degree:\\n            try:\\n                x_mean = np.mean(x_fit_segment_local)\\n                x_fit_segment_local_centered = x_fit_segment_local - x_mean\\n\\n                poly_coeffs = np.polyfit(x_fit_segment_local_centered, y_fit_segment, effective_degree)\\n                trend_poly = np.poly1d(poly_coeffs)\\n\\n                x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n                x_forecast_centered = x_forecast_raw - x_mean\\n                trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n                x_full_train_global_indices = np.arange(train_len)\\n                x_full_train_centered_for_trend_poly = (x_full_train_global_indices - fit_start_idx) - x_mean\\n                fitted_trend_on_full_train = trend_poly(x_full_train_centered_for_trend_poly)\\n\\n            except np.linalg.LinAlgError:\\n                # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n                median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n                trend_forecast_component = np.full(prediction_length, median_val)\\n                fitted_trend_on_full_train = np.full(train_len, median_val)\\n        else: # Not enough points for desired degree, fall back to median\\n            median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n\\n    # All outputs are guaranteed finite by the above logic or fallbacks.\\n    return trend_forecast_component, fitted_trend_on_full_train\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` here is expected to be the raw \`processed_targets_np\` (or log-transformed version),\\n    and is guaranteed to be finite by the main \`fit_and_predict_fn\`.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback for calculation\\n\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1]\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n\\n        k_window_size_val = max(1, min(k_window_size_val, train_len))\\n\\n        if k_window_size_val > 0:\\n            base_val = np.median(historical_data[-k_window_size_val:])\\n        else:\\n            base_val = np.median(historical_data)\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data)\\n\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\"\\"\\"\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates primary seasonal pattern based on \`season_length\`.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    if seasonal_data_len_for_avg > 0:\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0)\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions.\\n    # np.nanmedian can return NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _calculate_category_medians(categories_train: np.ndarray, residuals: np.ndarray, max_category: int, fallback_value: float = 0.0):\\n    \\"\\"\\"\\n    Calculates the median of residuals for each unique category using NumPy.\\n    Args:\\n        categories_train (np.ndarray): Array of integer categories for training data.\\n        residuals (np.ndarray): Array of residuals corresponding to categories_train.\\n        max_category (int): The maximum possible integer value for a category in the dataset\\n                            (e.g., 6 for dayofweek, 23 for hour, 366 for dayofyear).\\n        fallback_value (float): Value to use for categories not present in categories_train.\\n    Returns:\\n        np.ndarray: An array where index is the category and value is the median residual.\\n    \\"\\"\\"\\n    medians_map = np.full(max_category + 1, fallback_value, dtype=float)\\n    \\n    # Get unique categories present in the training data\\n    unique_categories = np.unique(categories_train)\\n\\n    for category in unique_categories:\\n        # Get indices corresponding to the current category\\n        indices = np.where(categories_train == category)[0]\\n        if len(indices) > 0:\\n            medians_map[category] = np.median(residuals[indices])\\n    \\n    return medians_map\\n\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals using NumPy.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\n    Optimized to use pure NumPy for speed.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    fallback_value = 0.0\\n\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else ''\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n    min_years_for_yearly_seasonality = 2.0\\n\\n    # Process Day of Week\\n    if component_config.get('use_dayofweek', False) and (is_hourly_or_subhourly or is_daily_or_coarser):\\n        train_dayofweek = input_targets_index.dayofweek.values # 0-6\\n        pred_dayofweek = prediction_index.dayofweek.values\\n        \\n        dayofweek_medians = _calculate_category_medians(train_dayofweek, historical_data, max_category=6, fallback_value=fallback_value)\\n        \\n        secondary_seasonal_forecast_component += dayofweek_medians[pred_dayofweek]\\n        fitted_secondary_seasonal_on_train += dayofweek_medians[train_dayofweek]\\n\\n    # Process Hour of Day\\n    if component_config.get('use_hourofday', False) and is_hourly_or_subhourly:\\n        train_hour = input_targets_index.hour.values # 0-23\\n        pred_hour = prediction_index.hour.values\\n\\n        hour_medians = _calculate_category_medians(train_hour, historical_data, max_category=23, fallback_value=fallback_value)\\n\\n        secondary_seasonal_forecast_component += hour_medians[pred_hour]\\n        fitted_secondary_seasonal_on_train += hour_medians[train_hour]\\n\\n    # Process Month of Year\\n    if component_config.get('use_month_of_year', False) and is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        train_month = input_targets_index.month.values # 1-12\\n        pred_month = prediction_index.month.values\\n\\n        month_medians = _calculate_category_medians(train_month, historical_data, max_category=12, fallback_value=fallback_value)\\n\\n        secondary_seasonal_forecast_component += month_medians[pred_month]\\n        fitted_secondary_seasonal_on_train += month_medians[train_month]\\n\\n    # Process Day of Year\\n    if component_config.get('use_day_of_year', False) and is_daily_or_coarser and \\\\\\n       data_span_days >= (365 * min_years_for_yearly_seasonality):\\n        train_dayofyear = input_targets_index.dayofyear.values # 1-366\\n        pred_dayofyear = prediction_index.dayofyear.values\\n        \\n        dayofyear_medians = _calculate_category_medians(train_dayofyear, historical_data, max_category=366, fallback_value=fallback_value)\\n\\n        secondary_seasonal_forecast_component += dayofyear_medians[pred_dayofyear]\\n        fitted_secondary_seasonal_on_train += dayofyear_medians[train_dayofyear]\\n\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates and applies a residual correction component.\\n    \`historical_data\` is guaranteed finite by the main \`fit_and_predict_fn\`.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    default_rc_fit_window = max(5, prediction_length)\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    if rc_fit_window == 0: # This can happen if train_len is too small (e.g., 0)\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n    \\n    residuals_to_correct = historical_data[-rc_fit_window:]\\n    correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n    residual_correction_value = 0.0\\n    if correction_method == 'median':\\n        residual_correction_value = np.median(residuals_to_correct)\\n    else: # Default to 'mean'\\n        residual_correction_value = np.mean(residuals_to_correct)\\n    \\n    fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n    residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n\\n    if residual_correction_decay_enabled:\\n        rc_forecast_component = residual_correction_value * (residual_correction_damping_factor ** np.arange(prediction_length))\\n    else:\\n        rc_forecast_component = np.full(prediction_length, residual_correction_value * residual_correction_damping_factor)\\n\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    original_targets_np = input_targets.values\\n\\n    # Forward-fill and then backward-fill NaNs.\\n    # For initial_base_level_fallback_val, get a robust estimate from finite values.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values)\\n    else:\\n        # If all original targets are NaN or empty, use 0 as a default fallback\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Handle NaNs in target series: ffill then bfill, finally replace any remaining NaNs (e.g., if series was all NaNs)\\n    processed_targets_np = np.array(original_targets_np) # Create a mutable copy\\n    nan_mask = np.isnan(processed_targets_np)\\n    if np.any(nan_mask):\\n        # Fill NaNs using pandas Series for ffill/bfill efficiency and robustness\\n        temp_series = pd.Series(processed_targets_np)\\n        processed_targets_np = temp_series.ffill().bfill().values\\n        # If still NaNs (e.g., series was all NaNs), fill with the fallback\\n        processed_targets_np = np.nan_to_num(processed_targets_np, nan=initial_base_level_fallback_val,\\n                                             posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n    \\n    # Ensure all values are finite after processing\\n    processed_targets_np = np.nan_to_num(processed_targets_np, nan=initial_base_level_fallback_val,\\n                                         posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n\\n    if transform_log:\\n        processed_targets_np = np.log1p(np.maximum(0, processed_targets_np))\\n        initial_base_level_fallback_val_transformed = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n    else:\\n        initial_base_level_fallback_val_transformed = initial_base_level_fallback_val\\n\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        if transform_log:\\n            final_fallback_pred = np.expm1(np.full(prediction_length, initial_base_level_fallback_val_transformed))\\n        else:\\n            final_fallback_pred = np.full(prediction_length, initial_base_level_fallback_val_transformed)\\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    effective_season_length_for_naive = max(1, season_length)\\n\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        base_seasonal_pattern_naive_processed_scale = np.array([initial_base_level_fallback_val_transformed])\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config == None:\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'}\\n\\n    # processed_targets_np is guaranteed finite.\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np,\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals will be finite because processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) in a fixed order ---\\n    component_configs_map = {c['type']: c for c in config.get('components', [])}\\n\\n    fixed_component_order = ['trend', 'seasonal_primary', 'seasonal_secondary', 'residual_correction']\\n\\n    for component_type in fixed_component_order:\\n        component_config = component_configs_map.get(component_type)\\n        if component_config:\\n            # historical_data (current_residuals) is guaranteed finite.\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals,\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index,\\n                prediction_index=prediction_index,\\n            )\\n\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>

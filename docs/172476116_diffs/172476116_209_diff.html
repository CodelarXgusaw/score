<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Diff Viewer</title>
    <!-- Google "Inter" font family -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        font-family: "Inter", sans-serif;
        display: flex;
        margin: 0; /* Simplify bounds so the parent can determine the correct iFrame height. */
        padding: 0;
        overflow: hidden; /* Code can be long and wide, causing scroll-within-scroll. */
      }

      .container {
        padding: 1.5rem;
        background-color: #fff;
      }

      h2 {
        font-size: 1.5rem;
        font-weight: 700;
        color: #1f2937;
        margin-bottom: 1rem;
        text-align: center;
      }

      .diff-output-container {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 1rem;
        background-color: #f8fafc;
        border: 1px solid #e2e8f0;
        border-radius: 0.75rem;
        box-shadow:
          0 4px 6px -1px rgba(0, 0, 0, 0.1),
          0 2px 4px -1px rgba(0, 0, 0, 0.06);
        padding: 1.5rem;
        font-size: 0.875rem;
        line-height: 1.5;
      }

      .diff-original-column {
        padding: 0.5rem;
        border-right: 1px solid #cbd5e1;
        min-height: 150px;
      }

      .diff-modified-column {
        padding: 0.5rem;
        min-height: 150px;
      }

      .diff-line {
        display: block;
        min-height: 1.5em;
        padding: 0 0.25rem;
        white-space: pre-wrap;
        word-break: break-word;
      }

      .diff-added {
        background-color: #d1fae5;
        color: #065f46;
      }
      .diff-removed {
        background-color: #fee2e2;
        color: #991b1b;
        text-decoration: line-through;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="diff-output-container">
        <div class="diff-original-column">
          <h2 id="original-title">Before</h2>
          <pre id="originalDiffOutput"></pre>
        </div>
        <div class="diff-modified-column">
          <h2 id="modified-title">After</h2>
          <pre id="modifiedDiffOutput"></pre>
        </div>
      </div>
    </div>
    <script>
      // Function to dynamically load the jsdiff library.
      function loadJsDiff() {
        const script = document.createElement("script");
        script.src = "https://cdnjs.cloudflare.com/ajax/libs/jsdiff/8.0.2/diff.min.js";
        script.integrity =
          "sha512-8pp155siHVmN5FYcqWNSFYn8Efr61/7mfg/F15auw8MCL3kvINbNT7gT8LldYPq3i/GkSADZd4IcUXPBoPP8gA==";
        script.crossOrigin = "anonymous";
        script.referrerPolicy = "no-referrer";
        script.onload = populateDiffs; // Call populateDiffs after the script is loaded
        script.onerror = () => {
          console.error("Error: Failed to load jsdiff library.");
          const originalDiffOutput = document.getElementById("originalDiffOutput");
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = "";
          }
        };
        document.head.appendChild(script);
      }

      function populateDiffs() {
        const originalDiffOutput = document.getElementById("originalDiffOutput");
        const modifiedDiffOutput = document.getElementById("modifiedDiffOutput");
        const originalTitle = document.getElementById("original-title");
        const modifiedTitle = document.getElementById("modified-title");

        // Check if jsdiff library is loaded.
        if (typeof Diff === "undefined") {
          console.error("Error: jsdiff library (Diff) is not loaded or defined.");
          // This case should ideally be caught by script.onerror, but keeping as a fallback.
          if (originalDiffOutput) {
            originalDiffOutput.innerHTML =
              '<p style="color: #dc2626; text-align: center; padding: 2rem;">Error: Diff library failed to load. Please try refreshing the page or check your internet connection.</p>';
          }
          if (modifiedDiffOutput) {
            modifiedDiffOutput.innerHTML = ""; // Clear modified output if error
          }
          return; // Exit since jsdiff is not loaded.
        }

        // The injected codes to display.
        const codes = JSON.parse(`{
  "old_index": 180.0,
  "old_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\nfrom typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 2 # Incremented version to reflect latest review and validation\\n\\n# The config_list can be adjusted to explore more hyperparameter combinations.\\n# The current config_list is designed to offer a diverse set of strategies, including\\n# simple baselines, additive/multiplicative models, different trend behaviors,\\n# primary/secondary seasonality, and residual corrections. This diversity aims\\n# to generalize well across the various GiftEval datasets.\\n\\n# Try to reduce the number of configs, prune unnecessary combinations, and\\n# up the diversity of the remaining configs. Do not overfit to validation data.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Ultra-robust for any series. Good for very short or highly erratic series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Multiplicative: Log-Linear Trend + Primary Seasonal (Windowed Trend).\\n    # Effective for data with exponential growth/decay and strong primary seasonality. (Original Config 4)\\n    {'name': 'multiplicative_linear_trend_primary_seasonal_log_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 30},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 50.0}, # Localized linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 2.5}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 2 (NEW/IMPROVED): Additive: Smoother Reactive Linear Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality, with a more robust reactive trend.\\n    {'name': 'additive_smoother_reactive_trend_seasonal_rc_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 20.0}, # Smoother reactive linear trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 3: Multiplicative: Log-Constant Trend + Secondary Seasonal (DOW/HOD) + RC.\\n    # For high-frequency data (e.g., hourly, 15min) where DOW/HOD seasonality is key, often scales multiplicatively. (Original Config 6)\\n    {'name': 'multiplicative_const_trend_secondary_seasonal_rc_log_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': False, 'use_day_of_year': False}, # Focus on high freq secondary\\n        {'type': 'residual_correction', 'rc_window_size': 30, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 1.0}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 4 (NEW/IMPROVED): Additive: Very Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_all_history'}, # Very stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 5: Additive: Long-term Linear Trend + Primary Seasonal.\\n    # Robust for datasets with consistent, long-term linear changes. (Original Config 8)\\n    {'name': 'additive_long_term_linear_trend_primary_5', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50},\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 5.0} # Moderately long seasonal average\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 6: Multiplicative: Log-Constant Trend + Primary Seasonal + RC.\\n    # For log-transformed series where the underlying level is stable but seasonality/residuals are key. (Original Config 9)\\n    {'name': 'multiplicative_stable_primary_rc_6_log', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60},\\n        {'type': 'trend', 'trend_method': 'constant_median_robust', 'trend_window_multiplier': None},\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 7: Comprehensive Additive: Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend. (Original Config 10)\\n    {'name': 'comprehensive_additive_seasonal_rc_7', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': True, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: float\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Improved: Centers x-coordinates for polyfit for numerical stability.\\n    Improved: Consistent robust handling for trend_degree=0.0.\\n    Ensures fitted_trend_on_full_train is fully populated when windowing is active.\\n    Added explicit check for sufficient data points before polyfit.\\n    Corrected trend_window_multiplier logic to adapt to season_length and handle\\n    absolute points vs. fraction for non-seasonal data.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros\\n\\n    # Robustness check for empty input array or all-NaN data.\\n    # Note: historical_residuals is expected to be already processed by nan_to_num.\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    # Corrected interpretation of trend_window_multiplier\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            # If season_length is available, interpret multiplier as number of seasons\\n            window_size_base = int(multiplier * season_length)\\n        else: # season_length is 0, no dominant primary seasonality, treat multiplier as absolute points\\n            # Interpret multiplier as an absolute number of points\\n            window_size_base = int(multiplier)\\n        \\n        # Ensure valid window size: at least 1, and not exceeding train_len\\n        window_size = max(1, min(window_size_base, train_len))\\n    else: # trend_window_multiplier is None, use full history\\n        window_size = train_len\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # --- Start of Logic for Trend Calculation ---\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        # historical_residuals is already guaranteed finite, so use np.median\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        # For a constant trend, it should apply to the entire history\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # applies to full history for constant\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        finite_segment = y_fit_segment # y_fit_segment is already finite\\n        if len(finite_segment) > 0 and np.all(finite_segment == finite_segment[0]):\\n            median_val = finite_segment[0] # Use the constant value\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train # Early exit for constant case\\n\\n        # Create local x-coordinates for the fitting segment, starting from 0\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        # Determine the effective degree: max(0, min(requested degree, max possible degree))\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Explicit check for sufficient points for polyfit\\n        if len(y_fit_segment) > effective_degree:\\n            try:\\n                # Explicitly center x-coordinates for improved numerical stability\\n                x_mean = np.mean(x_fit_segment_local)\\n                x_fit_segment_local_centered = x_fit_segment_local - x_mean\\n\\n                poly_coeffs = np.polyfit(x_fit_segment_local_centered, y_fit_segment, effective_degree)\\n                # Removed redundant np.all(np.isfinite(poly_coeffs)) check. If polyfit succeeds, coeffs are finite.\\n                trend_poly = np.poly1d(poly_coeffs)\\n\\n                # Forecast: x-values for prediction are relative to the start of the *fitting segment*\\n                # and also centered by the same mean as the training x-values.\\n                x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n                x_forecast_centered = x_forecast_raw - x_mean\\n                trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n                # Fitted: For the entire training history, extrapolate the trend\\n                # x-values for the full train length, shifted to start where the fit segment starts locally, and then centered\\n                x_full_train_relative_to_fit_start_local_zero = np.arange(-fit_start_idx, train_len - fit_start_idx)\\n                x_full_train_centered = x_full_train_relative_to_fit_start_local_zero - x_mean\\n                fitted_trend_on_full_train = trend_poly(x_full_train_centered)\\n\\n            except np.linalg.LinAlgError:\\n                # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n                median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n                trend_forecast_component = np.full(prediction_length, median_val)\\n                fitted_trend_on_full_train = np.full(train_len, median_val)\\n        else: # Not enough points for desired degree, fall back to median\\n            median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n    # --- End of Logic ---\\n\\n    # Ensure outputs are finite before returning. This is the ultimate safeguard against polyfit issues.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` here is expected to be the raw \`processed_targets_np\` (or log-transformed version),\\n    and is guaranteed to be finite.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    # Try to calculate base_val based on the chosen method\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1] # historical_data is finite, so this is finite.\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:]) # Use np.median\\n        else: # Fallback to median of all available data if not enough for a full season\\n            base_val = np.median(historical_data) # Use np.median\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n\\n        # Ensure window does not exceed available data and is at least 1\\n        k_window_size_val = max(1, min(k_window_size_val, train_len))\\n\\n        if k_window_size_val > 0:\\n            base_val = np.median(historical_data[-k_window_size_val:]) # Use np.median\\n        else:\\n            base_val = np.median(historical_data) # Fallback to median of all history, use np.median\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data) # Use np.median\\n\\n    # base_val is already finite because historical_data is finite.\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted.\\"\\"\\"\\n    # historical_data is expected to be already processed by nan_to_num and is finite.\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This is the current \\"residuals\\", which are finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates primary seasonal pattern based on \`season_length\`.\\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    # historical_data is expected to be already processed by nan_to_num and is finite.\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n\\n    # Determine the amount of historical residuals to use for averaging the seasonal pattern\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    # Always attempt the padded median calculation for robustness with partial cycles.\\n    if seasonal_data_len_for_avg > 0: # Only proceed if there's any data in the window\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            # Use np.nanmedian to handle NaNs from padding and calculate the robust seasonal pattern\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0)\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions\\n    # This nan_to_num is necessary as np.nanmedian can still produce NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    Adjusted threshold for yearly seasonality robustness.\\n    Optimized using Pandas groupby for efficiency and .reindex() for robust lookup.\\n    historical_data is expected to be already processed by nan_to_num and is finite.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    # A more sensible fallback value for unseen categories: 0.0 for additive components\\n    fallback_value = 0.0\\n\\n    # Added robustness check for empty/all-NaN data or missing frequency\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else '' # Ensure freq_str is a string\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    # Calculate span of the training data in days\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    actual_use_dayofweek_seasonal = component_config.get('use_dayofweek', False) and (is_hourly_or_subhourly or is_daily_or_coarser)\\n    actual_use_hourofday_seasonal = component_config.get('use_hourofday', False) and is_hourly_or_subhourly\\n\\n    # Condition for month_of_year and day_of_year:\\n    # Increased min_years_for_yearly_seasonality to 2.0 years for more robust estimation on yearly patterns.\\n    min_years_for_yearly_seasonality = 2.0 # Heuristic threshold for robust yearly patterns\\n    actual_use_month_of_year_seasonal = component_config.get('use_month_of_year', False) and \\\\\\n                                        is_daily_or_coarser and \\\\\\n                                        data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    actual_use_day_of_year_seasonal = component_config.get('use_day_of_year', False) and \\\\\\n                                      is_daily_or_coarser and \\\\\\n                                      data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    # Convert historical_data to a temporary Pandas Series for efficient groupby operations\\n    # historical_data is finite, so residuals_series_for_groupby will also be finite.\\n    residuals_series_for_groupby = pd.Series(historical_data, index=input_targets_index)\\n\\n    if actual_use_dayofweek_seasonal:\\n        day_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofweek).median()\\n        pred_dayofweek = prediction_index.dayofweek.values\\n        train_dayofweek = input_targets_index.dayofweek.values\\n\\n        # Use .reindex() to handle missing keys gracefully, filling with fallback_value\\n        projected_day_forecast = day_to_median_map_series.reindex(pred_dayofweek, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_day_forecast\\n        fitted_secondary_seasonal_on_train += day_to_median_map_series.reindex(train_dayofweek, fill_value=fallback_value).values\\n\\n    if actual_use_hourofday_seasonal:\\n        hour_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.hour).median()\\n        pred_hour = prediction_index.hour.values\\n        train_hour = input_targets_index.hour.values\\n\\n        # Use .reindex()\\n        projected_hour_forecast = hour_to_median_map_series.reindex(pred_hour, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_hour_forecast\\n        fitted_secondary_seasonal_on_train += hour_to_median_map_series.reindex(train_hour, fill_value=fallback_value).values\\n\\n    if actual_use_month_of_year_seasonal:\\n        month_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.month).median()\\n        pred_month = prediction_index.month.values\\n        train_month = input_targets_index.month.values\\n\\n        # Use .reindex()\\n        projected_month_forecast = month_to_median_map_series.reindex(pred_month, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_month_forecast\\n        fitted_secondary_seasonal_on_train += month_to_median_map_series.reindex(train_month, fill_value=fallback_value).values\\n\\n    if actual_use_day_of_year_seasonal:\\n        dayofyear_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofyear).median()\\n        pred_dayofyear = prediction_index.dayofyear.values\\n        train_dayofyear = input_targets_index.dayofyear.values\\n\\n        # Use .reindex()\\n        projected_dayofyear_forecast = dayofyear_to_median_map_series.reindex(pred_dayofyear, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_dayofyear_forecast\\n        fitted_secondary_seasonal_on_train += dayofyear_to_median_map_series.reindex(train_dayofyear, fill_value=fallback_value).values\\n\\n    # No np.nan_to_num needed here as all intermediate values are finite.\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates and applies a residual correction component.\\n    historical_data is expected to be already processed by nan_to_num and is finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    # Determine the window size for calculating residual correction\\n    default_rc_fit_window = max(5, prediction_length) # A reasonable default based on prediction length\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    # Get the damping factor for the residual correction. Default to 1.0 (no damping)\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n\\n\\n    if rc_fit_window > 0 and train_len >= rc_fit_window:\\n        residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n\\n        # Determine the correction method (mean or median)\\n        correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n        residual_correction_value = 0.0\\n        if correction_method == 'median':\\n            residual_correction_value = np.median(residuals_to_correct) # Use np.median\\n        else: # Default to 'mean'\\n            residual_correction_value = np.mean(residuals_to_correct) # Use np.mean\\n        \\n        # The fitted part should represent the constant correction value within the learning window.\\n        # This value should NOT be damped, as it's what's subtracted from actual residuals for subsequent components.\\n        if rc_fit_window > 0:\\n            fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n        residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n\\n        if residual_correction_decay_enabled:\\n            # Linearly decaying residual correction for forecast\\n            decay_factor_forecast = np.linspace(1.0, 0.0, prediction_length)\\n            rc_forecast_component = residual_correction_value * decay_factor_forecast\\n        else:\\n            # Constant residual correction across the entire prediction horizon\\n            rc_forecast_component = np.full(prediction_length, residual_correction_value)\\n    \\n    # Apply the damping factor ONLY to the forecast component.\\n    # This was the bug fix: The damping factor was not previously applied to the forecast.\\n    rc_forecast_component *= residual_correction_damping_factor\\n\\n    # No np.nan_to_num needed here as all intermediate values are finite.\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    # Convert original Series to numpy array for efficient processing\\n    original_targets_np = input_targets.values\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Before processing or log transformation, ensure all values are finite for a robust fallback.\\n    # The initial_base_level_fallback_val is computed on original finite values.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values) # Use nanmedian here as original_targets_np can have NaNs\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    filled_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        # Crucial change: Map 0 to 0 on the log1p scale. Negative values are also capped at 0.\\n        processed_targets_np = np.log1p(np.maximum(0, filled_targets_np))\\n        initial_base_level_fallback_val_transformed = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n    else:\\n        processed_targets_np = filled_targets_np\\n        initial_base_level_fallback_val_transformed = initial_base_level_fallback_val # No transform for fallback value\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Return fallback predictions (transformed back if necessary)\\n        if transform_log:\\n            final_fallback_pred = np.expm1(np.full(prediction_length, initial_base_level_fallback_val_transformed))\\n        else:\\n            final_fallback_pred = np.full(prediction_length, initial_base_level_fallback_val_transformed)\\n        # Apply non-negativity to this early exit fallback as well\\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    # Calculate seasonal naive fallback on the PROCESSED scale, then transform back.\\n    effective_season_length_for_naive = max(1, season_length)\\n\\n    # Use the processed_targets_np (potentially log-transformed data) for the fallback pattern\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Fallback to the transformed initial_base_level_fallback_val if no data for naive pattern\\n        base_seasonal_pattern_naive_processed_scale = np.array([initial_base_level_fallback_val_transformed])\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n\\n    # Generate the seasonal naive pattern on the processed scale\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    # Convert this fallback to the original scale if log transform was applied.\\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    # Apply non-negativity to this ultimate fallback, as it's a direct fallback path.\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    # Map component types to their respective functions\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Extract base_level config or use a default\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config == None: # Explicitly checking against None\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'} # Default base level\\n\\n    # Calculate initial base level and initialize predictions and residuals on the processed scale\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np, # processed_targets_np is finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals should be finite because processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        # Skip base_level as it's handled explicitly first\\n        if component_type == 'base_level':\\n            continue\\n\\n        if component_type in component_functions:\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # Subsequent components train on residuals (which are finite)\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index, # For secondary seasonality\\n                prediction_index=prediction_index, # For secondary seasonality\\n            )\\n\\n            # All component_forecast and component_fitted_on_train are guaranteed finite.\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n            # No np.nan_to_num needed on current_residuals as all operations are on finite arrays.\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # If predictions contain any NaNs or Infs (e.g., from extreme extrapolation or component failure),\\n    # fall back to seasonal naive predictions (which are robust and on the original scale, now correctly transformed).\\n    # This check remains important for overall robustness.\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    # This is the very last step for the final predictions\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")",
  "new_index": 209,
  "new_code": "from typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\nfrom typing import Any, Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n# You are starting from a top performing seed solution using only numpy\\n# You should improve the accuracy while also simplifiying the config list,\\n# speeding up the solution, and exploring more complex generalizable methods.\\nMODEL_NAME = \\"HybridDecompositionModel\\" # Name your solution here\\nMODEL_VERSION = 3 # Incremented version to reflect latest review and validation\\n\\n# The config_list can be adjusted to explore more hyperparameter combinations.\\n# The current config_list is designed to offer a diverse set of strategies, including\\n# simple baselines, additive/multiplicative models, different trend behaviors,\\n# primary/secondary seasonality, and residual corrections. This diversity aims\\n# to generalize well across the various GiftEval datasets.\\n\\n# Try to reduce the number of configs, prune unnecessary combinations, and\\n# up the diversity of the remaining configs. Do not overfit to validation data.\\nconfig_list = [\\n    # Config 0: Last Value Naive. Ultra-robust for any series. Good for very short or highly erratic series.\\n    {'name': 'last_value_naive_0', 'components': [{'type': 'base_level', 'base_level_method': 'last_value'}], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 1: Additive: Adaptive Trend + Primary Seasonal + Median RC.\\n    # For moderately fast-changing patterns with clear primary seasonality.\\n    {'name': 'additive_adaptive_trend_seasonal_rc_1', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 50}, # Moderately adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': 10.0}, # More reactive trend over 10 * season_length (or 10 points)\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 3.0},\\n        {'type': 'residual_correction', 'rc_window_size': 15, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.98}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 2: Multiplicative: Global Linear Trend + Primary Seasonal + RC (Log-transformed).\\n    # For data with exponential growth/decay and strong primary seasonality, where trend is global.\\n    {'name': 'multiplicative_global_linear_trend_primary_seasonal_rc_log_2', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_all_history'}, # Stable base on log scale\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None}, # Use full history for trend\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 8.0},\\n        {'type': 'residual_correction', 'rc_window_size': 20, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.95}\\n    ], 'non_negative': True, 'transform_log': True, 'version': MODEL_VERSION},\\n\\n    # Config 3: Comprehensive Additive: Global Linear Trend + Full Secondary Seasonal + RC.\\n    # Robust for complex datasets with multiple seasonalities and a general trend.\\n    {'name': 'comprehensive_additive_secondary_seasonal_3', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_last_k_window', 'k_window_size': 60}, # More adaptive base\\n        {'type': 'trend', 'trend_method': 'linear_polyfit', 'trend_degree': 1, 'trend_window_multiplier': None},\\n        {'type': 'seasonal_secondary', 'use_dayofweek': True, 'use_hourofday': True, 'use_month_of_year': True, 'use_day_of_year': True},\\n        {'type': 'residual_correction', 'rc_window_size': 25, 'residual_correction_method': 'median', 'residual_correction_decay_enabled': True, 'residual_correction_damping_factor': 0.90}\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n\\n    # Config 4: Additive: Very Stable Base + Long Primary Seasonal Only.\\n    # For very stable, strong seasonal patterns where trend/residual are negligible or captured by stable base, with longer averaging.\\n    {'name': 'additive_stable_base_long_primary_seasonal_4', 'components': [\\n        {'type': 'base_level', 'base_level_method': 'median_all_history'}, # Very stable base\\n        {'type': 'seasonal_primary', 'seasonal_avg_window_multiplier': 15.0} # Even longer averaging window for seasonality\\n    ], 'non_negative': False, 'transform_log': False, 'version': MODEL_VERSION},\\n]\\n\\n# Helper function for robust trend calculation\\ndef _calculate_trend_and_fitted(\\n    historical_residuals: np.ndarray,\\n    prediction_length: int,\\n    train_len: int,\\n    season_length: int,\\n    trend_method: str,\\n    trend_degree: int, # Only relevant for 'linear_polyfit'\\n    trend_window_multiplier: float\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates trend forecast and fitted trend on historical data.\\n    Returns (trend_forecast_component, fitted_trend_on_full_train).\\n    Ensures outputs are finite by falling back to zero if trend cannot be reliably estimated.\\n    Improved: Centers x-coordinates for polyfit for numerical stability.\\n    Improved: Consistent robust handling for trend_degree=0.0.\\n    Ensures fitted_trend_on_full_train is fully populated when windowing is active.\\n    Added explicit check for sufficient data points before polyfit.\\n    Corrected trend_window_multiplier logic to adapt to season_length and handle\\n    absolute points vs. fraction for non-seasonal data.\\n    \\"\\"\\"\\n    trend_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_trend_on_full_train = np.zeros(train_len, dtype=float) # Initialize with zeros\\n\\n    # Robustness check for empty input array or all-NaN data.\\n    # Note: historical_residuals is expected to be already processed by nan_to_num.\\n    if train_len < 1:\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # Determine the segment of data to use for fitting the trend\\n    window_size = train_len # Default to full history (no windowing)\\n\\n    # Corrected interpretation of trend_window_multiplier\\n    if trend_window_multiplier is not None:\\n        multiplier = trend_window_multiplier\\n        if season_length > 0:\\n            # If season_length is available, interpret multiplier as number of seasons\\n            window_size_base = int(multiplier * season_length)\\n        else: # season_length is 0, no dominant primary seasonality, treat multiplier as absolute points\\n            # Interpret multiplier as an absolute number of points\\n            window_size_base = int(multiplier)\\n        \\n        # Ensure valid window size: at least 1, and not exceeding train_len\\n        window_size = max(1, min(window_size_base, train_len))\\n    else: # trend_window_multiplier is None, use full history\\n        window_size = train_len\\n\\n    fit_start_idx = train_len - window_size\\n    y_fit_segment = historical_residuals[fit_start_idx:]\\n\\n\\n    if len(y_fit_segment) < 1: # No data in segment to fit\\n        return trend_forecast_component, fitted_trend_on_full_train\\n\\n    # --- Start of Logic for Trend Calculation ---\\n    # If degree is 0, or method is 'constant_median_robust', use median for robustness\\n    if trend_degree == 0 or trend_method == 'constant_median_robust':\\n        # historical_residuals is already guaranteed finite, so use np.median\\n        median_val = np.median(y_fit_segment)\\n        trend_forecast_component = np.full(prediction_length, median_val)\\n        # For a constant trend, it should apply to the entire history\\n        fitted_trend_on_full_train = np.full(train_len, median_val) # applies to full history for constant\\n    elif trend_method == 'linear_polyfit':\\n        # Add explicit check for constant segment to prevent polyfit errors/instability\\n        finite_segment = y_fit_segment # y_fit_segment is already finite\\n        if len(finite_segment) > 0 and np.all(finite_segment == finite_segment[0]):\\n            median_val = finite_segment[0] # Use the constant value\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n            return trend_forecast_component, fitted_trend_on_full_train # Early exit for constant case\\n\\n        # Create local x-coordinates for the fitting segment, starting from 0\\n        x_fit_segment_local = np.arange(len(y_fit_segment))\\n\\n        # Determine the effective degree: max(0, min(requested degree, max possible degree))\\n        effective_degree = min(trend_degree, len(y_fit_segment) - 1)\\n        effective_degree = max(0, effective_degree)\\n\\n        # Explicit check for sufficient points for polyfit\\n        if len(y_fit_segment) > effective_degree:\\n            try:\\n                # Explicitly center x-coordinates for improved numerical stability\\n                x_mean = np.mean(x_fit_segment_local)\\n                x_fit_segment_local_centered = x_fit_segment_local - x_mean\\n\\n                poly_coeffs = np.polyfit(x_fit_segment_local_centered, y_fit_segment, effective_degree)\\n                # Removed redundant np.all(np.isfinite(poly_coeffs)) check. If polyfit succeeds, coeffs are finite.\\n                trend_poly = np.poly1d(poly_coeffs)\\n\\n                # Forecast: x-values for prediction are relative to the start of the *fitting segment*\\n                # and also centered by the same mean as the training x-values.\\n                x_forecast_raw = np.arange(len(y_fit_segment), len(y_fit_segment) + prediction_length)\\n                x_forecast_centered = x_forecast_raw - x_mean\\n                trend_forecast_component = trend_poly(x_forecast_centered)\\n\\n                # Fitted: For the entire training history, extrapolate the trend\\n                # x-values for the full train length, shifted to start where the fit segment starts locally, and then centered\\n                x_full_train_relative_to_fit_start_local_zero = np.arange(-fit_start_idx, train_len - fit_start_idx)\\n                x_full_train_centered = x_full_train_relative_to_fit_start_local_zero - x_mean\\n                fitted_trend_on_full_train = trend_poly(x_full_train_centered)\\n\\n            except np.linalg.LinAlgError:\\n                # Fallback to constant median if polyfit fails (e.g., singular matrix, not enough unique points)\\n                median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n                trend_forecast_component = np.full(prediction_length, median_val)\\n                fitted_trend_on_full_train = np.full(train_len, median_val)\\n        else: # Not enough points for desired degree, fall back to median\\n            median_val = np.median(y_fit_segment) # y_fit_segment is finite\\n            trend_forecast_component = np.full(prediction_length, median_val)\\n            fitted_trend_on_full_train = np.full(train_len, median_val)\\n    # --- End of Logic ---\\n\\n    # Ensure outputs are finite before returning. This is the ultimate safeguard against polyfit issues.\\n    return np.nan_to_num(trend_forecast_component, nan=0.0, posinf=0.0, neginf=0.0), \\\\\\n           np.nan_to_num(fitted_trend_on_full_train, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n\\n# --- New Modular Component Functions ---\\n\\ndef _fit_predict_base_level(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates a dynamic base level based on \`base_level_method\` and returns it as forecast and fitted.\\n    \`historical_data\` here is expected to be the raw \`processed_targets_np\` (or log-transformed version),\\n    and is guaranteed to be finite.\\n    \\"\\"\\"\\n    if train_len == 0:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    base_level_method = component_config.get('base_level_method', 'median_all_history')\\n    base_val = 0.0 # Default fallback\\n\\n    # Try to calculate base_val based on the chosen method\\n    if base_level_method == 'last_value':\\n        base_val = historical_data[-1] # historical_data is finite, so this is finite.\\n    elif base_level_method == 'median_last_season':\\n        effective_season_length = max(1, season_length)\\n        if train_len >= effective_season_length:\\n            base_val = np.median(historical_data[-effective_season_length:]) # Use np.median\\n        else: # Fallback to median of all available data if not enough for a full season\\n            base_val = np.median(historical_data) # Use np.median\\n    elif base_level_method == 'median_last_k_window':\\n        k_window_size_default = max(7, season_length) if season_length > 0 else 7\\n        k_window_size_val = component_config.get('k_window_size', k_window_size_default)\\n\\n        # Ensure window does not exceed available data and is at least 1\\n        k_window_size_val = max(1, min(k_window_size_val, train_len))\\n\\n        if k_window_size_val > 0:\\n            base_val = np.median(historical_data[-k_window_size_val:]) # Use np.median\\n        else:\\n            base_val = np.median(historical_data) # Fallback to median of all history, use np.median\\n    elif base_level_method == 'zero_constant':\\n        base_val = 0.0\\n    else: # Default or 'median_all_history'\\n        base_val = np.median(historical_data) # Use np.median\\n\\n    # base_val is already finite because historical_data is finite.\\n    return np.full(prediction_length, base_val), np.full(train_len, base_val)\\n\\ndef _fit_predict_trend_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Wrapper for _calculate_trend_and_fitted.\\"\\"\\"\\n    # historical_data is expected to be already processed by nan_to_num and is finite.\\n    if train_len < 1:\\n        return np.zeros(prediction_length), np.zeros(train_len)\\n\\n    return _calculate_trend_and_fitted(\\n        historical_residuals=historical_data, # This is the current \\"residuals\\", which are finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        trend_method=component_config.get('trend_method', 'linear_polyfit'),\\n        trend_degree=component_config.get('trend_degree', 1),\\n        trend_window_multiplier=component_config.get('trend_window_multiplier')\\n    )\\n\\n# New helper functions for frequency type checks\\ndef _dataset_has_hourly_or_subhourly_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is hourly or sub-hourly.\\"\\"\\"\\n    return any(f in freq_str for f in ['T', 'H'])\\n\\ndef _dataset_has_daily_or_coarser_freq(freq_str: str) -> bool:\\n    \\"\\"\\"True if frequency is daily or coarser (weekly, monthly, etc.).\\"\\"\\"\\n    return any(f in freq_str for f in ['D', 'W', 'M', 'Q', 'A'])\\n\\n\\ndef _fit_predict_seasonal_primary_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates primary seasonal pattern based on \`season_length\`.\\"\\"\\"\\n    seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    effective_season_length = max(1, season_length)\\n    # historical_data is expected to be already processed by nan_to_num and is finite.\\n    if effective_season_length <= 1 or train_len == 0:\\n        return seasonal_forecast_component, fitted_seasonal_on_train\\n\\n    seasonal_avg_window_multiplier = component_config.get('seasonal_avg_window_multiplier', 3.0)\\n\\n    # Determine the amount of historical residuals to use for averaging the seasonal pattern\\n    seasonal_data_len_for_avg = min(train_len, int(seasonal_avg_window_multiplier * effective_season_length))\\n\\n    seasonal_pattern = np.zeros(effective_season_length, dtype=float)\\n\\n    # Always attempt the padded median calculation for robustness with partial cycles.\\n    if seasonal_data_len_for_avg > 0: # Only proceed if there's any data in the window\\n        seasonal_avg_data = historical_data[-seasonal_data_len_for_avg:]\\n        # Pad with NaNs for robust median calculation across cycles\\n        padding_needed = (effective_season_length - (len(seasonal_avg_data) % effective_season_length)) % effective_season_length\\n        padded_seasonal_avg_data = np.pad(seasonal_avg_data, (padding_needed, 0), 'constant', constant_values=np.nan)\\n\\n        num_cycles = len(padded_seasonal_avg_data) // effective_season_length\\n        if num_cycles > 0:\\n            reshaped_data = padded_seasonal_avg_data.reshape(num_cycles, effective_season_length)\\n            # Use np.nanmedian to handle NaNs from padding and calculate the robust seasonal pattern\\n            seasonal_pattern = np.nanmedian(reshaped_data, axis=0)\\n    \\n    # Ensure seasonal pattern is finite before tiling and using in predictions\\n    # This nan_to_num is necessary as np.nanmedian can still produce NaN if all values in a column are NaN.\\n    seasonal_pattern = np.nan_to_num(seasonal_pattern, nan=0.0, posinf=0.0, neginf=0.0)\\n\\n    effective_pattern_length = max(1, len(seasonal_pattern))\\n    num_repeats_seasonal = (prediction_length + effective_pattern_length - 1) // effective_pattern_length\\n    seasonal_forecast_component = np.tile(seasonal_pattern, num_repeats_seasonal)[:prediction_length]\\n\\n    fitted_seasonal_on_train = np.tile(seasonal_pattern, (train_len + effective_pattern_length - 1) // effective_pattern_length)[:train_len]\\n\\n    return seasonal_forecast_component, fitted_seasonal_on_train\\n\\ndef _fit_predict_seasonal_secondary_component(\\n    historical_data: np.ndarray, prediction_index: pd.Index, input_targets_index: pd.Index,\\n    prediction_length: int, train_len: int, season_length: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"\\n    Calculates secondary seasonal patterns (DOW, HOD, Month of Year, Day of Year) from residuals.\\n    Adjusted threshold for yearly seasonality robustness.\\n    Optimized using Pandas groupby for efficiency and .reindex() for robust lookup.\\n    historical_data is expected to be already processed by nan_to_num and is finite.\\n    \\"\\"\\"\\n    secondary_seasonal_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_secondary_seasonal_on_train = np.zeros(train_len, dtype=float)\\n\\n    # A more sensible fallback value for unseen categories: 0.0 for additive components\\n    fallback_value = 0.0\\n\\n    # Added robustness check for empty/all-NaN data or missing frequency\\n    freq_str = input_targets_index.freqstr if input_targets_index.freqstr is not None else '' # Ensure freq_str is a string\\n    if not freq_str or train_len == 0:\\n        return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\n    is_hourly_or_subhourly = _dataset_has_hourly_or_subhourly_freq(freq_str)\\n    is_daily_or_coarser = _dataset_has_daily_or_coarser_freq(freq_str)\\n\\n    # Calculate span of the training data in days\\n    data_span_days = (input_targets_index.max() - input_targets_index.min()).days if train_len > 1 else 0\\n\\n    actual_use_dayofweek_seasonal = component_config.get('use_dayofweek', False) and (is_hourly_or_subhourly or is_daily_or_coarser)\\n    actual_use_hourofday_seasonal = component_config.get('use_hourofday', False) and is_hourly_or_subhourly\\n\\n    # Condition for month_of_year and day_of_year:\\n    # Increased min_years_for_yearly_seasonality to 2.0 years for more robust estimation on yearly patterns.\\n    min_years_for_yearly_seasonality = 2.0 # Heuristic threshold for robust yearly patterns\\n    actual_use_month_of_year_seasonal = component_config.get('use_month_of_year', False) and \\\\\\n                                        is_daily_or_coarser and \\\\\\n                                        data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    actual_use_day_of_year_seasonal = component_config.get('use_day_of_year', False) and \\\\\\n                                      is_daily_or_coarser and \\\\\\n                                      data_span_days >= (365 * min_years_for_yearly_seasonality)\\n\\n    # Convert historical_data to a temporary Pandas Series for efficient groupby operations\\n    # historical_data is finite, so residuals_series_for_groupby will also be finite.\\n    residuals_series_for_groupby = pd.Series(historical_data, index=input_targets_index)\\n\\n    if actual_use_dayofweek_seasonal:\\n        day_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofweek).median()\\n        pred_dayofweek = prediction_index.dayofweek.values\\n        train_dayofweek = input_targets_index.dayofweek.values\\n\\n        # Use .reindex() to handle missing keys gracefully, filling with fallback_value\\n        projected_day_forecast = day_to_median_map_series.reindex(pred_dayofweek, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_day_forecast\\n        fitted_secondary_seasonal_on_train += day_to_median_map_series.reindex(train_dayofweek, fill_value=fallback_value).values\\n\\n    if actual_use_hourofday_seasonal:\\n        hour_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.hour).median()\\n        pred_hour = prediction_index.hour.values\\n        train_hour = input_targets_index.hour.values\\n\\n        # Use .reindex()\\n        projected_hour_forecast = hour_to_median_map_series.reindex(pred_hour, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_hour_forecast\\n        fitted_secondary_seasonal_on_train += hour_to_median_map_series.reindex(train_hour, fill_value=fallback_value).values\\n\\n    if actual_use_month_of_year_seasonal:\\n        month_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.month).median()\\n        pred_month = prediction_index.month.values\\n        train_month = input_targets_index.month.values\\n\\n        # Use .reindex()\\n        projected_month_forecast = month_to_median_map_series.reindex(pred_month, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_month_forecast\\n        fitted_secondary_seasonal_on_train += month_to_median_map_series.reindex(train_month, fill_value=fallback_value).values\\n\\n    if actual_use_day_of_year_seasonal:\\n        dayofyear_to_median_map_series = residuals_series_for_groupby.groupby(residuals_series_for_groupby.index.dayofyear).median()\\n        pred_dayofyear = prediction_index.dayofyear.values\\n        train_dayofyear = input_targets_index.dayofyear.values\\n\\n        # Use .reindex()\\n        projected_dayofyear_forecast = dayofyear_to_median_map_series.reindex(pred_dayofyear, fill_value=fallback_value).values\\n        secondary_seasonal_forecast_component += projected_dayofyear_forecast\\n        fitted_secondary_seasonal_on_train += dayofyear_to_median_map_series.reindex(train_dayofyear, fill_value=fallback_value).values\\n\\n    # No np.nan_to_num needed here as all intermediate values are finite.\\n    return secondary_seasonal_forecast_component, fitted_secondary_seasonal_on_train\\n\\ndef _fit_predict_residual_correction_component(\\n    historical_data: np.ndarray, prediction_length: int, train_len: int, component_config: Dict[str, Any], **kwargs\\n) -> Tuple[np.ndarray, np.ndarray]:\\n    \\"\\"\\"Calculates and applies a residual correction component.\\n    historical_data is expected to be already processed by nan_to_num and is finite.\\n    \\"\\"\\"\\n    rc_forecast_component = np.zeros(prediction_length, dtype=float)\\n    fitted_rc_on_train = np.zeros(train_len, dtype=float)\\n\\n    if train_len == 0:\\n        return rc_forecast_component, fitted_rc_on_train\\n\\n    # Determine the window size for calculating residual correction\\n    default_rc_fit_window = max(5, prediction_length) # A reasonable default based on prediction length\\n    rc_fit_window = max(1, min(component_config.get('rc_window_size', default_rc_fit_window), train_len))\\n\\n    # Get the damping factor for the residual correction. Default to 1.0 (no damping)\\n    residual_correction_damping_factor = component_config.get('residual_correction_damping_factor', 1.0)\\n\\n\\n    if rc_fit_window > 0 and train_len >= rc_fit_window:\\n        residuals_to_correct = historical_data[-rc_fit_window:] # This data is finite.\\n\\n        # Determine the correction method (mean or median)\\n        correction_method = component_config.get('residual_correction_method', 'mean')\\n\\n        residual_correction_value = 0.0\\n        if correction_method == 'median':\\n            residual_correction_value = np.median(residuals_to_correct) # Use np.median\\n        else: # Default to 'mean'\\n            residual_correction_value = np.mean(residuals_to_correct) # Use np.mean\\n        \\n        # The fitted part should represent the constant correction value within the learning window.\\n        # This value should NOT be damped, as it's what's subtracted from actual residuals for subsequent components.\\n        if rc_fit_window > 0:\\n            fitted_rc_on_train[-rc_fit_window:] = residual_correction_value\\n\\n        residual_correction_decay_enabled = component_config.get('residual_correction_decay_enabled', True)\\n\\n        if residual_correction_decay_enabled:\\n            # Linearly decaying residual correction for forecast\\n            decay_factor_forecast = np.linspace(1.0, 0.0, prediction_length)\\n            rc_forecast_component = residual_correction_value * decay_factor_forecast\\n        else:\\n            # Constant residual correction across the entire prediction horizon\\n            rc_forecast_component = np.full(prediction_length, residual_correction_value)\\n    \\n    # Apply the damping factor ONLY to the forecast component.\\n    # This was the bug fix: The damping factor was not previously applied to the forecast.\\n    rc_forecast_component *= residual_correction_damping_factor\\n\\n    # No np.nan_to_num needed here as all intermediate values are finite.\\n    return rc_forecast_component, fitted_rc_on_train\\n\\n\\n# Main forecasting function\\ndef fit_and_predict_fn(input_targets: pd.Series, prediction_index: pd.Index, season_length: int, config: Dict[str, Any]) -> pd.Series:\\n    \\"\\"\\"\\n    Forecasting function implementing an additive model with configurable components,\\n    now with an optional log transformation for multiplicative modeling.\\n    Components are applied sequentially, with each subsequent component learning on the residuals\\n    from the previous ones, akin to a gradient boosting approach.\\n    It robustly handles NaNs and provides ultimate fallbacks for edge cases.\\n    \\"\\"\\"\\n    prediction_length = len(prediction_index)\\n    train_len = len(input_targets)\\n\\n    transform_log = config.get('transform_log', False)\\n\\n    # --- 1. Robust NaN Handling for input_targets and determining initial base_level for processing ---\\n    # Convert original Series to numpy array for efficient processing\\n    original_targets_np = input_targets.values\\n\\n    # Apply ffill, bfill using a temporary Pandas Series for convenience.\\n    temp_series = pd.Series(original_targets_np, index=input_targets.index)\\n    filled_targets_np = temp_series.ffill().bfill().values\\n\\n    # Before processing or log transformation, ensure all values are finite for a robust fallback.\\n    # The initial_base_level_fallback_val is computed on original finite values.\\n    finite_original_values = original_targets_np[np.isfinite(original_targets_np)]\\n    if len(finite_original_values) > 0:\\n        initial_base_level_fallback_val = np.nanmedian(finite_original_values) # Use nanmedian here as original_targets_np can have NaNs\\n    else:\\n        initial_base_level_fallback_val = 0.0\\n\\n    # Ensure all values in filled_targets_np are finite, using initial_base_level_fallback_val\\n    filled_targets_np = np.nan_to_num(filled_targets_np, nan=initial_base_level_fallback_val, posinf=initial_base_level_fallback_val, neginf=initial_base_level_fallback_val)\\n\\n    # Apply log transformation if configured, ensuring values are non-negative for log1p\\n    if transform_log:\\n        # Crucial change: Map 0 to 0 on the log1p scale. Negative values are also capped at 0.\\n        processed_targets_np = np.log1p(np.maximum(0, filled_targets_np))\\n        initial_base_level_fallback_val_transformed = np.log1p(np.maximum(0, initial_base_level_fallback_val))\\n    else:\\n        processed_targets_np = filled_targets_np\\n        initial_base_level_fallback_val_transformed = initial_base_level_fallback_val # No transform for fallback value\\n\\n    # --- 2. Handle Edge Case: Empty or Very Short Processed Input ---\\n    if train_len == 0:\\n        # Return fallback predictions (transformed back if necessary)\\n        if transform_log:\\n            final_fallback_pred = np.expm1(np.full(prediction_length, initial_base_level_fallback_val_transformed))\\n        else:\\n            final_fallback_pred = np.full(prediction_length, initial_base_level_fallback_val_transformed)\\n        # Apply non-negativity to this early exit fallback as well\\n        if config.get('non_negative', False):\\n            final_fallback_pred = np.maximum(0, final_fallback_pred)\\n        return pd.Series(final_fallback_pred, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")\\n\\n\\n    # --- 3. Prepare Seasonal Naive Fallback (always available for robustness) ---\\n    # Calculate seasonal naive fallback on the PROCESSED scale, then transform back.\\n    effective_season_length_for_naive = max(1, season_length)\\n\\n    # Use the processed_targets_np (potentially log-transformed data) for the fallback pattern\\n    base_seasonal_pattern_naive_processed_scale = processed_targets_np[-effective_season_length_for_naive:]\\n\\n    if len(base_seasonal_pattern_naive_processed_scale) == 0:\\n        # Fallback to the transformed initial_base_level_fallback_val if no data for naive pattern\\n        base_seasonal_pattern_naive_processed_scale = np.array([initial_base_level_fallback_val_transformed])\\n\\n    effective_pattern_length_naive = max(1, len(base_seasonal_pattern_naive_processed_scale))\\n    num_repeats_naive = (prediction_length + effective_pattern_length_naive - 1) // effective_pattern_length_naive\\n\\n    # Generate the seasonal naive pattern on the processed scale\\n    seasonal_naive_fallback_predictions_processed_scale = np.tile(base_seasonal_pattern_naive_processed_scale, num_repeats_naive)[:prediction_length]\\n    \\n    # Convert this fallback to the original scale if log transform was applied.\\n    if transform_log:\\n        seasonal_naive_fallback_predictions = np.expm1(seasonal_naive_fallback_predictions_processed_scale)\\n    else:\\n        seasonal_naive_fallback_predictions = seasonal_naive_fallback_predictions_processed_scale\\n\\n    # Apply non-negativity to this ultimate fallback, as it's a direct fallback path.\\n    seasonal_naive_fallback_predictions = np.maximum(0, seasonal_naive_fallback_predictions)\\n\\n\\n    # --- 4. Initialize Additive Model Components ---\\n    # Map component types to their respective functions\\n    component_functions = {\\n        'base_level': _fit_predict_base_level,\\n        'trend': _fit_predict_trend_component,\\n        'seasonal_primary': _fit_predict_seasonal_primary_component,\\n        'seasonal_secondary': _fit_predict_seasonal_secondary_component,\\n        'residual_correction': _fit_predict_residual_correction_component,\\n    }\\n\\n    # Extract base_level config or use a default\\n    base_level_config = next((c for c in config.get('components', []) if c.get('type') == 'base_level'), None)\\n    if base_level_config == None: # Explicitly checking against None\\n        base_level_config = {'type': 'base_level', 'base_level_method': 'median_all_history'} # Default base level\\n\\n    # Calculate initial base level and initialize predictions and residuals on the processed scale\\n    initial_base_forecast, initial_base_fitted_on_train = component_functions['base_level'](\\n        historical_data=processed_targets_np, # processed_targets_np is finite\\n        prediction_length=prediction_length,\\n        train_len=train_len,\\n        season_length=season_length,\\n        component_config=base_level_config\\n    )\\n\\n    predictions_on_processed_scale = initial_base_forecast\\n    # current_residuals should be finite because processed_targets_np and initial_base_fitted_on_train are finite.\\n    current_residuals = processed_targets_np - initial_base_fitted_on_train\\n\\n    # --- 5. Sequentially apply other components (boosting-like approach) ---\\n    for component_config in config.get('components', []):\\n        component_type = component_config.get('type')\\n        # Skip base_level as it's handled explicitly first\\n        if component_type == 'base_level':\\n            continue\\n\\n        if component_type in component_functions:\\n            component_forecast, component_fitted_on_train = component_functions[component_type](\\n                historical_data=current_residuals, # Subsequent components train on residuals (which are finite)\\n                prediction_length=prediction_length,\\n                train_len=train_len,\\n                season_length=season_length,\\n                component_config=component_config,\\n                input_targets_index=input_targets.index, # For secondary seasonality\\n                prediction_index=prediction_index, # For secondary seasonality\\n            )\\n\\n            # All component_forecast and component_fitted_on_train are guaranteed finite.\\n            predictions_on_processed_scale += component_forecast\\n            current_residuals -= component_fitted_on_train\\n            # No np.nan_to_num needed on current_residuals as all operations are on finite arrays.\\n\\n    # --- 6. Transform back if log transformation was applied ---\\n    if transform_log:\\n        predictions = np.expm1(predictions_on_processed_scale)\\n    else:\\n        predictions = predictions_on_processed_scale\\n\\n    # --- 7. Final Robustness Checks ---\\n    # If predictions contain any NaNs or Infs (e.g., from extreme extrapolation or component failure),\\n    # fall back to seasonal naive predictions (which are robust and on the original scale, now correctly transformed).\\n    # This check remains important for overall robustness.\\n    if not np.all(np.isfinite(predictions)):\\n        predictions = seasonal_naive_fallback_predictions\\n\\n    # --- 8. Apply Non-Negativity Constraint if configured ---\\n    # This is the very last step for the final predictions\\n    if config.get('non_negative', False):\\n        predictions = np.maximum(0, predictions)\\n\\n    return pd.Series(predictions, index=prediction_index, name=f\\"{input_targets.name}_predictions\\")"
}`);
        const originalCode = codes["old_code"];
        const modifiedCode = codes["new_code"];
        const originalIndex = codes["old_index"];
        const modifiedIndex = codes["new_index"];

        function displaySideBySideDiff(originalText, modifiedText) {
          const diff = Diff.diffLines(originalText, modifiedText);

          let originalHtmlLines = [];
          let modifiedHtmlLines = [];

          const escapeHtml = (text) =>
            text.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/>/g, "&gt;");

          diff.forEach((part) => {
            // Split the part's value into individual lines.
            // If the string ends with a newline, split will add an empty string at the end.
            // We need to filter this out unless it's an actual empty line in the code.
            const lines = part.value.split("\n");
            const actualContentLines =
              lines.length > 0 && lines[lines.length - 1] === "" ? lines.slice(0, -1) : lines;

            actualContentLines.forEach((lineContent) => {
              const escapedLineContent = escapeHtml(lineContent);

              if (part.removed) {
                // Line removed from original, display in original column, add blank in modified.
                originalHtmlLines.push(
                  `<span class="diff-line diff-removed">${escapedLineContent}</span>`,
                );
                // Use &nbsp; for consistent line height.
                modifiedHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
              } else if (part.added) {
                // Line added to modified, add blank in original column, display in modified.
                // Use &nbsp; for consistent line height.
                originalHtmlLines.push(`<span class="diff-line">&nbsp;</span>`);
                modifiedHtmlLines.push(
                  `<span class="diff-line diff-added">${escapedLineContent}</span>`,
                );
              } else {
                // Equal part - no special styling (no background)
                // Common line, display in both columns without any specific diff class.
                originalHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
                modifiedHtmlLines.push(`<span class="diff-line">${escapedLineContent}</span>`);
              }
            });
          });

          // Join the lines and set innerHTML.
          originalDiffOutput.innerHTML = originalHtmlLines.join("");
          modifiedDiffOutput.innerHTML = modifiedHtmlLines.join("");
        }

        // Initial display with default content on DOMContentLoaded.
        displaySideBySideDiff(originalCode, modifiedCode);

        // Title the texts with their node numbers.
        originalTitle.textContent = `Parent #${originalIndex}`;
        modifiedTitle.textContent = `Child #${modifiedIndex}`;
      }

      // Load the jsdiff script when the DOM is fully loaded.
      document.addEventListener("DOMContentLoaded", loadJsDiff);
    </script>
  </body>
</html>
